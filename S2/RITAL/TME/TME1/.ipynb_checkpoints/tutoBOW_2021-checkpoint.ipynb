{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de documents : prise en main des outils\n",
    "\n",
    "Le but de ce TP est de classer des documents textuels... Dans un premier temps, nous allons vérifier le bon fonctionnement des outils sur des données jouets puis appliquer les concepts sur des données réelles.\n",
    "\n",
    "\n",
    "## Conception de la chaine de traitement\n",
    "Pour rappel, une chaine de traitement de documents classique est composée des étapes suivantes:\n",
    "1. Lecture des données et importation\n",
    "    - Dans le cadre de nos TP, nous faisons l'hypothèse que le corpus tient en mémoire... Si ce n'est pas le cas, il faut alors ajouter des structures de données avec des buffers (*data-reader*), bien plus complexes à mettre en place.\n",
    "    - Le plus grand piège concerne l'encodage des données. Dans le TP... Pas (ou peu) de problème. Dans la vraie vie: il faut faire attention à toujours maitriser les formats d'entrée et de sortie.\n",
    "1. Traitement des données brutes paramétrique. Chaque traitement doit être activable ou desactivable + paramétrable si besoin.\n",
    "    - Enlever les informations *inutiles* : chiffre, ponctuations, majuscules, etc... <BR>\n",
    "    **L'utilité dépend de l'application!**\n",
    "    - Segmenter en mots (=*Tokenization*)\n",
    "    - Elimination des stop-words\n",
    "    - Stemming/lemmatisation (racinisation)\n",
    "    - Byte-pair encoding pour trouver les mots composés (e.g. Sorbonne Université, Ville de Paris, Premier Ministre, etc...)\n",
    "1. Traitement des données numériques\n",
    "    - Normalisation *term-frequency* / binarisation\n",
    "    - Normalisation *inverse document frequency*\n",
    "    - Elimination des mots rares, des mots trop fréquents\n",
    "    - Construction de critère de séparabilité pour éliminer des mots etc...\n",
    "1. Apprentissage d'un classifieur\n",
    "    - Choix du type de classifieur\n",
    "    - Réglage des paramètres du classifieur (régularisation, etc...)\n",
    "\n",
    "## Exploitation de la chaine de traitement\n",
    "\n",
    "On appelle cette étape la réalisation d'une campagne d'expériences: c'est le point clé que nous voulons traviller en TAL cette année.\n",
    "1. Il est impossible de tester toutes les combinaisons par rapport aux propositions ci-dessus... Il faut donc en éliminer un certain nombre.\n",
    "    - En discutant avec les experts métiers\n",
    "    - En faisant des tests préliminaires\n",
    "1. Après ce premier filtrage, il faut:\n",
    "    - Choisir une évaluation fiable et pas trop lente (validation croisée, leave-one-out, split apprentissage/test simple)\n",
    "    - Lancer des expériences en grand\n",
    "        - = *grid-search*\n",
    "        - parallèliser sur plusieurs machines\n",
    "        - savoir lancer sur un serveur et se déconnecter\n",
    "1. Collecter et analyser les résultats\n",
    "\n",
    "\n",
    "## Inférence\n",
    "\n",
    "L'inférence est ensuite très classique: la chaine de traitement optimale est apte à traiter de nouveaux documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction de traitement des chaines de caractères\n",
    "\n",
    "Voici quelques fonctions de traitements des chaines de caractères. Il faut:\n",
    "1. Comprendre le fonctionnement des fonctions suivantes\n",
    "1. Les intégrer dans une ou plusieurs fonctions permettant d'activer/desactiver les traitements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le chat est devant la maison, 9 rue du zoo. Le chien attend à l'intérieur.\n",
      "Son site web préféré est www.spa.fr \n"
     ]
    }
   ],
   "source": [
    "doc = 'Le chat est devant la maison, 9 rue du zoo. Le chien attend à l\\'intérieur.\\nSon site web préféré est www.spa.fr '\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le chat est devant la maison  9 rue du zoo  Le chien attend à l intérieur  Son site web préféré est www spa fr \n"
     ]
    }
   ],
   "source": [
    "# récupération de la ponctuation\n",
    "import string\n",
    "\n",
    "punc = string.punctuation  # recupération de la ponctuation\n",
    "punc += '\\n\\r\\t'\n",
    "doc = doc.translate(str.maketrans(punc, ' ' * len(punc)))  \n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le chat est devant la maison  9 rue du zoo  le chien attend a l interieur  son site web prefere est www spa fr \n"
     ]
    }
   ],
   "source": [
    "# suppression des accents et des caractères non normalisés\n",
    "import unicodedata\n",
    "\n",
    "doc = unicodedata.normalize('NFD', doc).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "doc = doc.lower()\n",
    "print(doc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le chat est devant la maison   rue du zoo  le chien attend a l interieur  son site web prefere est www spa fr \n"
     ]
    }
   ],
   "source": [
    "# suppression des nombres\n",
    "import re\n",
    "doc = re.sub('[0-9]+', '', doc) # remplacer une séquence de chiffres par rien\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Division de la chaine de caractères, construction d'un dictionnaire, stockage dans une matrice\n",
    "\n",
    "J'utilise volontairement des structures de données qui doivent être nouvelles... Vous devez les comprendre a minima. N'hésitez pas à jeter un oeil sur la documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'le': 2, 'est': 2, 'chat': 1, 'devant': 1, 'la': 1, 'maison': 1, 'rue': 1, 'du': 1, 'zoo': 1, 'chien': 1, 'attend': 1, 'a': 1, 'l': 1, 'interieur': 1, 'son': 1, 'site': 1, 'web': 1, 'prefere': 1, 'www': 1, 'spa': 1, 'fr': 1})\n"
     ]
    }
   ],
   "source": [
    "# liste de mots\n",
    "mots = doc.split() # on peut choisir les séparateurs dans la tokenisation\n",
    "\n",
    "# comptage\n",
    "from collections import Counter\n",
    "\n",
    "dico = Counter(mots)\n",
    "print(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'le': 0, 'chat': 1, 'est': 2, 'devant': 3, 'la': 4, 'maison': 5, 'rue': 6, 'du': 7, 'zoo': 8, 'chien': 9, 'attend': 10, 'a': 11, 'l': 12, 'interieur': 13, 'son': 14, 'site': 15, 'web': 16, 'prefere': 17, 'www': 18, 'spa': 19, 'fr': 20}\n"
     ]
    }
   ],
   "source": [
    "# construction d'un mapping dictionnaire : mot => indice du mot dans le dictionnaire\n",
    "\n",
    "trans = dict(zip(list(dico.keys()), np.arange(len(dico)).tolist()))\n",
    "\n",
    "print(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# stockage dans une matrice \n",
    "\n",
    "d = np.zeros(len(trans))\n",
    "# remplissage\n",
    "for m in mots:\n",
    "    d[trans[m]] += 1\n",
    "    \n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t2.0\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t2.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "  (0, 6)\t1.0\n",
      "  (0, 7)\t1.0\n",
      "  (0, 8)\t1.0\n",
      "  (0, 9)\t1.0\n",
      "  (0, 10)\t1.0\n",
      "  (0, 11)\t1.0\n",
      "  (0, 12)\t1.0\n",
      "  (0, 13)\t1.0\n",
      "  (0, 14)\t1.0\n",
      "  (0, 15)\t1.0\n",
      "  (0, 16)\t1.0\n",
      "  (0, 17)\t1.0\n",
      "  (0, 18)\t1.0\n",
      "  (0, 19)\t1.0\n",
      "  (0, 20)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# passage à une matrice sparse\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "ds = coo_matrix(d)\n",
    "\n",
    "print(ds) # evidemment, pour l'instant le vecteur est plein, la démonstration est moins impressionnante!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-traitements statistiques\n",
    "\n",
    "A partir de l'objet dico, on peut éliminer les mots rares, les mots fréquents etc...\n",
    "\n",
    "A partir de la matrice de documents, on peut calculer les tf, idf et autres critères de contraste.\n",
    "\n",
    "1. Tracer des histogrammes de fréquence d'apparition des mots pour retrouver la loi de zipf\n",
    "<a href=\"https://en.wikipedia.org/wiki/Zipf%27s_law\">wikipedia</a>\n",
    "\n",
    "1. Prendre en main une librairie de word cloud pour l'affichage: ce n'est pas très scientifique... Mais ça permet de voir des choses intéressantes et c'est un outil dont vous aurez besoin en entreprise.\n",
    "Vous pourrez par exemple utiliser : \n",
    "<a href=\"https://github.com/amueller/word_cloud\">github</a> (qui est installable par pip)\n",
    "    \n",
    "1. Pour calculer une fréquence documentaire, il faut plusieurs documents: ajouter manuellement quelques entrées pour valider votre code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passage à une fonction automatique:\n",
    "\n",
    "La fonction ```sklearn.feature_extraction.text.CountVectorizer``` permet de réaliser les opérations précédentes automatiquement, avec même un certain nombre d'options supplémentaires:\n",
    "\n",
    "1. La documentation est disponible ici: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\"> lien </a>\n",
    "    - tester cette fonction sur vos données jouets pour comprendre son fonctionnement\n",
    "    - vérifier votre capacité à retrouver les mots associés à un indices\n",
    "    - vérifier le filtrage des mots peu fréquents ou très fréquents pour comprendre la signification des paramètres\n",
    "\n",
    "1. Parmi les options supplémentaires, étudiez la possibilité d'extraire des bi-grammes ou des tri-grammes de mots et visualiser le dictionnaire associé.\n",
    "\n",
    "1. Il est essentiel de distinguer la consitution du dictionnaire et l'exploitation du dictionnaire pour les données de test. Re-faire un dictionnaire pour les données de test mènerait inévitablemenet à une catastrophe: il faut faire attention à ne pas tomber dans le piège.\n",
    "    - séparer votre jeu de données jouet en deux\n",
    "    - constituer le dictionnaire sur les données d'apprentissage\n",
    "    - appliquer sur les données de test et vérifier que les indices d'un même mot sont bien identiques entre l'apprentissage et le test.\n",
    "    \n",
    "**ATTENTION** à ne pas confondre *fit_transform* = construction + usage du dictionnaire avec *transform* (usage seul). Il faut se poser cette question avec en tête la distinction apprentissage / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\\\n",
    "     'This is the first document.',\\\n",
    "     'This document is the second document.',\\\n",
    "     'And this is the third one.',\\\n",
    "     'Is this the first document?',\\\n",
    " ]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the']\n",
      "[[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())\n",
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prise en main de scikit-learn: les classifieurs\n",
    "\n",
    "L'architecture de scikit learn est objet: tous les classifieurs sont interchangeables... Ainsi que les procédures d'évaluations. Voici quelques lignes pour prendre en main les classifieurs qui nous intéressent dans l'UE.\n",
    "La base proposée est minimale et obligatoire... Mais rien ne vous empêche d'aller au-delà."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prédiction: [1]\n",
      "classifieur: [[1.48490813 1.30218492]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model as lin\n",
    "\n",
    "# données ultra basiques, à remplacer par vos corpus vectorisés\n",
    "N = 100\n",
    "X = np.random.randn(N,2) \n",
    "X[:int(N/2), :] += 2\n",
    "X[int(N/2):, :] -= 2\n",
    "y = np.array([1]*int(N/2) + [-1]*int(N/2))\n",
    "\n",
    "# SVM => Penser à utiliser des SVM linéaire !!!!\n",
    "clf = svm.LinearSVC()\n",
    "# Naive Bayes\n",
    "clf = nb.MultinomialNB()\n",
    "# regression logistique\n",
    "clf = lin.LogisticRegression()\n",
    "\n",
    "# apprentissage\n",
    "clf.fit(X, y)  \n",
    "yhat = clf.predict([[2., 2.]]) # usage sur une nouvelle donnée\n",
    "\n",
    "print(\"prédiction:\",yhat)\n",
    "print(\"classifieur:\",clf.coef_) # pour rentrer dans le classifieur... Depend évidemment du classifieur!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95 1.   1.   1.   1.  ]\n"
     ]
    }
   ],
   "source": [
    "# Solution 1 pour l'évaluation: validation croisée classique intégrée\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# usage en boucle implicite\n",
    "# le classifieur est donné en argument, tout ce fait implicitement (possibilité de paralléliser avec @@n_jobs@@)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1 -1  1 -1\n",
      "  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1 -1 -1 -1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vguigue/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# SOlution 2: simplifiée en train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# avec une graine pour la reproductibilité\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=0) \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Application \n",
    "yhat = clf.predict(X_test)\n",
    "print(yhat)\n",
    "\n",
    "# calcul d'indicateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73\n",
      " 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97\n",
      " 98 99] [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49] [50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73\n",
      " 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97\n",
      " 98 99]\n"
     ]
    }
   ],
   "source": [
    "# SOlution 3: validation croisée \"explicite\" avec accès aux classifieurs à chaque étape\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=2)\n",
    "for train, test in kf.split(X):\n",
    "    print(\"%s %s\" % (train, test))\n",
    "    X_train = X[train]\n",
    "    y_train = y[train]\n",
    "    X_test  = X[test]\n",
    "    y_test  = y[test]\n",
    "    # apprentissage\n",
    "    # evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
