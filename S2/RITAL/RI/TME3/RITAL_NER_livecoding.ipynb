{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RITAL_NER_livecoding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reconnaissance d'entités nommées avec un Bi-LSTM (pytorch)\n",
        "\n",
        "Avant toute chose, n'oubliez pas de choisir un environnement GPU dans Colab (`Exécution` $\\rightarrow$ `Modifier le type d'exécution`)\n",
        "\n",
        "Xavier Tannier"
      ],
      "metadata": {
        "id": "ODqoPoY0RP4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning --quiet\n",
        "!pip install torchmetrics --quiet"
      ],
      "metadata": {
        "id": "gapfzE_9ZcL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsKRdwGBREL-"
      },
      "outputs": [],
      "source": [
        "from os.path import isfile, isdir, join\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import autograd\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchmetrics\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from collections import Counter\n",
        "import codecs \n",
        "\n",
        "# Manual seed to ensure reproducibility\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connexion à la source de données"
      ],
      "metadata": {
        "id": "lGJGkT4wRbnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab & Drive libraries \n",
        "from google.colab import files\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import drive\n",
        "# Mount Google drive. This will prompt for authorization.\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "FNcXKqnIRL8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = '/content/drive/My Drive/data/conll/eng/train.txt'\n",
        "val_file = '/content/drive/My Drive/data/conll/eng/valid.txt'\n",
        "test_file = '/content/drive/My Drive/data/conll/eng/test.txt'\n",
        "\n",
        "# minimum frequency for a word to have its own embeddings\n",
        "min_word_freq = 2\n",
        "# Batch size\n",
        "batch_size = 64\n",
        "\n",
        "# how big is each word vector (if not preloaded)\n",
        "embed_size = 50 \n",
        "\n",
        "# how many times to iterate over all samples\n",
        "n_epochs = 15 \n",
        "\n",
        "# CPU workers\n",
        "workers = 1\n",
        "\n",
        "assert isfile(train_file)\n",
        "assert isfile(val_file)\n",
        "assert isfile(test_file)"
      ],
      "metadata": {
        "id": "7WXlKpUWRNar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture des fichiers au format IOB"
      ],
      "metadata": {
        "id": "nsQHNTWmRlcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_words_tags(file, tag_ind, caseless=True):\n",
        "    \"\"\"\n",
        "    Reads raw data in the CoNLL 2003 format and returns word and tag sequences.\n",
        "    :param file: file with raw data in the CoNLL 2003 format\n",
        "    :param tag_ind: column index of tag\n",
        "    :param caseless: lowercase words?\n",
        "    :return: word, tag sequences\n",
        "    \"\"\"\n",
        "    with codecs.open(file, 'r', 'utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    words = []\n",
        "    tags = []\n",
        "    temp_w = []\n",
        "    temp_t = []\n",
        "    for line in lines:\n",
        "        if not (line.isspace() or (len(line) > 10 and line[0:10] == '-DOCSTART-')):\n",
        "            feats = line.rstrip('\\n').split()\n",
        "            temp_w.append(feats[0].lower() if caseless else feats[0])\n",
        "            temp_t.append(feats[tag_ind])\n",
        "        elif len(temp_w) > 0:\n",
        "            assert len(temp_w) == len(temp_t)\n",
        "            words.append(temp_w)\n",
        "            tags.append(temp_t)\n",
        "            temp_w = []\n",
        "            temp_t = []\n",
        "    # last sentence\n",
        "    if len(temp_w) > 0:\n",
        "        assert len(temp_w) == len(temp_t)\n",
        "        words.append(temp_w)\n",
        "        tags.append(temp_t)\n",
        "\n",
        "    # Sanity check\n",
        "    assert len(words) == len(tags)\n",
        "\n",
        "    return words, tags\n"
      ],
      "metadata": {
        "id": "IZ2R11XyRW4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens, train_tags = read_words_tags(train_file,-1)\n",
        "assert len(train_tokens) == len(train_tags)"
      ],
      "metadata": {
        "id": "P7_k2EvlRosd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comptage de mots\n",
        "\n",
        "- Création d'un compteur de tokens\n",
        "- Création d'un dictionnaire de tokens (token --> identifiant)\n",
        "- Création d'un dictionnaire de labels (label --> identifiant)"
      ],
      "metadata": {
        "id": "U96J4KqoR434"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compteur de tokens\n",
        "word_freq = Counter()\n",
        "# Dictionnaire de labels\n",
        "tag_map = {}\n",
        "# Dictionnaire de tokens\n",
        "word_map = {}"
      ],
      "metadata": {
        "id": "6JZaDkJnRsd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder les phrases avec les identifiants des mots\n",
        "\n",
        "avec un token `<end>` à la fin de chaque phrase\n",
        "\n",
        "`[['dunston', 'checks', 'in', '<end>']]` -> `[[4670, 4670, 185, 4669]]`"
      ],
      "metadata": {
        "id": "NKolEwYASv73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode sentences into word maps with <end> at the end\n",
        "train_word_inputs = []\n"
      ],
      "metadata": {
        "id": "76qp29rISplo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder les listes de labels avec les identifiants des labels\n",
        "\n",
        "avec un token `<end>` à la fin"
      ],
      "metadata": {
        "id": "yVWqIKrkTMSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tag_inputs = []"
      ],
      "metadata": {
        "id": "f1Nj5zHxTJI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding\n",
        "\n",
        "Le padding transforme les phrases de tailles différentes en une matrice dans laquelle les phrases plus courtes sont complétées par des `0`."
      ],
      "metadata": {
        "id": "7BBul4Q_YgEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tailles et masques des phrases\n",
        "\n",
        "- Création d'une liste contenant la longueur de chaque phrase\n",
        "- Création des masques (pour application après le padding) : une matrice phrases/mots avec des `True` quand la case correspond à un vrai token dans la phrase, et `False` quand la phrase est terminée (padding)."
      ],
      "metadata": {
        "id": "7QOVLlTdTyit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sent_lengths = []\n"
      ],
      "metadata": {
        "id": "tNJf-SwbT3Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Création des matrices avec padding"
      ],
      "metadata": {
        "id": "u2aV1ge_Yv3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Nm1PXrdrYySA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Création du \"`Dataset`\"\n",
        "\n",
        "Un `Dataset` est un objet *pytorch* qui permet d'itérer sur les objets du jeu de données (phrases)."
      ],
      "metadata": {
        "id": "AhD6FiLeY3R2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NERDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, word_inputs, tag_inputs, sent_lengths, masks):\n",
        "        \"\"\"\n",
        "        :param word_inputs: padded word sequences\n",
        "        :param tag_inputs: padded tag sequences \n",
        "        :param sent_lengths: word sequence lengths\n",
        "        :param masks: masks\n",
        "        \"\"\"\n",
        "        self.word_inputs = word_inputs\n",
        "        self.tag_inputs = tag_inputs\n",
        "        self.sent_lengths = sent_lengths\n",
        "        self.masks = masks\n",
        "\n",
        "        self.data_size = len(self.word_inputs)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.word_inputs[i], self.tag_inputs[i], \\\n",
        "               self.sent_lengths[i], self.masks[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size"
      ],
      "metadata": {
        "id": "4EQKG04_Y59Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Création du `DataLoader`\n",
        "\n",
        "Le `DataLoader` est l'objet qui enveloppe le `Dataset` dans un mécanisme permettant de livrer des **mini-batchs** de données au modèle lors de l'entraînement."
      ],
      "metadata": {
        "id": "YGYPDZLYZlpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(NERDataset(train_word_inputs, train_tag_inputs, train_sent_lengths), \n",
        "                                           batch_size=batch_size, shuffle=True,\n",
        "                                           num_workers=workers, pin_memory=False)"
      ],
      "metadata": {
        "id": "f6zMS0-JZRhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Le modèle"
      ],
      "metadata": {
        "id": "c3H9cJG2apFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Sequence classification module\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, label_number,\n",
        "                 batch_size,\n",
        "                 hidden_size=100, dropout=0.5):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.automatic_optimization = True\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, \n",
        "                            batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classif = nn.Linear(2*hidden_size, label_number)\n",
        "\n",
        "        self.loss_fn= nn.NLLLoss(reduction='mean')\n",
        "\n",
        "        self.train_metrics = torchmetrics.MetricCollection({\n",
        "            'precision/train': torchmetrics.Precision(num_classes=label_number, average='macro'),\n",
        "            'recall/train': torchmetrics.Recall(num_classes=label_number, average='macro'),\n",
        "            'F1/train': torchmetrics.F1(num_classes=label_number, average='macro'),\n",
        "            'accuracy/train': torchmetrics.Accuracy()\n",
        "        })\n",
        "        self.val_metrics = torchmetrics.MetricCollection({\n",
        "            'precision/val': torchmetrics.Precision(num_classes=label_number, average='macro'),\n",
        "            'recall/val': torchmetrics.Recall(num_classes=label_number, average='macro'),\n",
        "            'F1/val': torchmetrics.F1(num_classes=label_number, average='macro'),\n",
        "            'accuracy/val': torchmetrics.Accuracy()\n",
        "        })\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "\n",
        "        #print('x', x.shape)\n",
        "\n",
        "        h_embedding = self.embedding(x)\n",
        "        #print('h_embedding', h_embedding.shape)\n",
        "        h_embedding = torch.nn.utils.rnn.pack_padded_sequence(h_embedding,\n",
        "                                                                lengths.cpu().numpy(),\n",
        "                                                                batch_first=True,\n",
        "                                                               enforce_sorted=False)\n",
        "        hidden = None\n",
        "        h_lstm, hidden = self.lstm(h_embedding, hidden)\n",
        "        output, input_sizes = torch.nn.utils.rnn.pad_packed_sequence(h_lstm, batch_first=True)  \n",
        "        #print('output', output.shape)\n",
        "\n",
        "        conc = output\n",
        "        conc = self.dropout(conc)\n",
        "        out = self.classif(conc)\n",
        "        #print('out', out.shape)\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # training_step defined the train loop.\n",
        "        # It is independent of forward\n",
        "        x, y, lengths, masks = batch\n",
        "        out = self(x, lengths)\n",
        "\n",
        "        pack_masks = torch.nn.utils.rnn.pack_padded_sequence(masks,\n",
        "                                                        lengths.cpu().numpy(),\n",
        "                                                        batch_first=True,\n",
        "                                                        enforce_sorted=False)\n",
        "        masks, _ = torch.nn.utils.rnn.pad_packed_sequence(pack_masks, batch_first=True)  \n",
        "        pack_y = torch.nn.utils.rnn.pack_padded_sequence(y,\n",
        "                                                        lengths.cpu().numpy(),\n",
        "                                                        batch_first=True,\n",
        "                                                        enforce_sorted=False)\n",
        "        y, _ = torch.nn.utils.rnn.pad_packed_sequence(pack_y, batch_first=True)  \n",
        "        masked_y = torch.masked_select(y, masks)\n",
        "        masked_out = out[masks] \n",
        "        score = F.log_softmax(masked_out, 1)\n",
        "        loss = self.loss_fn(score, masked_y)\n",
        "\n",
        "\n",
        "        _, preds  = torch.max(score, 1)\n",
        "        self.train_metrics(preds, masked_y)\n",
        "        return loss\n",
        "\n",
        "    def training_epoch_end(self, outs):\n",
        "        m = self.train_metrics.compute()\n",
        "        self.log_dict(m, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        print('train', m)\n",
        "        self.train_metrics.reset()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # training_step defined the train loop.\n",
        "        # It is independent of forward\n",
        "        x, y, lengths, masks = batch\n",
        "        out = self(x, lengths)\n",
        "\n",
        "        pack_masks = torch.nn.utils.rnn.pack_padded_sequence(masks,\n",
        "                                                        lengths.cpu().numpy(),\n",
        "                                                        batch_first=True,\n",
        "                                                        enforce_sorted=False)\n",
        "        masks, _ = torch.nn.utils.rnn.pad_packed_sequence(pack_masks, batch_first=True)  \n",
        "        pack_y = torch.nn.utils.rnn.pack_padded_sequence(y,\n",
        "                                                        lengths.cpu().numpy(),\n",
        "                                                        batch_first=True,\n",
        "                                                        enforce_sorted=False)\n",
        "        y, _ = torch.nn.utils.rnn.pad_packed_sequence(pack_y, batch_first=True)  \n",
        "\n",
        "        masked_y = torch.masked_select(y, masks)\n",
        "        masked_out = out[masks] \n",
        "        score = F.log_softmax(masked_out, 1)\n",
        "        loss = self.loss_fn(score, masked_y)\n",
        "        _, preds  = torch.max(score, 1)\n",
        "\n",
        "        self.val_metrics(preds, masked_y)\n",
        "        return loss\n",
        "\n",
        "    def validation_epoch_end(self, outs):\n",
        "        # log epoch metric\n",
        "        m = self.val_metrics.compute()\n",
        "        self.log_dict(m, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        print('val', m)\n",
        "        self.val_metrics.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=0.015) \n",
        "        return optimizer\n",
        "\n"
      ],
      "metadata": {
        "id": "o78_Px7aaqQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NsTsfIugaqx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}