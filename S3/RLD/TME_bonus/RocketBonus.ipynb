{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///mnt/c/Users/karna/Desktop/Projects/MasterArtificialIntelligencePTs/S3/RLD/TME_bonus/gym-rocketlander\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: gym in /home/hanzopgp/miniconda3/envs/deepdac/lib/python3.9/site-packages (from gym-rocketlander==0.0.1) (0.21.0)\n",
            "Requirement already satisfied: Box2D in /home/hanzopgp/miniconda3/envs/deepdac/lib/python3.9/site-packages (from gym-rocketlander==0.0.1) (2.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /home/hanzopgp/miniconda3/envs/deepdac/lib/python3.9/site-packages (from gym->gym-rocketlander==0.0.1) (1.23.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/hanzopgp/miniconda3/envs/deepdac/lib/python3.9/site-packages (from gym->gym-rocketlander==0.0.1) (2.2.0)\n",
            "Installing collected packages: gym-rocketlander\n",
            "  Attempting uninstall: gym-rocketlander\n",
            "    Found existing installation: gym-rocketlander 0.0.1\n",
            "    Uninstalling gym-rocketlander-0.0.1:\n",
            "      Successfully uninstalled gym-rocketlander-0.0.1\n",
            "  Running setup.py develop for gym-rocketlander\n",
            "Successfully installed gym-rocketlander-0.0.1\n"
          ]
        }
      ],
      "source": [
        "from easypip import easyimport\n",
        "import functools\n",
        "import time\n",
        "\n",
        "easyimport(\"importlib_metadata==4.13.0\")\n",
        "OmegaConf = easyimport(\"omegaconf\").OmegaConf\n",
        "bbrl_gym = easyimport(\"bbrl_gym\")\n",
        "bbrl = easyimport(\"bbrl>=0.1.6\")\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.distributions.independent import Independent\n",
        "\n",
        "import gym\n",
        "from bbrl.agents.agent import Agent\n",
        "from bbrl import get_arguments, get_class, instantiate_class\n",
        "from bbrl.workspace import Workspace\n",
        "from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n",
        "from bbrl.agents.gymb import AutoResetGymAgent, NoAutoResetGymAgent\n",
        "from bbrl.visu.play import load_agent, play\n",
        "from bbrl.utils.replay_buffer import ReplayBuffer\n",
        "from bbrl.utils.functionalb import gae\n",
        "\n",
        "# !rm -rf gym-rocketlander\n",
        "# !git clone https://github.com/EmbersArc/gym-rocketlander\n",
        "\n",
        "# !conda uninstall gym-rocketlander\n",
        "!pip install -e ./gym-rocketlander\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ROCKETLANDER ENV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pKR4bKBbQ8Wo"
      },
      "outputs": [],
      "source": [
        "class RocketLanderWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Specific wrapper to shape the reward of the rocket lander environment\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(RocketLanderWrapper, self).__init__(env)\n",
        "        self.env = env\n",
        "        self.prev_shaping = None\n",
        "        \n",
        "    def reset(self):\n",
        "        self.prev_shaping = None\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        d = 1\n",
        "        next_state, reward, done, info = self.env.step(action)\n",
        "        # reward shaping\n",
        "        \"\"\"\n",
        "        shaping = -0.5 * (self.env.distance + self.env.speed + abs(self.env.angle) ** 2)\n",
        "        shaping += 0.1 * (\n",
        "            self.env.legs[0].ground_contact + self.env.legs[1].ground_contact\n",
        "        )\n",
        "        if self.prev_shaping is not None:\n",
        "            reward += shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "        \"\"\"\n",
        "        # print (\"distance\", self.env.distance)\n",
        "        \n",
        "        # shaping = 0.02\n",
        "        shaping = 0.008 * (1 - self.env.distance)\n",
        "        # shaping = 0.1 * (self.env.groundcontact - self.env.speed)\n",
        "        if (\n",
        "            self.env.legs[0].ground_contact > 0\n",
        "            and self.env.legs[1].ground_contact > 0\n",
        "            and self.env.speed < 0.1\n",
        "        ):\n",
        "            d = d * 2\n",
        "            print(\"landed !\")\n",
        "            print (\"speed\", self.env.speed)\n",
        "            shaping += 6.0 * d / self.env.speed\n",
        "        else:\n",
        "          d = 1\n",
        "        reward += shaping\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def old_step(self, action):\n",
        "        next_state, reward, done, info = self.env.step(action)\n",
        "        # reward shaping\n",
        "        # shaping = -0.5 * (self.env.distance + self.env.speed + abs(self.env.angle) ** 2)\n",
        "        # shaping += 0.1 * (self.env.legs[0].ground_contact + self.env.legs[1].ground_contact)\n",
        "        shaping = 0\n",
        "        if self.prev_shaping is not None:\n",
        "            reward += shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "\n",
        "\n",
        "class FrameSkip(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Return only every ``skip``-th frame (frameskipping)\n",
        "    :param env: the environment\n",
        "    :param skip: number of ``skip``-th frame\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env: gym.Env, skip: int = 1):\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action: np.ndarray):\n",
        "        \"\"\"\n",
        "        Step the environment with the given action\n",
        "        Repeat action, sum reward, and max over last observations.\n",
        "        :param action: the action\n",
        "        :return: observation, reward, done, information\n",
        "        \"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The observation space: Box([ -1.  -1.  -1.  -1.  -1.  -1.  -1. -inf -inf -inf], [ 1.  1.  1.  1.  1.  1.  1. inf inf inf], (10,), float32)\n",
            "The action space: Box([-1. -1. -1.], [1. 1. 1.], (3,), float32)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"RocketLander-v0\")\n",
        "env = RocketLanderWrapper(env)\n",
        "env = FrameSkip(env, skip=1)\n",
        "obs_space = env.observation_space\n",
        "action_space = env.action_space\n",
        "print(\"The observation space: {}\".format(obs_space))\n",
        "print(\"The action space: {}\".format(action_space))\n",
        "\n",
        "# env = gym.make(\"LunarLander-v2\")\n",
        "# obs_space = env.observation_space\n",
        "# action_space = env.action_space\n",
        "# print(\"The observation space: {}\".format(obs_space))\n",
        "# print(\"The action space: {}\".format(action_space))\n",
        "\n",
        "# env = gym.make(\"LunarLanderContinuous-v2\")\n",
        "# obs_space = env.observation_space\n",
        "# action_space = env.action_space\n",
        "# print(\"The observation space: {}\".format(obs_space))\n",
        "# print(\"The action space: {}\".format(action_space))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GENERAL FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n",
        "    layers = []\n",
        "    for j in range(len(sizes) - 1):\n",
        "        act = activation if j < len(sizes) - 2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def make_gym_env(env_name):\n",
        "\trocket_env = gym.make(env_name)\n",
        "\trocket_env = RocketLanderWrapper(rocket_env)\n",
        "\treturn FrameSkip(rocket_env, skip=1)\n",
        "\n",
        "def get_env_agents(cfg):\n",
        "    train_env_agent = AutoResetGymAgent(\n",
        "        get_class(cfg.gym_env),\n",
        "        get_arguments(cfg.gym_env),\n",
        "        cfg.algorithm.n_envs,\n",
        "        cfg.algorithm.seed,\n",
        "    )\n",
        "    eval_env_agent = NoAutoResetGymAgent(\n",
        "    get_class(cfg.gym_env),\n",
        "    get_arguments(cfg.gym_env),\n",
        "    cfg.algorithm.nb_evals,\n",
        "    cfg.algorithm.seed,\n",
        "    )\n",
        "    return train_env_agent, eval_env_agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LOGGER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Logger():\n",
        "  def __init__(self, cfg, log_string):\n",
        "    self.logger = instantiate_class(cfg.logger)\n",
        "\n",
        "  def add_log(self, log_string, loss, epoch):\n",
        "    self.logger.add_scalar(log_string, loss.item(), epoch)\n",
        "\n",
        "  def add_q_norms(self, q_norm, target_q_norm, epoch):\n",
        "    # my_summary_writer.add_scalars(f'loss/check_info', {\n",
        "    #                               'score': score[iteration],\n",
        "    #                               'score_nf': score_nf[iteration],\n",
        "    #                           }, iteration)\n",
        "    # self.logger.add_scalar(log_string, q_norm, epoch)\n",
        "\n",
        "    # self.logger.add_scalar(f'q_value/q_norm',    q_norm,    epoch)\n",
        "    # self.logger.add_scalar(f'q_value/target_q_norm', target_q_norm, epoch)\n",
        "\n",
        "    self.logger.add_scalars(f'q_value', {\n",
        "        'q_norm': q_norm,\n",
        "        'target_q_norm': target_q_norm,\n",
        "    }, epoch)\n",
        "    \n",
        "  def add_q_norms2(self, q_norm1, q_norm2, target, epoch):\n",
        "    self.logger.add_scalars(f'q_value', {\n",
        "        'q_norm1': q_norm1,\n",
        "        'q_norm2': q_norm2,\n",
        "        'target_q_norm': target,\n",
        "    }, epoch)\n",
        "\n",
        "  # def log_losses(self, cfg, epoch, critic_loss, entropy_loss, a2c_loss):\n",
        "  #   self.add_log(\"critic_loss\", critic_loss, epoch)\n",
        "  #   self.add_log(\"entropy_loss\", entropy_loss, epoch)\n",
        "  #   self.add_log(\"a2c_loss\", a2c_loss, epoch)\n",
        "\n",
        "  def log_losses(self, epoch, critic_loss, entropy_loss, actor_loss):\n",
        "    self.add_log(\"critic_loss\", critic_loss, epoch)\n",
        "    self.add_log(\"entropy_loss\", entropy_loss, epoch)\n",
        "    self.add_log(\"actor_loss\", actor_loss, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseActor(Agent):\n",
        "    def copy_parameters(self, other):\n",
        "        \"\"\"Copy parameters from other agent\"\"\"\n",
        "        for self_p, other_p in zip(self.parameters(), other.parameters()):\n",
        "            self_p.data.copy_(other_p)\n",
        "\n",
        "        \n",
        "\n",
        "class DiscreteActor(BaseActor):\n",
        "    def __init__(self, state_dim, hidden_size, n_actions):\n",
        "        super().__init__()\n",
        "        self.model = build_mlp(\n",
        "            [state_dim] + list(hidden_size) + [n_actions], activation=nn.ReLU()\n",
        "        )\n",
        "        \n",
        "    def dist(self, obs):\n",
        "        scores = self.model(obs)\n",
        "        probs = torch.softmax(scores, dim=-1)\n",
        "        return torch.distributions.Categorical(probs)\n",
        "\n",
        "    def forward(self, t, *, stochastic=True, predict_proba=False, compute_entropy=False, **kwargs):\n",
        "        \"\"\"\n",
        "        Compute the action given either a time step (looking into the workspace)\n",
        "        or an observation (in kwargs)\n",
        "        \"\"\"\n",
        "        if \"observation\" in kwargs:\n",
        "            observation = kwargs[\"observation\"]\n",
        "        else:\n",
        "            observation = self.get((\"env/env_obs\", t))\n",
        "        scores = self.model(observation)\n",
        "        probs = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        if predict_proba:\n",
        "            action = self.get((\"action\", t))\n",
        "            log_prob = probs[torch.arange(probs.size()[0]), action].log()\n",
        "            self.set((\"logprob_predict\", t), log_prob)\n",
        "        else:\n",
        "            if stochastic:\n",
        "                action = torch.distributions.Categorical(probs).sample()\n",
        "            else:\n",
        "                action = scores.argmax(1)\n",
        "\n",
        "            log_probs = probs[torch.arange(probs.size()[0]), action].log()\n",
        "\n",
        "            self.set((\"action\", t), action)\n",
        "            self.set((\"action_logprobs\", t), log_probs)\n",
        "\n",
        "        if compute_entropy:\n",
        "            entropy = torch.distributions.Categorical(probs).entropy()\n",
        "            self.set((\"entropy\", t), entropy)\n",
        "\n",
        "    def predict_action(self, obs, stochastic):\n",
        "        scores = self.model(obs)\n",
        "\n",
        "        if stochastic:\n",
        "            probs = torch.softmax(scores, dim=-1)\n",
        "            action = torch.distributions.Categorical(probs).sample()\n",
        "        else:\n",
        "            action = scores.argmax(0)\n",
        "        return action\n",
        "\n",
        "\n",
        "\n",
        "class TunableVarianceContinuousActor(BaseActor):\n",
        "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
        "        super().__init__()\n",
        "        layers = [state_dim] + list(hidden_layers) + [action_dim]\n",
        "        self.model = build_mlp(layers, activation=nn.ReLU())\n",
        "\n",
        "        # The standard deviation associated with each dimension\n",
        "        self.std_param = nn.parameter.Parameter(torch.randn(action_dim, 1))\n",
        "        \n",
        "        # We use the softplus function to compute the variance for the normal\n",
        "        # The base version computes exp(1+log(x)) component-wise\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html\n",
        "        self.soft_plus = torch.nn.Softplus()\n",
        "\n",
        "    def dist(self, obs: torch.Tensor):\n",
        "        mean = self.model(obs)\n",
        "        return Independent(Normal(mean, self.soft_plus(self.std_param)), 1)    \n",
        "            \n",
        "    def forward(self, t, *, stochastic=True, predict_proba=False, compute_entropy=False, **kwargs):\n",
        "        obs = self.get((\"env/env_obs\", t))\n",
        "        dist = self.dist(obs)\n",
        "\n",
        "        if predict_proba:\n",
        "            action = self.get((\"action\", t))\n",
        "            self.set((\"logprob_predict\", t), dist.log_prob(action))\n",
        "        else:\n",
        "            action = dist.sample() if stochastic else dist.mean\n",
        "            logp_pi = dist.log_prob(action)\n",
        "\n",
        "            self.set((\"action\", t), action)\n",
        "            self.set((\"action_logprobs\", t), logp_pi)\n",
        "\n",
        "        if compute_entropy:\n",
        "            self.set((\"entropy\", t), dist.entropy())\n",
        "\n",
        "    def predict_action(self, obs, stochastic):\n",
        "        \"\"\"Predict just one action (without using the workspace)\"\"\"\n",
        "        dist = self.dist(obs)\n",
        "        action = dist.sample() if stochastic else dist.mean\n",
        "        return action\n",
        "\n",
        "\n",
        "\n",
        "class VAgent(Agent):\n",
        "    def __init__(self, state_dim, hidden_layers):\n",
        "        super().__init__()\n",
        "        self.is_q_function = False\n",
        "        self.model = build_mlp(\n",
        "            [state_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, t, **kwargs):\n",
        "        observation = self.get((\"env/env_obs\", t))\n",
        "        critic = self.model(observation).squeeze(-1)\n",
        "        self.set((\"v_value\", t), critic)\n",
        "\n",
        "\n",
        "\n",
        "class KLAgent(Agent):\n",
        "    def __init__(self, model_1, model_2):\n",
        "        super().__init__()\n",
        "        self.model_1 = model_1\n",
        "        self.model_2 = model_2\n",
        "\n",
        "    def forward(self, t, **kwargs):\n",
        "        obs = self.get((\"env/env_obs\", t))\n",
        "        \n",
        "        dist_1 = self.model_1.dist(obs)\n",
        "        dist_2 = self.model_2.dist(obs)\n",
        "        kl = torch.distributions.kl.kl_divergence(dist_1, dist_2)\n",
        "        self.set((\"kl\", t), kl)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_ppo_agent(cfg, train_env_agent, eval_env_agent, needs_kl=None):\n",
        "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n",
        "\n",
        "    if train_env_agent.is_continuous_action():\n",
        "        action_agent = TunableVarianceContinuousActor(\n",
        "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
        "        )\n",
        "    else:\n",
        "        action_agent = DiscreteActor(\n",
        "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
        "        )\n",
        "\n",
        "    tr_agent = Agents(train_env_agent, action_agent)\n",
        "    ev_agent = Agents(eval_env_agent, action_agent)\n",
        "\n",
        "    critic_agent = TemporalAgent(\n",
        "        VAgent(obs_size, cfg.algorithm.architecture.critic_hidden_size)\n",
        "    )\n",
        "\n",
        "    train_agent = TemporalAgent(tr_agent)\n",
        "    eval_agent = TemporalAgent(ev_agent)\n",
        "    train_agent.seed(cfg.algorithm.seed)\n",
        "\n",
        "    old_policy = copy.deepcopy(action_agent)\n",
        "    old_critic_agent = copy.deepcopy(critic_agent)\n",
        "    \n",
        "    kl_agent = None\n",
        "    if needs_kl:\n",
        "        kl_agent = TemporalAgent(KLAgent(old_policy, action_agent))\n",
        "\n",
        "    return action_agent, train_agent, eval_agent, critic_agent, old_policy, old_critic_agent, kl_agent\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_advantage_loss(cfg, reward, must_bootstrap, v_value):\n",
        "    # Compute temporal difference with GAE\n",
        "    advantage = gae(\n",
        "        v_value,\n",
        "        reward,\n",
        "        must_bootstrap,\n",
        "        cfg.algorithm.discount_factor,\n",
        "        cfg.algorithm.gae,\n",
        "    )\n",
        "    # Compute critic loss\n",
        "    td_error = advantage**2\n",
        "    critic_loss = td_error.mean()\n",
        "    return critic_loss, advantage\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_clip_agent_loss(cfg, advantage, ratio, kl):\n",
        "    \"\"\"Computes the PPO CLIP loss\n",
        "    \"\"\"\n",
        "    clip_range = cfg.clip_range\n",
        "    actor_loss = torch.min(\n",
        "        advantage * ratio,\n",
        "        advantage * torch.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
        "    ).mean()\n",
        "    return actor_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def setup_optimizer(cfg, action_agent, critic_agent):\n",
        "    optimizer_args = get_arguments(cfg.optimizer)\n",
        "    parameters = nn.Sequential(action_agent, critic_agent).parameters()\n",
        "    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_ppo(cfg, log_string, variant=\"clip\", compute_actor_loss=compute_clip_agent_loss, needs_kl=False, verbose=False):\n",
        "    # 1)  Build the  logger\n",
        "    logger = Logger(cfg, log_string)\n",
        "    best_reward = -10e9\n",
        "\n",
        "    # 2) Create the environment agent\n",
        "    train_env_agent = AutoResetGymAgent(\n",
        "        get_class(cfg.gym_env),\n",
        "        get_arguments(cfg.gym_env),\n",
        "        cfg.algorithm.n_envs,\n",
        "        cfg.algorithm.seed,\n",
        "    )\n",
        "    \n",
        "    eval_env_agent = NoAutoResetGymAgent(\n",
        "        get_class(cfg.gym_env),\n",
        "        get_arguments(cfg.gym_env),\n",
        "        cfg.algorithm.nb_evals,\n",
        "        cfg.algorithm.seed,\n",
        "    )\n",
        "\n",
        "    (\n",
        "        policy,\n",
        "        train_agent,\n",
        "        eval_agent,\n",
        "        critic_agent,\n",
        "        old_policy,\n",
        "        old_critic_agent,\n",
        "        kl_agent\n",
        "    ) = create_ppo_agent(cfg, train_env_agent, eval_env_agent, needs_kl=needs_kl)\n",
        "    \n",
        "    \n",
        "    action_agent = TemporalAgent(policy)\n",
        "    old_train_agent = TemporalAgent(old_policy)\n",
        "    train_workspace = Workspace()\n",
        "\n",
        "    # Configure the optimizer\n",
        "    optimizer = setup_optimizer(cfg, train_agent, critic_agent)\n",
        "    nb_steps = 0\n",
        "    tmp_steps = 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(cfg.algorithm.max_epochs)):\n",
        "        # Execute the agent in the workspace\n",
        "        \n",
        "        # Handles continuation\n",
        "        delta_t = 0\n",
        "        if epoch > 0:\n",
        "            train_workspace.zero_grad()\n",
        "            delta_t = 1\n",
        "            train_workspace.copy_n_last_steps(delta_t)\n",
        "\n",
        "        # Run the train/old_train agents\n",
        "        train_agent(\n",
        "            train_workspace,\n",
        "            t=delta_t,\n",
        "            n_steps=cfg.algorithm.n_steps - delta_t,\n",
        "            stochastic=True,\n",
        "            predict_proba=False,\n",
        "            compute_entropy=False\n",
        "        )\n",
        "        old_train_agent(\n",
        "            train_workspace,\n",
        "            t=delta_t,\n",
        "            n_steps=cfg.algorithm.n_steps - delta_t,\n",
        "            # Just computes the probability\n",
        "            predict_proba=True,\n",
        "        )\n",
        "\n",
        "        # Compute the critic value over the whole workspace\n",
        "        critic_agent(train_workspace, n_steps=cfg.algorithm.n_steps)\n",
        "\n",
        "        transition_workspace = train_workspace.get_transitions()\n",
        "        done, truncated, reward, action, action_logp, v_value = transition_workspace[\n",
        "            \"env/done\",\n",
        "            \"env/truncated\",\n",
        "            \"env/reward\",\n",
        "            \"action\",\n",
        "            \"action_logprobs\",\n",
        "            \"v_value\",\n",
        "        ]\n",
        "\n",
        "        nb_steps += action[0].shape[0]\n",
        "\n",
        "        # Determines whether values of the critic should be propagated\n",
        "        # True if the episode reached a time limit or if the task was not done\n",
        "        # See https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing\n",
        "        must_bootstrap = torch.logical_or(~done[1], truncated[1])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            old_critic_agent(train_workspace, n_steps=cfg.algorithm.n_steps)\n",
        "        old_action_logp = transition_workspace[\"logprob_predict\"].detach()\n",
        "        old_v_value = transition_workspace[\"v_value\"]\n",
        "        if cfg.algorithm.clip_range_vf > 0:\n",
        "            # Clip the difference between old and new values\n",
        "            # NOTE: this depends on the reward scaling\n",
        "            v_value = old_v_value + torch.clamp(\n",
        "                v_value - old_v_value,\n",
        "                -cfg.algorithm.clip_range_vf,\n",
        "                cfg.algorithm.clip_range_vf,\n",
        "            )\n",
        "            \n",
        "        critic_loss, advantage = compute_advantage_loss(\n",
        "            cfg, reward, must_bootstrap, v_value\n",
        "        )\n",
        "        \n",
        "        # We store the advantage into the transition_workspace\n",
        "        advantage = advantage.detach().squeeze(0)\n",
        "        transition_workspace.set(\"advantage\", 0, advantage)\n",
        "        transition_workspace.set(\"advantage\", 1, torch.zeros_like(advantage))\n",
        "        transition_workspace.set_full(\"old_action_logprobs\", transition_workspace[\"logprob_predict\"].detach())\n",
        "        transition_workspace.clear(\"logprob_predict\")\n",
        "    \n",
        "        for opt_epoch in range(cfg.algorithm.opt_epochs):\n",
        "            if cfg.algorithm.minibatch_size > 0:\n",
        "                sample_workspace = transition_workspace.sample_subworkspace(1, cfg.algorithm.minibatch_size, 2)\n",
        "            else:\n",
        "                sample_workspace = transition_workspace\n",
        "                                 \n",
        "            if opt_epoch > 0:\n",
        "                critic_loss = 0. # We don't want to optimize the critic after the first mini-epoch\n",
        "\n",
        "            action_agent(sample_workspace, t=0, n_steps=1, compute_entropy=True, predict_proba=True)\n",
        "\n",
        "            advantage, action_logp, old_action_logp, entropy = sample_workspace[\n",
        "                \"advantage\",\n",
        "                \"logprob_predict\",\n",
        "                \"old_action_logprobs\",\n",
        "                \"entropy\"\n",
        "            ]\n",
        "            advantage = advantage[0]\n",
        "            act_diff = action_logp[0] - old_action_logp[0]\n",
        "            ratios = act_diff.exp()\n",
        "\n",
        "            kl = None\n",
        "            if kl_agent:\n",
        "                kl_agent(sample_workspace, t=0, n_steps=1)\n",
        "                kl = sample_workspace[\"kl\"][0]\n",
        "\n",
        "            actor_loss = compute_actor_loss(\n",
        "                cfg.algorithm, advantage, ratios, kl\n",
        "            )\n",
        "                            \n",
        "            # Entropy loss favor exploration\n",
        "            entropy_loss = torch.mean(entropy[0])\n",
        "\n",
        "            # Store the losses for tensorboard display\n",
        "            if opt_epoch == 0:\n",
        "                # Just for the first epoch\n",
        "                logger.log_losses(nb_steps, critic_loss, entropy_loss, actor_loss)\n",
        "\n",
        "            loss = (\n",
        "                cfg.algorithm.critic_coef * critic_loss\n",
        "                - cfg.algorithm.actor_coef * actor_loss\n",
        "                - cfg.algorithm.entropy_coef * entropy_loss\n",
        "            )\n",
        "            \n",
        "\n",
        "            old_policy.copy_parameters(policy)\n",
        "            old_critic_agent = copy.deepcopy(critic_agent)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                critic_agent.parameters(), cfg.algorithm.max_grad_norm\n",
        "            )\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                train_agent.parameters(), cfg.algorithm.max_grad_norm\n",
        "            )\n",
        "            optimizer.step() \n",
        "\n",
        "        # Evaluate if enough steps have been performed\n",
        "        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n",
        "            tmp_steps = nb_steps\n",
        "            eval_workspace = Workspace()  # Used for evaluation\n",
        "            eval_agent(\n",
        "                eval_workspace,\n",
        "                t=0,\n",
        "                stop_variable=\"env/done\",\n",
        "                stochastic=True,\n",
        "                predict_proba=False,\n",
        "            )\n",
        "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
        "            mean = rewards.mean()\n",
        "            logger.add_log(\"reward_mean\", mean, nb_steps)\n",
        "            logger.add_log(\"reward_max\", rewards.max(), nb_steps)\n",
        "            logger.add_log(\"reward_min\", rewards.min(), nb_steps)\n",
        "            logger.add_log(\"reward_std\", rewards.std(), nb_steps)\n",
        "            if verbose: print(f\"nb_steps: {nb_steps}, reward: {mean}\")\n",
        "            if cfg.save_best and mean > best_reward:\n",
        "                best_reward = mean\n",
        "                directory = f\"./ppo_agent/{cfg.gym_env.env_name}/{variant}/\"\n",
        "                if not os.path.exists(directory):\n",
        "                    os.makedirs(directory)\n",
        "                filename = directory + \"ppo_\" + str(mean.item()) + \".agt\"\n",
        "                policy.save_model(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZZnmiEiQ8bi"
      },
      "source": [
        "# SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbIL63_tQ8gm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUJjPFppYCje"
      },
      "source": [
        "# ROCKETLANDER / PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENV = \"RocketLander-v0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "FfNk_iNKYCsS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%tensorboard` not found.\n"
          ]
        }
      ],
      "source": [
        "# %reload_ext tensorboard\n",
        "%tensorboard --logdir ./runs/rocketlander/discrete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "E69CMfNwYCuf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--> Testing HP\n",
            "       gae_lambda: 0.9\n",
            "       gamma: 0.95\n",
            "       arch: [64, 64]\n",
            "       lr: 0.0001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (8) must match the size of tensor b (3) at non-singleton dimension 0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [32], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m       lr:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lr)\n\u001b[1;32m     67\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 68\u001b[0m run_ppo(config_ppo, log_string, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--> Done in:\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn [29], line 265\u001b[0m, in \u001b[0;36mrun_ppo\u001b[0;34m(cfg, log_string, variant, compute_actor_loss, needs_kl, verbose)\u001b[0m\n\u001b[1;32m    262\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mcopy_n_last_steps(delta_t)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# Run the train/old_train agents\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_workspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelta_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelta_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstochastic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_entropy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m old_train_agent(\n\u001b[1;32m    274\u001b[0m     train_workspace,\n\u001b[1;32m    275\u001b[0m     t\u001b[38;5;241m=\u001b[39mdelta_t,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m     predict_proba\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m )\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Compute the critic value over the whole workspace\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/utils.py:79\u001b[0m, in \u001b[0;36mTemporalAgent.__call__\u001b[0;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m _t \u001b[39m=\u001b[39m t\n\u001b[1;32m     78\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent(workspace, t\u001b[39m=\u001b[39;49m_t, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m     \u001b[39mif\u001b[39;00m stop_variable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         s \u001b[39m=\u001b[39m workspace\u001b[39m.\u001b[39mget(stop_variable, _t)\n",
            "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/utils.py:31\u001b[0m, in \u001b[0;36mAgents.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, workspace, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents:\n\u001b[0;32m---> 31\u001b[0m         a(workspace, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/agent.py:64\u001b[0m, in \u001b[0;36mAgent.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39massert\u001b[39;00m workspace \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39m[Agent.__call__] workspace must not be None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkspace \u001b[39m=\u001b[39m workspace\n\u001b[0;32m---> 64\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkspace \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn [29], line 84\u001b[0m, in \u001b[0;36mTunableVarianceContinuousActor.forward\u001b[0;34m(self, t, stochastic, predict_proba, compute_entropy, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, \u001b[38;5;241m*\u001b[39m, stochastic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, predict_proba\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, compute_entropy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     83\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv/env_obs\u001b[39m\u001b[38;5;124m\"\u001b[39m, t))\n\u001b[0;32m---> 84\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m predict_proba:\n\u001b[1;32m     87\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m, t))\n",
            "Cell \u001b[0;32mIn [29], line 80\u001b[0m, in \u001b[0;36mTunableVarianceContinuousActor.dist\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdist\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     79\u001b[0m     mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(obs)\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Independent(\u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoft_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd_param\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/torch/distributions/normal.py:49\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, loc, scale, validate_args\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale \u001b[39m=\u001b[39m broadcast_all(loc, scale)\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, Number) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(scale, Number):\n\u001b[1;32m     51\u001b[0m         batch_shape \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mSize()\n",
            "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/torch/distributions/utils.py:40\u001b[0m, in \u001b[0;36mbroadcast_all\u001b[0;34m(*values)\u001b[0m\n\u001b[1;32m     37\u001b[0m     new_values \u001b[39m=\u001b[39m [v \u001b[39mif\u001b[39;00m is_tensor_like(v) \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(v, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m     38\u001b[0m                   \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m values]\n\u001b[1;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39m*\u001b[39mnew_values)\n\u001b[0;32m---> 40\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39m*\u001b[39;49mvalues)\n",
            "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/torch/functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> 73\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (3) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "# n_envss = [8, 16]\n",
        "gae_lambdas = [0.90, 0.94, 0.98]\n",
        "gammas = [0.95, 0.99, 0.999]\n",
        "# ent_coefs = [1e-4, 1e-3, 1e-2]\n",
        "archs = [[64, 64], [128, 128]]\n",
        "lrs = [1e-4, 1e-3]\n",
        "\n",
        "for gae_lambda in gae_lambdas:\n",
        "\tfor gamma in gammas:\n",
        "\t\tfor arch in archs:\n",
        "\t\t\tfor lr in lrs:\n",
        "\t\t\t\tlog_string = \"PPO_gae_lambda_\"+str(gae_lambda)+\"_gamma_\"+str(gamma)+\"_arch_\"+str(arch)+\"_lr_\"+str(lr)\n",
        "\n",
        "\t\t\t\tparams_ppo={\n",
        "\t\t\t\t\"save_best\": True,\n",
        "\t\t\t\t\"plot_policy\": True,\n",
        "\n",
        "\t\t\t\t\"logger\":{\n",
        "\t\t\t\t\t\"classname\": \"bbrl.utils.logger.TFLogger\",\n",
        "\t\t\t\t\t\"log_dir\": f\"{os.getcwd()}/runs/{log_string}\",\n",
        "\t\t\t\t\t\"cache_size\": 10000,\n",
        "\t\t\t\t\t\"every_n_seconds\": 10,\n",
        "\t\t\t\t\t\"verbose\": False,    \n",
        "\t\t\t\t\t},\n",
        "\n",
        "\t\t\t\t\"algorithm\":{\n",
        "\t\t\t\t\t\"seed\": 4,\n",
        "\t\t\t\t\t\"n_envs\": 8,\n",
        "\t\t\t\t\t\"max_grad_norm\": 0.5,\n",
        "\t\t\t\t\t\"nb_evals\":10,\n",
        "\t\t\t\t\t\"n_steps\": 20,\n",
        "\t\t\t\t\t\"eval_interval\": 1000,\n",
        "\t\t\t\t\t\"max_epochs\": 3000, # 3000\n",
        "\t\t\t\t\t\"discount_factor\": gamma,\n",
        "\t\t\t\t\t\"entropy_coef\": 2.55e-5,\n",
        "\t\t\t\t\t\"beta_kl\": 1,\n",
        "\t\t\t\t\t\"critic_coef\": 0.6,\n",
        "\t\t\t\t\t\"actor_coef\": 1.0,\n",
        "\t\t\t\t\t\"gae\": gae_lambda,\n",
        "\t\t\t\t\t\"clip_range\": 0.2,\n",
        "\t\t\t\t\t\"clip_range_vf\": 0,\n",
        "\t\t\t\t\t\"opt_epochs\": 1,\n",
        "\t\t\t\t\t\"minibatch_size\": 0,\n",
        "\t\t\t\t\t\"architecture\":{\n",
        "\t\t\t\t\t\"actor_hidden_size\": arch,\n",
        "\t\t\t\t\t\"critic_hidden_size\": arch,\n",
        "\t\t\t\t\t},\n",
        "\t\t\t\t},\n",
        "\t\t\t\t\"gym_env\":{\n",
        "\t\t\t\t\t\"classname\": \"__main__.make_gym_env\",\n",
        "\t\t\t\t\t\"env_name\": ENV,\n",
        "\t\t\t\t\t},\n",
        "\t\t\t\t\"optimizer\":{\n",
        "\t\t\t\t\t\"classname\": \"torch.optim.Adam\",\n",
        "\t\t\t\t\t\"lr\": lr,\n",
        "\t\t\t\t}\n",
        "\t\t\t\t}\n",
        "\n",
        "\t\t\t\tconfig_ppo=OmegaConf.create(params_ppo)\n",
        "\t\t\t\ttorch.manual_seed(config_ppo.algorithm.seed)\n",
        "\n",
        "\t\t\t\tprint(\"--> Testing HP\")\n",
        "\t\t\t\tprint(\"       gae_lambda:\", gae_lambda)\n",
        "\t\t\t\tprint(\"       gamma:\", gamma)\n",
        "\t\t\t\tprint(\"       arch:\", arch)\n",
        "\t\t\t\tprint(\"       lr:\", lr)\n",
        "\t\t\t\tstart = time.time()\n",
        "\t\t\t\trun_ppo(config_ppo, log_string, verbose=False)\n",
        "\t\t\t\tprint(\"--> Done in:\", time.time() - start, \"s\")\n",
        "\n",
        "# 36 fits, 15min per fits --> roughly 9 hours run"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('deepdac')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "9bf73cbd865082e3cd06c05babb8829fd907c8e55c85361e30fff74a8577746e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
