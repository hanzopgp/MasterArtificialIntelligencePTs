{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///home/hanzopgp/projects/MasterArtificialIntelligencePTs/S3/RLD/TME_bonus/gym-rocketlander\n",
            "\u001b[31mERROR: file:///home/hanzopgp/projects/MasterArtificialIntelligencePTs/S3/RLD/TME_bonus/gym-rocketlander does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "from easypip import easyimport\n",
        "import functools\n",
        "import time\n",
        "\n",
        "easyimport(\"importlib_metadata==4.13.0\")\n",
        "OmegaConf = easyimport(\"omegaconf\").OmegaConf\n",
        "bbrl_gym = easyimport(\"bbrl_gym\")\n",
        "bbrl = easyimport(\"bbrl>=0.1.6\")\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.distributions.independent import Independent\n",
        "\n",
        "import gym\n",
        "from bbrl.agents.agent import Agent\n",
        "from bbrl import get_arguments, get_class, instantiate_class\n",
        "from bbrl.workspace import Workspace\n",
        "from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n",
        "from bbrl.agents.gymb import AutoResetGymAgent, NoAutoResetGymAgent\n",
        "from bbrl.visu.play import load_agent, play\n",
        "from bbrl.utils.replay_buffer import ReplayBuffer\n",
        "from bbrl.utils.functionalb import gae\n",
        "\n",
        "# !rm -rf gym-rocketlander\n",
        "# !git clone https://github.com/EmbersArc/gym-rocketlander\n",
        "\n",
        "# !conda uninstall gym-rocketlander\n",
        "!pip install -e ./gym-rocketlander\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ROCKETLANDER ENV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pKR4bKBbQ8Wo"
      },
      "outputs": [],
      "source": [
        "class RocketLanderWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Specific wrapper to shape the reward of the rocket lander environment\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(RocketLanderWrapper, self).__init__(env)\n",
        "        self.env = env\n",
        "        self.prev_shaping = None\n",
        "        \n",
        "    def reset(self):\n",
        "        self.prev_shaping = None\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        d = 1\n",
        "        next_state, reward, done, info = self.env.step(action)\n",
        "        # reward shaping\n",
        "        \"\"\"\n",
        "        shaping = -0.5 * (self.env.distance + self.env.speed + abs(self.env.angle) ** 2)\n",
        "        shaping += 0.1 * (\n",
        "            self.env.legs[0].ground_contact + self.env.legs[1].ground_contact\n",
        "        )\n",
        "        if self.prev_shaping is not None:\n",
        "            reward += shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "        \"\"\"\n",
        "        # print (\"distance\", self.env.distance)\n",
        "        \n",
        "        # shaping = 0.02\n",
        "        shaping = 0.008 * (1 - self.env.distance)\n",
        "        # shaping = 0.1 * (self.env.groundcontact - self.env.speed)\n",
        "        if (\n",
        "            self.env.legs[0].ground_contact > 0\n",
        "            and self.env.legs[1].ground_contact > 0\n",
        "            and self.env.speed < 0.1\n",
        "        ):\n",
        "            d = d * 2\n",
        "            print(\"landed !\")\n",
        "            print (\"speed\", self.env.speed)\n",
        "            shaping += 6.0 * d / self.env.speed\n",
        "        else:\n",
        "          d = 1\n",
        "        reward += shaping\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def old_step(self, action):\n",
        "        next_state, reward, done, info = self.env.step(action)\n",
        "        # reward shaping\n",
        "        # shaping = -0.5 * (self.env.distance + self.env.speed + abs(self.env.angle) ** 2)\n",
        "        # shaping += 0.1 * (self.env.legs[0].ground_contact + self.env.legs[1].ground_contact)\n",
        "        shaping = 0\n",
        "        if self.prev_shaping is not None:\n",
        "            reward += shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "\n",
        "class FrameSkip(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Return only every ``skip``-th frame (frameskipping)\n",
        "    :param env: the environment\n",
        "    :param skip: number of ``skip``-th frame\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env: gym.Env, skip: int = 1):\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action: np.ndarray):\n",
        "        \"\"\"\n",
        "        Step the environment with the given action\n",
        "        Repeat action, sum reward, and max over last observations.\n",
        "        :param action: the action\n",
        "        :return: observation, reward, done, information\n",
        "        \"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matplotlib backend: module://matplotlib_inline.backend_inline\n",
            "The observation space: Box([ -1.  -1.  -1.  -1.  -1.  -1.  -1. -inf -inf -inf], [ 1.  1.  1.  1.  1.  1.  1. inf inf inf], (10,), float32)\n",
            "The action space: Box([-1. -1. -1.], [1. 1. 1.], (3,), float32)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"RocketLander-v0\")\n",
        "env = RocketLanderWrapper(env)\n",
        "env = FrameSkip(env, skip=1)\n",
        "obs_space = env.observation_space\n",
        "action_space = env.action_space\n",
        "print(\"The observation space: {}\".format(obs_space))\n",
        "print(\"The action space: {}\".format(action_space))\n",
        "\n",
        "# env = gym.make(\"LunarLander-v2\")\n",
        "# obs_space = env.observation_space\n",
        "# action_space = env.action_space\n",
        "# print(\"The observation space: {}\".format(obs_space))\n",
        "# print(\"The action space: {}\".format(action_space))\n",
        "\n",
        "# env = gym.make(\"LunarLanderContinuous-v2\")\n",
        "# obs_space = env.observation_space\n",
        "# action_space = env.action_space\n",
        "# print(\"The observation space: {}\".format(obs_space))\n",
        "# print(\"The action space: {}\".format(action_space))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GENERAL FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n",
        "    layers = []\n",
        "    for j in range(len(sizes) - 1):\n",
        "        act = activation if j < len(sizes) - 2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def build_backbone(sizes, activation):\n",
        "    layers = []\n",
        "    for j in range(len(sizes) - 2):\n",
        "        layers += [nn.Linear(sizes[j], sizes[j + 1]), activation]\n",
        "    return layers\n",
        "\n",
        "def make_gym_env(env_name):\n",
        "\trocket_env = gym.make(env_name)\n",
        "\trocket_env = RocketLanderWrapper(rocket_env)\n",
        "\treturn FrameSkip(rocket_env, skip=1)\n",
        "\n",
        "def get_env_agents(cfg):\n",
        "    train_env_agent = AutoResetGymAgent(\n",
        "        get_class(cfg.gym_env),\n",
        "        get_arguments(cfg.gym_env),\n",
        "        cfg.algorithm.n_envs,\n",
        "        cfg.algorithm.seed,\n",
        "    )\n",
        "    eval_env_agent = NoAutoResetGymAgent(\n",
        "    get_class(cfg.gym_env),\n",
        "    get_arguments(cfg.gym_env),\n",
        "    cfg.algorithm.nb_evals,\n",
        "    cfg.algorithm.seed,\n",
        "    )\n",
        "    return train_env_agent, eval_env_agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LOGGER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Logger():\n",
        "  def __init__(self, cfg, log_string):\n",
        "    self.logger = instantiate_class(cfg.logger)\n",
        "\n",
        "  def add_log(self, log_string, loss, epoch):\n",
        "    self.logger.add_scalar(log_string, loss.item(), epoch)\n",
        "\n",
        "  def log_losses(self, epoch, critic_loss, entropy_loss, actor_loss):\n",
        "    self.add_log(\"critic_loss\", critic_loss, epoch)\n",
        "    self.add_log(\"entropy_loss\", entropy_loss, epoch)\n",
        "    self.add_log(\"actor_loss\", actor_loss, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bbrl.utils.distributions import SquashedDiagGaussianDistribution\n",
        "\n",
        "class SquashedGaussianActor(Agent):\n",
        "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
        "        super().__init__()\n",
        "        backbone_dim = [state_dim] + list(hidden_layers)\n",
        "        self.layers = build_backbone(backbone_dim, activation=nn.ReLU())\n",
        "        self.backbone = nn.Sequential(*self.layers)\n",
        "        self.last_mean_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
        "        self.last_std_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
        "        self.action_dist = SquashedDiagGaussianDistribution(action_dim)\n",
        "        # std must be positive\n",
        "        self.std_layer = nn.Softplus()\n",
        "\n",
        "    def dist(self, obs: torch.Tensor):\n",
        "        \"\"\"Computes action distributions given observation(s)\"\"\"\n",
        "        backbone_output = self.backbone(obs)\n",
        "        mean = self.last_mean_layer(backbone_output)\n",
        "        std_out = self.last_std_layer(backbone_output)\n",
        "        std = self.std_layer(std_out)\n",
        "        return self.action_dist.make_distribution(mean, std)\n",
        "\n",
        "\n",
        "    def forward(self, t, stochastic):\n",
        "        action_dist = self.dist(self.get((\"env/env_obs\", t)))\n",
        "        action = action_dist.sample() if stochastic else action_dist.mode()\n",
        "\n",
        "        log_prob = action_dist.log_prob(action)\n",
        "        self.set((f\"action\", t), action)\n",
        "        self.set((\"action_logprobs\", t), log_prob)\n",
        "\n",
        "    def predict_action(self, obs, stochastic: bool):\n",
        "        action_dist = self.dist(obs)\n",
        "        action = action_dist.sample() if stochastic else action_dist.mode()\n",
        "        return action\n",
        "\n",
        "\n",
        "class ContinuousQAgent(Agent):\n",
        "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
        "        super().__init__()\n",
        "        self.is_q_function = True\n",
        "        self.model = build_mlp(\n",
        "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, t, detach_actions=False):\n",
        "        obs = self.get((\"env/env_obs\", t))\n",
        "        action = self.get((\"action\", t))\n",
        "        if detach_actions:\n",
        "            action = action.detach()\n",
        "        osb_act = torch.cat((obs, action), dim=1)\n",
        "        q_value = self.model(osb_act)\n",
        "        self.set((\"q_value\", t), q_value)\n",
        "\n",
        "    def predict_value(self, obs, action):\n",
        "        osb_act = torch.cat((obs, action), dim=0)\n",
        "        q_value = self.model(osb_act)\n",
        "        return q_value\n",
        "\n",
        "\n",
        "\n",
        "# Create the SAC Agent\n",
        "def create_sac_agent(cfg, train_env_agent, eval_env_agent):\n",
        "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n",
        "    assert (\n",
        "        train_env_agent.is_continuous_action()\n",
        "    ), \"SAC code dedicated to continuous actions\"\n",
        "\n",
        "    # Actor\n",
        "    actor = SquashedGaussianActor(\n",
        "        obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
        "    )\n",
        "\n",
        "    # Train/Test agents\n",
        "    tr_agent = Agents(train_env_agent, actor)\n",
        "    ev_agent = Agents(eval_env_agent, actor)\n",
        "\n",
        "    # Builds the critics\n",
        "    critic_1 = ContinuousQAgent(\n",
        "        obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
        "    )\n",
        "    target_critic_1 = copy.deepcopy(critic_1)\n",
        "    critic_2 = ContinuousQAgent(\n",
        "        obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
        "    )\n",
        "    target_critic_2 = copy.deepcopy(critic_2)\n",
        "\n",
        "    train_agent = TemporalAgent(tr_agent)\n",
        "    eval_agent = TemporalAgent(ev_agent)\n",
        "    train_agent.seed(cfg.algorithm.seed)\n",
        "    return (\n",
        "        train_agent,\n",
        "        eval_agent,\n",
        "        actor,\n",
        "        critic_1,\n",
        "        target_critic_1,\n",
        "        critic_2,\n",
        "        target_critic_2,\n",
        "    )\n",
        "\n",
        "\n",
        "# Configure the optimizer for the actor and critic\n",
        "def setup_optimizers(cfg, actor, critic_1, critic_2):\n",
        "    actor_optimizer_args = get_arguments(cfg.actor_optimizer)\n",
        "    parameters = actor.parameters()\n",
        "    actor_optimizer = get_class(cfg.actor_optimizer)(parameters, **actor_optimizer_args)\n",
        "    critic_optimizer_args = get_arguments(cfg.critic_optimizer)\n",
        "    parameters = nn.Sequential(critic_1, critic_2).parameters()\n",
        "    critic_optimizer = get_class(cfg.critic_optimizer)(\n",
        "        parameters, **critic_optimizer_args\n",
        "    )\n",
        "    return actor_optimizer, critic_optimizer\n",
        "\n",
        "\n",
        "def setup_entropy_optimizers(cfg):\n",
        "    if cfg.algorithm.target_entropy == \"auto\":\n",
        "        entropy_coef_optimizer_args = get_arguments(cfg.entropy_coef_optimizer)\n",
        "        # Note: we optimize the log of the entropy coefficient which is slightly different from the paper\n",
        "        # as discussed in https://github.com/rail-berkeley/softlearning/issues/37\n",
        "        # Comment and code taken from the SB3 version of SAC\n",
        "        log_entropy_coef = torch.log(\n",
        "            torch.ones(1) * cfg.algorithm.entropy_coef\n",
        "        ).requires_grad_(True)\n",
        "        entropy_coef_optimizer = get_class(cfg.entropy_coef_optimizer)(\n",
        "            [log_entropy_coef], **entropy_coef_optimizer_args\n",
        "        )\n",
        "    else:\n",
        "        log_entropy_coef = 0\n",
        "        entropy_coef_optimizer = None\n",
        "    return entropy_coef_optimizer, log_entropy_coef\n",
        "\n",
        "\n",
        "def compute_critic_loss(\n",
        "    cfg, reward, must_bootstrap,\n",
        "    t_actor, \n",
        "    q_agent_1, q_agent_2, \n",
        "    target_q_agent_1, target_q_agent_2, \n",
        "    rb_workspace,\n",
        "    ent_coef\n",
        "):\n",
        "    \"\"\"Computes the critic loss for a set of $S$ transition samples\n",
        "\n",
        "    Args:\n",
        "        cfg: The experimental configuration\n",
        "        reward: _description_\n",
        "        must_bootstrap: Tensor of indicators (2 x S)\n",
        "        t_actor: The actor agent (as a TemporalAgent)\n",
        "        q_agent_1: The first critic (as a TemporalAgent)\n",
        "        q_agent_2: The second critic (as a TemporalAgent)\n",
        "        target_q_agent_1: The target of the first critic\n",
        "        target_q_agent_2: The target of the second critic\n",
        "        rb_workspace: The transition workspace\n",
        "        ent_coef: The entropy coefficient\n",
        "\n",
        "    Returns:\n",
        "        Tuple[torch.Tensor, torch.Tensor]: The two critic losses (scalars)\n",
        "    \"\"\"\n",
        "    # Compute q_values from both critics with the actions present in the buffer:\n",
        "    # at t, we have Q(s,a) from the (s,a) in the RB\n",
        "    q_agent_1(rb_workspace, t=0, n_steps=1)\n",
        "    q_values_rb_1 = rb_workspace[\"q_value\"]\n",
        "    \n",
        "    q_agent_2(rb_workspace, t=0, n_steps=1)\n",
        "    q_values_rb_2 = rb_workspace[\"q_value\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Replay the current actor on the replay buffer to get actions of the\n",
        "        # current policy\n",
        "        t_actor(rb_workspace, t=1, n_steps=1, stochastic=True)\n",
        "        action_logprobs_next = rb_workspace[\"action_logprobs\"]\n",
        "\n",
        "        # Compute target q_values from both target critics: at t+1, we have\n",
        "        # Q(s+1,a+1) from the (s+1,a+1) where a+1 has been replaced in the RB\n",
        "\n",
        "        target_q_agent_1(rb_workspace, t=1, n_steps=1)\n",
        "        post_q_values_1 = rb_workspace[\"q_value\"]\n",
        "\n",
        "        target_q_agent_2(rb_workspace, t=1, n_steps=1)\n",
        "        post_q_values_2 = rb_workspace[\"q_value\"]\n",
        "\n",
        "    post_q_values = torch.min(post_q_values_1, post_q_values_2).squeeze(-1)\n",
        "\n",
        "    v_phi = post_q_values[1] - ent_coef * action_logprobs_next\n",
        "    target = (\n",
        "        reward[:-1][0] + cfg.algorithm.discount_factor * v_phi * must_bootstrap.int()\n",
        "    )\n",
        "    td_1 = target - q_values_rb_1[0].squeeze(-1)\n",
        "    td_2 = target - q_values_rb_2[0].squeeze(-1)\n",
        "    td_error_1 = td_1**2\n",
        "    td_error_2 = td_2**2\n",
        "    critic_loss_1 = td_error_1.mean()\n",
        "    critic_loss_2 = td_error_2.mean()\n",
        "\n",
        "    return critic_loss_1, critic_loss_2\n",
        "\n",
        "\n",
        "def soft_update_params(net, target_net, tau):\n",
        "    for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
        "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "\n",
        "def compute_actor_loss(ent_coef, t_actor, q_agent_1, q_agent_2, rb_workspace):\n",
        "    \"\"\"Actor loss computation\n",
        "    \n",
        "    :param ent_coef: The entropy coefficient $\\alpha$\n",
        "    :param t_actor: The actor agent (temporal agent)\n",
        "    :param q_agent_1: The first critic (temporal agent)\n",
        "    :param q_agent_2: The second critic (temporal agent)\n",
        "    :param rb_workspace: The replay buffer (2 time steps, $t$ and $t+1$)\n",
        "    \"\"\"\n",
        "    # Recompute the q_values from the current policy, not from the actions in the buffer\n",
        "\n",
        "    t_actor(rb_workspace, t=0, n_steps=1, stochastic=True)\n",
        "    action_logprobs_new = rb_workspace[\"action_logprobs\"]\n",
        "\n",
        "    q_agent_1(rb_workspace, t=0, n_steps=1)\n",
        "    q_values_1 = rb_workspace[\"q_value\"]\n",
        "    q_agent_2(rb_workspace, t=0, n_steps=1)\n",
        "    q_values_2 = rb_workspace[\"q_value\"]\n",
        "\n",
        "    current_q_values = torch.min(q_values_1, q_values_2).squeeze(-1)\n",
        "\n",
        "    actor_loss = ent_coef * action_logprobs_new[0] - current_q_values[0]\n",
        "\n",
        "    return actor_loss.mean()\n",
        "\n",
        "\n",
        "\n",
        "def run_sac(cfg, log_string):\n",
        "    # 1)  Build the  logger\n",
        "    logger = Logger(cfg, log_string)\n",
        "    best_reward = -10e9\n",
        "    ent_coef = cfg.algorithm.entropy_coef\n",
        "\n",
        "    # 2) Create the environment agent\n",
        "    train_env_agent = AutoResetGymAgent(\n",
        "        get_class(cfg.gym_env),\n",
        "        get_arguments(cfg.gym_env),\n",
        "        cfg.algorithm.n_envs,\n",
        "        cfg.algorithm.seed,\n",
        "    )\n",
        "    eval_env_agent = NoAutoResetGymAgent(\n",
        "        get_class(cfg.gym_env),\n",
        "        get_arguments(cfg.gym_env),\n",
        "        cfg.algorithm.nb_evals,\n",
        "        cfg.algorithm.seed,\n",
        "    )\n",
        "\n",
        "    # 3) Create the A2C Agent\n",
        "    (\n",
        "        train_agent,\n",
        "        eval_agent,\n",
        "        actor,\n",
        "        critic_1,\n",
        "        target_critic_1,\n",
        "        critic_2,\n",
        "        target_critic_2,\n",
        "    ) = create_sac_agent(cfg, train_env_agent, eval_env_agent)\n",
        "\n",
        "    t_actor = TemporalAgent(actor)\n",
        "    q_agent_1 = TemporalAgent(critic_1)\n",
        "    target_q_agent_1 = TemporalAgent(target_critic_1)\n",
        "    q_agent_2 = TemporalAgent(critic_2)\n",
        "    target_q_agent_2 = TemporalAgent(target_critic_2)\n",
        "    train_workspace = Workspace()\n",
        "\n",
        "    # Creates a replay buffer\n",
        "    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n",
        "\n",
        "    # Configure the optimizer\n",
        "    actor_optimizer, critic_optimizer = setup_optimizers(cfg, actor, critic_1, critic_2)\n",
        "    entropy_coef_optimizer, log_entropy_coef = setup_entropy_optimizers(cfg)\n",
        "    nb_steps = 0\n",
        "    tmp_steps = 0\n",
        "\n",
        "    # Initial value of the entropy coef alpha. If target_entropy is not auto,\n",
        "    # will remain fixed\n",
        "    if cfg.algorithm.target_entropy == \"auto\":\n",
        "        target_entropy = -np.prod(train_env_agent.action_space.shape).astype(np.float32)\n",
        "    else:\n",
        "        target_entropy = cfg.algorithm.target_entropy\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(cfg.algorithm.max_epochs)):\n",
        "        # Execute the agent in the workspace\n",
        "        if epoch > 0:\n",
        "            train_workspace.zero_grad()\n",
        "            train_workspace.copy_n_last_steps(1)\n",
        "            train_agent(\n",
        "                train_workspace,\n",
        "                t=1,\n",
        "                n_steps=cfg.algorithm.n_steps - 1,\n",
        "                stochastic=True,\n",
        "            )\n",
        "        else:\n",
        "            train_agent(\n",
        "                train_workspace,\n",
        "                t=0,\n",
        "                n_steps=cfg.algorithm.n_steps,\n",
        "                stochastic=True,\n",
        "            )\n",
        "\n",
        "        transition_workspace = train_workspace.get_transitions()\n",
        "        action = transition_workspace[\"action\"]\n",
        "        nb_steps += action[0].shape[0]\n",
        "        rb.put(transition_workspace)\n",
        "\n",
        "        if nb_steps > cfg.algorithm.learning_starts:\n",
        "            # Get a sample from the workspace\n",
        "            rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n",
        "\n",
        "            done, truncated, reward, action_logprobs_rb = rb_workspace[\n",
        "                \"env/done\", \"env/truncated\", \"env/reward\", \"action_logprobs\"\n",
        "            ]\n",
        "\n",
        "            # Determines whether values of the critic should be propagated\n",
        "            # True if the episode reached a time limit or if the task was not done\n",
        "            # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing\n",
        "            must_bootstrap = torch.logical_or(~done[1], truncated[1])\n",
        "\n",
        "            (\n",
        "                critic_loss_1, critic_loss_2\n",
        "            ) = compute_critic_loss(\n",
        "                cfg, \n",
        "                reward, \n",
        "                must_bootstrap,\n",
        "                t_actor,\n",
        "                q_agent_1,\n",
        "                q_agent_2,\n",
        "                target_q_agent_1,\n",
        "                target_q_agent_2,\n",
        "                rb_workspace,\n",
        "                ent_coef\n",
        "            )\n",
        "\n",
        "            logger.add_log(\"critic_loss_1\", critic_loss_1, nb_steps)\n",
        "            logger.add_log(\"critic_loss_2\", critic_loss_2, nb_steps)\n",
        "            critic_loss = critic_loss_1 + critic_loss_2\n",
        "\n",
        "            actor_loss = compute_actor_loss(\n",
        "                ent_coef, t_actor, q_agent_1, q_agent_2, rb_workspace\n",
        "            )\n",
        "            logger.add_log(\"actor_loss\", actor_loss, nb_steps)\n",
        "\n",
        "            # Entropy coef update part #####################################################\n",
        "            if entropy_coef_optimizer is not None:\n",
        "                # Important: detach the variable from the graph\n",
        "                # so that we don't change it with other losses\n",
        "                # see https://github.com/rail-berkeley/softlearning/issues/60\n",
        "                ent_coef = torch.exp(log_entropy_coef.detach())\n",
        "                # See Eq. (17) of the SAC and Applications paper\n",
        "                entropy_coef_loss = -(\n",
        "                    log_entropy_coef * (action_logprobs_rb + target_entropy)\n",
        "                ).mean()\n",
        "                entropy_coef_optimizer.zero_grad()\n",
        "                # We need to retain the graph because we reuse the\n",
        "                # action_logprobs are used to compute both the actor loss and\n",
        "                # the critic loss\n",
        "                entropy_coef_loss.backward(retain_graph=True)\n",
        "                entropy_coef_optimizer.step()\n",
        "                logger.add_log(\"entropy_coef_loss\", entropy_coef_loss, nb_steps)\n",
        "                logger.add_log(\"entropy_coef\", ent_coef, nb_steps)\n",
        "\n",
        "            # Actor update part ###############################\n",
        "            actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                actor.parameters(), cfg.algorithm.max_grad_norm\n",
        "            )\n",
        "            actor_optimizer.step()\n",
        "\n",
        "\n",
        "            # Critic update part ###############################\n",
        "            critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                critic_1.parameters(), cfg.algorithm.max_grad_norm\n",
        "            )\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                critic_2.parameters(), cfg.algorithm.max_grad_norm\n",
        "            )\n",
        "            critic_optimizer.step()\n",
        "            ####################################################\n",
        "\n",
        "            # Soft update of target q function\n",
        "            tau = cfg.algorithm.tau_target\n",
        "            soft_update_params(critic_1, target_critic_1, tau)\n",
        "            soft_update_params(critic_2, target_critic_2, tau)\n",
        "            # soft_update_params(actor, target_actor, tau)\n",
        "\n",
        "        # Evaluate ###########################################\n",
        "        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n",
        "            tmp_steps = nb_steps\n",
        "            eval_workspace = Workspace()  # Used for evaluation\n",
        "            eval_agent(\n",
        "                eval_workspace,\n",
        "                t=0,\n",
        "                stop_variable=\"env/done\",\n",
        "                stochastic=False,\n",
        "            )\n",
        "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
        "            mean = rewards.mean()\n",
        "            logger.add_log(\"reward/mean\", mean, nb_steps)\n",
        "            logger.add_log(\"reward/max\", rewards.max(), nb_steps)\n",
        "            logger.add_log(\"reward/min\", rewards.min(), nb_steps)\n",
        "            # logger.add_log(\"reward/med\", rewards.median(), nb_steps)\n",
        "\n",
        "            # print(f\"nb_steps: {nb_steps}, reward: {mean}\")\n",
        "            # print(\"ent_coef\", ent_coef)\n",
        "            if cfg.save_best and mean > best_reward:\n",
        "                best_reward = mean\n",
        "                directory = f\"./agents/rocketlander/sac/\"+log_string\n",
        "                if not os.path.exists(directory):\n",
        "                    os.makedirs(directory)\n",
        "                filename = directory + \"sac_\" + str(mean.item()) + \".agt\"\n",
        "                actor.save_model(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUJjPFppYCje"
      },
      "source": [
        "# ROCKETLANDER / SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENV = \"RocketLander-v0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FfNk_iNKYCsS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%tensorboard` not found.\n"
          ]
        }
      ],
      "source": [
        "# %reload_ext tensorboard\n",
        "%tensorboard --logdir ./runs/rocketlander/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E69CMfNwYCuf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [37:21<00:00,  2.23it/s]  \n",
            "100%|██████████| 5000/5000 [40:19<00:00,  2.07it/s]  \n",
            "100%|██████████| 5000/5000 [37:51<00:00,  2.20it/s]  \n",
            "100%|██████████| 5000/5000 [46:43<00:00,  1.78it/s]  \n",
            "100%|██████████| 5000/5000 [41:39<00:00,  2.00it/s]  \n",
            "100%|██████████| 5000/5000 [47:22<00:00,  1.76it/s]  \n",
            "100%|██████████| 5000/5000 [50:04<00:00,  1.66it/s]  \n",
            "100%|██████████| 5000/5000 [49:16<00:00,  1.69it/s]  \n",
            "100%|██████████| 5000/5000 [50:08<00:00,  1.66it/s]  \n",
            "100%|██████████| 5000/5000 [54:28<00:00,  1.53it/s]  \n",
            "100%|██████████| 5000/5000 [50:07<00:00,  1.66it/s]  \n",
            "100%|██████████| 5000/5000 [46:47<00:00,  1.78it/s]  \n"
          ]
        }
      ],
      "source": [
        "batches = [256]\n",
        "grad_norms = [0.5]\n",
        "gammas = [0.9] \n",
        "ent_coefs = [1e-7, 1e-6, 1e-5]\n",
        "archs_actor = [[128,128]] \n",
        "archs_critic = [[256,256], [400,400]]\n",
        "lrs_actor = [1e-4]\n",
        "lrs_critic = [1e-4, 5e-5] \n",
        "lrs_ent = [1e-4] \n",
        "\n",
        "# first run\n",
        "# 1e-3 critic lr works only if actor is 128,128 atleast\n",
        "# 1e-4 critic lr seems to work better\n",
        "\n",
        "# second run\n",
        "# 0.9 seems better than 0.99 for gamma\n",
        "# 1e-4 best lr for critic\n",
        "# 128,128 seems a bit better for actor\n",
        "\n",
        "# third run\n",
        "# 400,400 seems to be way better for critic arch\n",
        "# works well with 5e-5 critic lr, maybe try to lower actor and ent lrs as well ?\n",
        "# ent coef was the best at 1e-7\n",
        "\n",
        "# batches = [256]\n",
        "# grad_norms = [0.5]\n",
        "# gammas = [0.9] \n",
        "# ent_coefs = [1e-7]\n",
        "# archs_actor = [[128,128], [256,256]] \n",
        "# archs_critic = [[400,400]]\n",
        "# lrs_actor = [1e-4, 5e-5]\n",
        "# lrs_critic = [5e-5] \n",
        "# lrs_ent = [1e-4, 5e-5]\n",
        "\n",
        "# fourth run\n",
        "\n",
        "for batch in batches:\n",
        "\tfor grad_norm in grad_norms:\n",
        "\t\tfor gamma in gammas:\n",
        "\t\t\tfor ent_coef in ent_coefs:\n",
        "\t\t\t\tfor arch_actor in archs_actor:\n",
        "\t\t\t\t\tfor arch_critic in archs_critic:\n",
        "\t\t\t\t\t\tfor lr_actor in lrs_actor:\n",
        "\t\t\t\t\t\t\tfor lr_critic in lrs_critic:\n",
        "\t\t\t\t\t\t\t\tfor lr_ent in lrs_ent:\n",
        "\t\t\t\t\t\t\t\t\tlog_string = \"SAC_\"+str(batch)+\"_\"+str(grad_norm)+\"_\"+str(gamma)+\"_\"+str(ent_coef)+\"_\"+str(arch_actor)+\"_\"+str(arch_critic)+\"_\"+str(lr_actor)+\"_\"+str(lr_critic)+\"_\"+str(lr_ent)\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t# Avoid doing the same run twice\n",
        "\t\t\t\t\t\t\t\t\tfor filename in os.listdir(\"runs/\"):\n",
        "\t\t\t\t\t\t\t\t\t\tif log_string == str(filename):\n",
        "\t\t\t\t\t\t\t\t\t\t\tprint(\"Skipping this one :\", log_string)\n",
        "\t\t\t\t\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\t\t\t\t\tparams={\n",
        "\t\t\t\t\t\t\t\t\t\t\"save_best\": True,\n",
        "\t\t\t\t\t\t\t\t\t\t\"logger\":{\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"classname\": \"bbrl.utils.logger.TFLogger\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"log_dir\": \"./runs/RocketLander-v0/\" + log_string,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"cache_size\": 10000,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"every_n_seconds\": 10,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"verbose\": False,    \n",
        "\t\t\t\t\t\t\t\t\t\t\t},\n",
        "\n",
        "\t\t\t\t\t\t\t\t\t\t\"algorithm\":{\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"seed\": 1,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"n_envs\": 8,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"n_steps\": 32,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"buffer_size\": 1e6,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"batch_size\": batch, # 256\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"max_grad_norm\": grad_norm, # 0.5\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"nb_evals\":10,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"eval_interval\": 2000,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"learning_starts\": 2000,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"max_epochs\": 5000,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"discount_factor\": gamma, # 0.98\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"entropy_coef\": ent_coef, # 1e-7\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"target_entropy\": \"auto\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"tau_target\": 0.05,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"architecture\":{\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\"actor_hidden_size\": arch_actor, # 32,32\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\"critic_hidden_size\": arch_critic, # 256,256\n",
        "\t\t\t\t\t\t\t\t\t\t\t},\n",
        "\t\t\t\t\t\t\t\t\t\t},\n",
        "\t\t\t\t\t\t\t\t\t\t\"gym_env\":{\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"classname\": \"__main__.make_gym_env\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"env_name\": ENV,\n",
        "\t\t\t\t\t\t\t\t\t\t\t},\n",
        "\t\t\t\t\t\t\t\t\t\t\"actor_optimizer\":{\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"classname\": \"torch.optim.Adam\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"lr\": lr_actor, # 1e-3\n",
        "\t\t\t\t\t\t\t\t\t\t\t},\n",
        "\t\t\t\t\t\t\t\t\t\t\"critic_optimizer\":{\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"classname\": \"torch.optim.Adam\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"lr\": lr_critic, # 1e-3\n",
        "\t\t\t\t\t\t\t\t\t\t\t},\n",
        "\t\t\t\t\t\t\t\t\t\t\"entropy_coef_optimizer\":{\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"classname\": \"torch.optim.Adam\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\"lr\": lr_ent, # 1e-3\n",
        "\t\t\t\t\t\t\t\t\t\t\t}\n",
        "\t\t\t\t\t\t\t\t\t}\n",
        "\n",
        "\t\t\t\t\t\t\t\t\tconfig=OmegaConf.create(params)\n",
        "\t\t\t\t\t\t\t\t\ttorch.manual_seed(config.algorithm.seed)\n",
        "\t\t\t\t\t\t\t\t\trun_sac(config, log_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('deepdac')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "9bf73cbd865082e3cd06c05babb8829fd907c8e55c85361e30fff74a8577746e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
