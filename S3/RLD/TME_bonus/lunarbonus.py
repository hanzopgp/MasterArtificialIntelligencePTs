# -*- coding: utf-8 -*-
"""LunarBonus.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jCRsaG-BvLXRWKxRFJ00E0chkjTTmhqS

# IMPORTS
"""

!pip install easypip

from easypip import easyimport
import functools
import time

easyimport("importlib_metadata==4.13.0")
OmegaConf = easyimport("omegaconf").OmegaConf
bbrl_gym = easyimport("bbrl_gym")
bbrl = easyimport("bbrl>=0.1.6")

import os
import copy
import time
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal
from torch.distributions.normal import Normal
from torch.distributions.independent import Independent

import gym
from bbrl.agents.agent import Agent
from bbrl import get_arguments, get_class, instantiate_class
from bbrl.workspace import Workspace
from bbrl.agents import Agents, RemoteAgent, TemporalAgent
from bbrl.agents.gymb import AutoResetGymAgent, NoAutoResetGymAgent
from bbrl.visu.play import load_agent, play
from bbrl.utils.replay_buffer import ReplayBuffer
from bbrl.utils.functionalb import gae

"""# GENERAL FUNCTIONS"""

def build_mlp(sizes, activation, output_activation=nn.Identity()):
    layers = []
    for j in range(len(sizes) - 1):
        act = activation if j < len(sizes) - 2 else output_activation
        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]
    return nn.Sequential(*layers)

def make_gym_env(env_name):
    return gym.make(env_name)

def get_env_agents(cfg):
    train_env_agent = AutoResetGymAgent(
        get_class(cfg.gym_env),
        get_arguments(cfg.gym_env),
        cfg.algorithm.n_envs,
        cfg.algorithm.seed,
    )
    eval_env_agent = NoAutoResetGymAgent(
    get_class(cfg.gym_env),
    get_arguments(cfg.gym_env),
    cfg.algorithm.nb_evals,
    cfg.algorithm.seed,
    )
    return train_env_agent, eval_env_agent

"""# LOGGER"""

class Logger():
  def __init__(self, cfg):
    self.logger = instantiate_class(cfg.logger)

  def add_log(self, log_string, loss, epoch):
    self.logger.add_scalar(log_string, loss.item(), epoch)

  def add_q_norms(self, q_norm, target_q_norm, epoch):
    # my_summary_writer.add_scalars(f'loss/check_info', {
    #                               'score': score[iteration],
    #                               'score_nf': score_nf[iteration],
    #                           }, iteration)
    # self.logger.add_scalar(log_string, q_norm, epoch)

    # self.logger.add_scalar(f'q_value/q_norm',    q_norm,    epoch)
    # self.logger.add_scalar(f'q_value/target_q_norm', target_q_norm, epoch)

    self.logger.add_scalars(f'q_value', {
        'q_norm': q_norm,
        'target_q_norm': target_q_norm,
    }, epoch)
    
  def add_q_norms2(self, q_norm1, q_norm2, target, epoch):
    self.logger.add_scalars(f'q_value', {
        'q_norm1': q_norm1,
        'q_norm2': q_norm2,
        'target_q_norm': target,
    }, epoch)

  # def log_losses(self, cfg, epoch, critic_loss, entropy_loss, a2c_loss):
  #   self.add_log("critic_loss", critic_loss, epoch)
  #   self.add_log("entropy_loss", entropy_loss, epoch)
  #   self.add_log("a2c_loss", a2c_loss, epoch)

  def log_losses(self, epoch, critic_loss, entropy_loss, actor_loss):
    self.add_log("critic_loss", critic_loss, epoch)
    self.add_log("entropy_loss", entropy_loss, epoch)
    self.add_log("actor_loss", actor_loss, epoch)

"""# DDPG & TD3"""

class ContinuousQAgent(Agent):
    def __init__(self, state_dim, hidden_layers, action_dim):
        super().__init__()
        self.is_q_function = True
        self.model = build_mlp(
            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()
        )

    def forward(self, t, detach_actions=False):
        obs = self.get(("env/env_obs", t))
        action = self.get(("action", t))
        if detach_actions:
            action = action.detach()
        osb_act = torch.cat((obs, action), dim=1)
        q_value = self.model(osb_act)
        self.set(("q_value", t), q_value)

    def predict_value(self, obs, action):
        osb_act = torch.cat((obs, action), dim=0)
        q_value = self.model(osb_act)
        return q_value



class ContinuousDeterministicActor(Agent):
    def __init__(self, state_dim, hidden_layers, action_dim):
        super().__init__()
        layers = [state_dim] + list(hidden_layers) + [action_dim]
        self.model = build_mlp(
            layers, activation=nn.ReLU(), output_activation=nn.Tanh()
        )

    def forward(self, t):
        obs = self.get(("env/env_obs", t))
        action = self.model(obs)
        self.set(("action", t), action)

    def predict_action(self, obs, stochastic):
        assert (
            not stochastic
        ), "ContinuousDeterministicActor cannot provide stochastic predictions"
        return self.model(obs)



class AddGaussianNoise(Agent):
    def __init__(self, sigma):
        super().__init__()
        self.sigma = sigma

    def forward(self, t, **kwargs):
        act = self.get(("action", t))
        dist = Normal(act, self.sigma)
        action = dist.sample()
        self.set(("action", t), action)



class AddOUNoise(Agent):
    def __init__(self, std_dev, theta=0.15, dt=1e-2):
        self.theta = theta
        self.std_dev = std_dev
        self.dt = dt
        self.x_prev = 0

    def forward(self, t, **kwargs):
        act = self.get(("action", t))
        x = (
            self.x_prev
            + self.theta * (act - self.x_prev) * self.dt
            + self.std_dev * math.sqrt(self.dt) * torch.randn(act.shape)
        )
        self.x_prev = x
        self.set(("action", t), x)



def create_ddpg_agent(cfg, train_env_agent, eval_env_agent):
    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()
    critic = ContinuousQAgent(
        obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size
    )
    target_critic = copy.deepcopy(critic)
    actor = ContinuousDeterministicActor(
        obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size
    )
    # target_actor = copy.deepcopy(actor) # not used in practice, though described in the paper
    noise_agent = AddGaussianNoise(cfg.algorithm.action_noise) # alternative : AddOUNoise
    tr_agent = Agents(train_env_agent, actor, noise_agent)  
    ev_agent = Agents(eval_env_agent, actor)

    # Get an agent that is executed on a complete workspace
    train_agent = TemporalAgent(tr_agent)
    eval_agent = TemporalAgent(ev_agent)
    train_agent.seed(cfg.algorithm.seed)
    return train_agent, eval_agent, actor, critic, target_critic  # , target_actor



def setup_optimizers(cfg, actor, critic):
    actor_optimizer_args = get_arguments(cfg.actor_optimizer)
    parameters = actor.parameters()
    actor_optimizer = get_class(cfg.actor_optimizer)(parameters, **actor_optimizer_args)
    critic_optimizer_args = get_arguments(cfg.critic_optimizer)
    parameters = critic.parameters()
    critic_optimizer = get_class(cfg.critic_optimizer)(
        parameters, **critic_optimizer_args
    )
    return actor_optimizer, critic_optimizer



def compute_critic_loss(cfg, reward, must_bootstrap, q_values, target_q_values):
    # Compute temporal difference
    q_next = target_q_values
    target = (
        reward[:-1][0]
        + cfg.algorithm.discount_factor * q_next.squeeze(-1) * must_bootstrap.int()
    )
    td = target - q_values.squeeze(-1)
    # Compute critic loss
    td_error = td**2
    critic_loss = td_error.mean()
    return critic_loss



def soft_update_params(net, target_net, tau):
    for param, target_param in zip(net.parameters(), target_net.parameters()):
        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)



def compute_actor_loss(q_values):
    return -q_values.mean()



def run_ddpg(cfg):
    # 1)  Build the  logger
    logger = Logger(cfg)
    best_reward = -10e9

    # 2) Create the environment agent
    train_env_agent = AutoResetGymAgent(
        get_class(cfg.gym_env),
        get_arguments(cfg.gym_env),
        cfg.algorithm.n_envs,
        cfg.algorithm.seed,
    )
    eval_env_agent = NoAutoResetGymAgent(
        get_class(cfg.gym_env),
        get_arguments(cfg.gym_env),
        cfg.algorithm.nb_evals,
        cfg.algorithm.seed,
    )

    # 3) Create the DDPG Agent
    (
        train_agent,
        eval_agent,
        actor,
        critic,
        # target_actor,
        target_critic,
    ) = create_ddpg_agent(cfg, train_env_agent, eval_env_agent)
    ag_actor = TemporalAgent(actor)
    # ag_target_actor = TemporalAgent(target_actor)
    q_agent = TemporalAgent(critic)
    target_q_agent = TemporalAgent(target_critic)

    train_workspace = Workspace()
    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)

    # Configure the optimizer
    actor_optimizer, critic_optimizer = setup_optimizers(cfg, actor, critic)
    nb_steps = 0
    tmp_steps = 0

    # Training loop
    for epoch in range(cfg.algorithm.max_epochs):
        # Execute the agent in the workspace
        if epoch > 0:
            train_workspace.zero_grad()
            train_workspace.copy_n_last_steps(1)
            train_agent(train_workspace, t=1, n_steps=cfg.algorithm.n_steps - 1)
        else:
            train_agent(train_workspace, t=0, n_steps=cfg.algorithm.n_steps)

        transition_workspace = train_workspace.get_transitions()
        action = transition_workspace["action"]
        nb_steps += action[0].shape[0]
        rb.put(transition_workspace)
        rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)

        done, truncated, reward, action = rb_workspace[
            "env/done", "env/truncated", "env/reward", "action"
        ]
        if nb_steps > cfg.algorithm.learning_starts:
            # Determines whether values of the critic should be propagated
            # True if the episode reached a time limit or if the task was not done
            # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing
            must_bootstrap = torch.logical_or(~done[1], truncated[1])

            # Critic update
            # compute q_values: at t, we have Q(s,a) from the (s,a) in the RB
            q_agent(rb_workspace, t=0, n_steps=1)
            q_values = rb_workspace["q_value"]

            with torch.no_grad():
                # replace the action at t+1 in the RB with \pi(s_{t+1}), to compute Q(s_{t+1}, \pi(s_{t+1}) below
                ag_actor(rb_workspace, t=1, n_steps=1)
                # compute q_values: at t+1 we have Q(s_{t+1}, \pi(s_{t+1})
                target_q_agent(rb_workspace, t=1, n_steps=1)
                # q_agent(rb_workspace, t=1, n_steps=1)
            # finally q_values contains the above collection at t=0 and t=1
            post_q_values = rb_workspace["q_value"]

            # Compute critic loss
            critic_loss = compute_critic_loss(
                cfg, reward, must_bootstrap, q_values[0], post_q_values[1]
            )
            logger.add_log("critic_loss", critic_loss, nb_steps)
            logger.add_q_norms(int(q_values[0].detach().numpy().sum()), int(post_q_values[1].detach().numpy().sum()), nb_steps)

            critic_optimizer.zero_grad()
            critic_loss.backward()
            torch.nn.utils.clip_grad_norm_(
                critic.parameters(), cfg.algorithm.max_grad_norm
            )
            critic_optimizer.step()

            # Actor update
            # Now we determine the actions the current policy would take in the states from the RB
            ag_actor(rb_workspace, t=0, n_steps=1)
            # We determine the Q values resulting from actions of the current policy
            q_agent(rb_workspace, t=0, n_steps=1)
            # and we back-propagate the corresponding loss to maximize the Q values
            q_values = rb_workspace["q_value"]
            actor_loss = compute_actor_loss(q_values)
            logger.add_log("actor_loss", actor_loss, nb_steps)

   
            actor_optimizer.zero_grad()
            actor_loss.backward()
            torch.nn.utils.clip_grad_norm_(
                actor.parameters(), cfg.algorithm.max_grad_norm
            )
            actor_optimizer.step()
            # Soft update of target q function
            tau = cfg.algorithm.tau_target
            soft_update_params(critic, target_critic, tau)
            # soft_update_params(actor, target_actor, tau)

        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:
            tmp_steps = nb_steps
            eval_workspace = Workspace()  # Used for evaluation
            eval_agent(eval_workspace, t=0, stop_variable="env/done")
            rewards = eval_workspace["env/cumulated_reward"][-1]
            mean = rewards.mean()
            logger.add_log("reward", mean, nb_steps)
            print(f"nb_steps: {nb_steps}, reward: {mean}")
            if cfg.save_best and mean > best_reward:
                best_reward = mean
                directory = "./ddpg_agent/"
                if not os.path.exists(directory):
                    os.makedirs(directory)
                filename = directory + "ddpg_" + str(mean.item()) + ".agt"
                eval_agent.save_model(filename)



def run_td3(cfg):
    # 1)  Build the  logger
    logger = Logger(cfg)
    best_reward = -10e9

    # 2) Create the environment agent
    train_env_agent = AutoResetGymAgent(
        get_class(cfg.gym_env),
        get_arguments(cfg.gym_env),
        cfg.algorithm.n_envs,
        cfg.algorithm.seed,
    )
    eval_env_agent = NoAutoResetGymAgent(
        get_class(cfg.gym_env),
        get_arguments(cfg.gym_env),
        cfg.algorithm.nb_evals,
        cfg.algorithm.seed,
    )

    # 3) Create the TD3 Agent
    (
        train_agent,
        eval_agent,
        actor,
        critic1,
        # target_actor,
        target_critic1,
    ) = create_ddpg_agent(cfg, train_env_agent, eval_env_agent)
    ag_actor = TemporalAgent(actor)
    # ag_target_actor = TemporalAgent(target_actor)

    # Manually create second critic agent
    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()
    critic2 = ContinuousQAgent(
        obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size
    )
    target_critic2 = copy.deepcopy(critic2)

    q_agent1, q_agent2 = TemporalAgent(critic1), TemporalAgent(critic2)
    target_q_agent1, target_q_agent2 = TemporalAgent(target_critic1), TemporalAgent(target_critic2)

    train_workspace = Workspace()
    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)

    # Configure the optimizer
    actor_optimizer1, critic_optimizer1 = setup_optimizers(cfg, actor, critic1)
    actor_optimizer2, critic_optimizer2 = setup_optimizers(cfg, actor, critic2)
    nb_steps = 0
    tmp_steps = 0

    # Training loop
    for epoch in range(cfg.algorithm.max_epochs):
        # Execute the agent in the workspace
        if epoch > 0:
            train_workspace.zero_grad()
            train_workspace.copy_n_last_steps(1)
            train_agent(train_workspace, t=1, n_steps=cfg.algorithm.n_steps - 1)
        else:
            train_agent(train_workspace, t=0, n_steps=cfg.algorithm.n_steps)

        transition_workspace = train_workspace.get_transitions()
        action = transition_workspace["action"]
        nb_steps += action[0].shape[0]
        rb.put(transition_workspace)
        
        rb_workspace1 = rb.get_shuffled(cfg.algorithm.batch_size)
        rb_workspace2 = copy.deepcopy(rb_workspace1)

        done1, truncated1, reward1, action1 = rb_workspace1[
            "env/done", "env/truncated", "env/reward", "action"
        ]
        done2, truncated2, reward2, action2 = rb_workspace2[
            "env/done", "env/truncated", "env/reward", "action"
        ]
        if nb_steps > cfg.algorithm.learning_starts:
            # Determines whether values of the critic should be propagated
            # True if the episode reached a time limit or if the task was not done
            # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing
            must_bootstrap1 = torch.logical_or(~done1[1], truncated1[1])
            must_bootstrap2 = torch.logical_or(~done2[1], truncated2[1])

            # Critic update
            # compute q_values: at t, we have Q(s,a) from the (s,a) in the RB
            q_agent1(rb_workspace1, t=0, n_steps=1)
            q_values1 = rb_workspace1["q_value"]
            q_agent2(rb_workspace2, t=0, n_steps=1)
            q_values2 = rb_workspace2["q_value"]

            with torch.no_grad():
                # replace the action at t+1 in the RB with \pi(s_{t+1}), to compute Q(s_{t+1}, \pi(s_{t+1}) below
                ag_actor(rb_workspace1, t=1, n_steps=1)
                ag_actor(rb_workspace2, t=1, n_steps=1)
                # compute q_values: at t+1 we have Q(s_{t+1}, \pi(s_{t+1})
                target_q_agent1(rb_workspace1, t=1, n_steps=1)
                target_q_agent2(rb_workspace2, t=1, n_steps=1)
                # q_agent(rb_workspace, t=1, n_steps=1)

                # finally q_values contains the above collection at t=0 and t=1
                post_q_values1 = rb_workspace1["q_value"]
                post_q_values2 = rb_workspace2["q_value"]
                target_q_values = torch.min(post_q_values1[1], post_q_values2[1])

            # Compute critic loss
            critic_loss1 = compute_critic_loss(
                cfg, reward1, must_bootstrap1, q_values1[0], target_q_values
            )
            critic_loss2 = compute_critic_loss(
                cfg, reward2, must_bootstrap2, q_values2[0], target_q_values
            )

            logger.add_q_norms(int(q_values1[0].detach().numpy().sum()), int(target_q_values.detach().numpy().sum()), nb_steps)
            logger.add_q_norms(int(q_values2[0].detach().numpy().sum()), int(target_q_values.detach().numpy().sum()), nb_steps)

            logger.add_log("critic_loss1", critic_loss1, nb_steps)
            logger.add_log("critic_loss2", critic_loss2, nb_steps)
            critic_optimizer1.zero_grad()
            critic_optimizer2.zero_grad()
            critic_loss1.backward()
            critic_loss2.backward()
            torch.nn.utils.clip_grad_norm_(
                critic1.parameters(), cfg.algorithm.max_grad_norm
            )
            torch.nn.utils.clip_grad_norm_(
                critic2.parameters(), cfg.algorithm.max_grad_norm
            )
            critic_optimizer1.step()
            critic_optimizer2.step()

            # Actor update (we can use critic 1 for actor update)
            # Now we determine the actions the current policy would take in the states from the RB
            ag_actor(rb_workspace1, t=0, n_steps=1)
            # We determine the Q values resulting from actions of the current policy
            q_agent1(rb_workspace1, t=0, n_steps=1)
            # and we back-propagate the corresponding loss to maximize the Q values
            q_values = rb_workspace1["q_value"]
            actor_loss = compute_actor_loss(q_values)
            logger.add_log("actor_loss", actor_loss, nb_steps)

            logger.add_log("value_norm", q_values.sum(), nb_steps)

            actor_optimizer1.zero_grad()
            actor_loss.backward()
            torch.nn.utils.clip_grad_norm_(
                actor.parameters(), cfg.algorithm.max_grad_norm
            )
            actor_optimizer1.step()
            # Soft update of target q function
            tau = cfg.algorithm.tau_target
            soft_update_params(critic1, target_critic1, tau)
            # soft_update_params(actor, target_actor, tau)

        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:
            tmp_steps = nb_steps
            eval_workspace = Workspace()  # Used for evaluation
            eval_agent(eval_workspace, t=0, stop_variable="env/done")
            rewards = eval_workspace["env/cumulated_reward"][-1]
            mean = rewards.mean()
            logger.add_log("reward", mean, nb_steps)
            print(f"nb_steps: {nb_steps}, reward: {mean}")
            if cfg.save_best and mean > best_reward:
                best_reward = mean
                directory = "./ddpg_agent/"
                if not os.path.exists(directory):
                    os.makedirs(directory)
                filename = directory + "ddpg_" + str(mean.item()) + ".agt"
                eval_agent.save_model(filename)

"""# PPO"""

class BaseActor(Agent):
    def copy_parameters(self, other):
        """Copy parameters from other agent"""
        for self_p, other_p in zip(self.parameters(), other.parameters()):
            self_p.data.copy_(other_p)

        

class DiscreteActor(BaseActor):
    def __init__(self, state_dim, hidden_size, n_actions):
        super().__init__()
        self.model = build_mlp(
            [state_dim] + list(hidden_size) + [n_actions], activation=nn.ReLU()
        )
        
    def dist(self, obs):
        scores = self.model(obs)
        probs = torch.softmax(scores, dim=-1)
        return torch.distributions.Categorical(probs)

    def forward(self, t, *, stochastic=True, predict_proba=False, compute_entropy=False, **kwargs):
        """
        Compute the action given either a time step (looking into the workspace)
        or an observation (in kwargs)
        """
        if "observation" in kwargs:
            observation = kwargs["observation"]
        else:
            observation = self.get(("env/env_obs", t))
        scores = self.model(observation)
        probs = torch.softmax(scores, dim=-1)

        if predict_proba:
            action = self.get(("action", t))
            log_prob = probs[torch.arange(probs.size()[0]), action].log()
            self.set(("logprob_predict", t), log_prob)
        else:
            if stochastic:
                action = torch.distributions.Categorical(probs).sample()
            else:
                action = scores.argmax(1)

            log_probs = probs[torch.arange(probs.size()[0]), action].log()

            self.set(("action", t), action)
            self.set(("action_logprobs", t), log_probs)

        if compute_entropy:
            entropy = torch.distributions.Categorical(probs).entropy()
            self.set(("entropy", t), entropy)

    def predict_action(self, obs, stochastic):
        scores = self.model(obs)

        if stochastic:
            probs = torch.softmax(scores, dim=-1)
            action = torch.distributions.Categorical(probs).sample()
        else:
            action = scores.argmax(0)
        return action



class TunableVarianceContinuousActor(BaseActor):
    def __init__(self, state_dim, hidden_layers, action_dim):
        super().__init__()
        layers = [state_dim] + list(hidden_layers) + [action_dim]
        self.model = build_mlp(layers, activation=nn.ReLU())

        # The standard deviation associated with each dimension
        self.std_param = nn.parameter.Parameter(torch.randn(action_dim, 1))
        
        # We use the softplus function to compute the variance for the normal
        # The base version computes exp(1+log(x)) component-wise
        # https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html
        self.soft_plus = torch.nn.Softplus()

    def dist(self, obs: torch.Tensor):
        mean = self.model(obs)
        return Independent(Normal(mean, self.soft_plus(self.std_param)), 1)    
            
    def forward(self, t, *, stochastic=True, predict_proba=False, compute_entropy=False, **kwargs):
        obs = self.get(("env/env_obs", t))
        dist = self.dist(obs)

        if predict_proba:
            action = self.get(("action", t))
            self.set(("logprob_predict", t), dist.log_prob(action))
        else:
            action = dist.sample() if stochastic else dist.mean
            logp_pi = dist.log_prob(action)

            self.set(("action", t), action)
            self.set(("action_logprobs", t), logp_pi)

        if compute_entropy:
            self.set(("entropy", t), dist.entropy())

    def predict_action(self, obs, stochastic):
        """Predict just one action (without using the workspace)"""
        dist = self.dist(obs)
        action = dist.sample() if stochastic else dist.mean
        return action



class VAgent(Agent):
    def __init__(self, state_dim, hidden_layers):
        super().__init__()
        self.is_q_function = False
        self.model = build_mlp(
            [state_dim] + list(hidden_layers) + [1], activation=nn.ReLU()
        )

    def forward(self, t, **kwargs):
        observation = self.get(("env/env_obs", t))
        critic = self.model(observation).squeeze(-1)
        self.set(("v_value", t), critic)



class KLAgent(Agent):
    def __init__(self, model_1, model_2):
        super().__init__()
        self.model_1 = model_1
        self.model_2 = model_2

    def forward(self, t, **kwargs):
        obs = self.get(("env/env_obs", t))
        
        dist_1 = self.model_1.dist(obs)
        dist_2 = self.model_2.dist(obs)
        kl = torch.distributions.kl.kl_divergence(dist_1, dist_2)
        self.set(("kl", t), kl)




def create_ppo_agent(cfg, train_env_agent, eval_env_agent, needs_kl=None):
    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()

    if train_env_agent.is_continuous_action():
        action_agent = TunableVarianceContinuousActor(
            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size
        )
    else:
        action_agent = DiscreteActor(
            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size
        )

    tr_agent = Agents(train_env_agent, action_agent)
    ev_agent = Agents(eval_env_agent, action_agent)

    critic_agent = TemporalAgent(
        VAgent(obs_size, cfg.algorithm.architecture.critic_hidden_size)
    )

    train_agent = TemporalAgent(tr_agent)
    eval_agent = TemporalAgent(ev_agent)
    train_agent.seed(cfg.algorithm.seed)

    old_policy = copy.deepcopy(action_agent)
    old_critic_agent = copy.deepcopy(critic_agent)
    
    kl_agent = None
    if needs_kl:
        kl_agent = TemporalAgent(KLAgent(old_policy, action_agent))

    return action_agent, train_agent, eval_agent, critic_agent, old_policy, old_critic_agent, kl_agent




def compute_advantage_loss(cfg, reward, must_bootstrap, v_value):
    # Compute temporal difference with GAE
    advantage = gae(
        v_value,
        reward,
        must_bootstrap,
        cfg.algorithm.discount_factor,
        cfg.algorithm.gae,
    )
    # Compute critic loss
    td_error = advantage**2
    critic_loss = td_error.mean()
    return critic_loss, advantage




def compute_clip_agent_loss(cfg, advantage, ratio, kl):
    """Computes the PPO CLIP loss
    """
    clip_range = cfg.clip_range
    actor_loss = torch.min(
        advantage * ratio,
        advantage * torch.clamp(ratio, 1 - clip_range, 1 + clip_range)
    ).mean()
    return actor_loss




def setup_optimizer(cfg, action_agent, critic_agent):
    optimizer_args = get_arguments(cfg.optimizer)
    parameters = nn.Sequential(action_agent, critic_agent).parameters()
    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)
    return optimizer




def run_ppo(cfg, variant="clip", compute_actor_loss=compute_clip_agent_loss, needs_kl=False, verbose=False):
    # 1)  Build the  logger
    logger = Logger(cfg)
    best_reward = -10e9

    # 2) Create the environment agent
    train_env_agent = AutoResetGymAgent(
        get_class(cfg.gym_env),
        get_arguments(cfg.gym_env),
        cfg.algorithm.n_envs,
        cfg.algorithm.seed,
    )
    
    eval_env_agent = NoAutoResetGymAgent(
        get_class(cfg.gym_env),
        get_arguments(cfg.gym_env),
        cfg.algorithm.nb_evals,
        cfg.algorithm.seed,
    )

    (
        policy,
        train_agent,
        eval_agent,
        critic_agent,
        old_policy,
        old_critic_agent,
        kl_agent
    ) = create_ppo_agent(cfg, train_env_agent, eval_env_agent, needs_kl=needs_kl)
    
    
    action_agent = TemporalAgent(policy)
    old_train_agent = TemporalAgent(old_policy)
    train_workspace = Workspace()

    # Configure the optimizer
    optimizer = setup_optimizer(cfg, train_agent, critic_agent)
    nb_steps = 0
    tmp_steps = 0

    # Training loop
    for epoch in tqdm(range(cfg.algorithm.max_epochs)):
        # Execute the agent in the workspace
        
        # Handles continuation
        delta_t = 0
        if epoch > 0:
            train_workspace.zero_grad()
            delta_t = 1
            train_workspace.copy_n_last_steps(delta_t)

        # Run the train/old_train agents
        train_agent(
            train_workspace,
            t=delta_t,
            n_steps=cfg.algorithm.n_steps - delta_t,
            stochastic=True,
            predict_proba=False,
            compute_entropy=False
        )
        old_train_agent(
            train_workspace,
            t=delta_t,
            n_steps=cfg.algorithm.n_steps - delta_t,
            # Just computes the probability
            predict_proba=True,
        )

        # Compute the critic value over the whole workspace
        critic_agent(train_workspace, n_steps=cfg.algorithm.n_steps)

        transition_workspace = train_workspace.get_transitions()
        done, truncated, reward, action, action_logp, v_value = transition_workspace[
            "env/done",
            "env/truncated",
            "env/reward",
            "action",
            "action_logprobs",
            "v_value",
        ]

        nb_steps += action[0].shape[0]

        # Determines whether values of the critic should be propagated
        # True if the episode reached a time limit or if the task was not done
        # See https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing
        must_bootstrap = torch.logical_or(~done[1], truncated[1])

        with torch.no_grad():
            old_critic_agent(train_workspace, n_steps=cfg.algorithm.n_steps)
        old_action_logp = transition_workspace["logprob_predict"].detach()
        old_v_value = transition_workspace["v_value"]
        if cfg.algorithm.clip_range_vf > 0:
            # Clip the difference between old and new values
            # NOTE: this depends on the reward scaling
            v_value = old_v_value + torch.clamp(
                v_value - old_v_value,
                -cfg.algorithm.clip_range_vf,
                cfg.algorithm.clip_range_vf,
            )
            
        critic_loss, advantage = compute_advantage_loss(
            cfg, reward, must_bootstrap, v_value
        )
        
        # We store the advantage into the transition_workspace
        advantage = advantage.detach().squeeze(0)
        transition_workspace.set("advantage", 0, advantage)
        transition_workspace.set("advantage", 1, torch.zeros_like(advantage))
        transition_workspace.set_full("old_action_logprobs", transition_workspace["logprob_predict"].detach())
        transition_workspace.clear("logprob_predict")
    
        for opt_epoch in range(cfg.algorithm.opt_epochs):
            if cfg.algorithm.minibatch_size > 0:
                sample_workspace = transition_workspace.sample_subworkspace(1, cfg.algorithm.minibatch_size, 2)
            else:
                sample_workspace = transition_workspace
                                 
            if opt_epoch > 0:
                critic_loss = 0. # We don't want to optimize the critic after the first mini-epoch

            action_agent(sample_workspace, t=0, n_steps=1, compute_entropy=True, predict_proba=True)

            advantage, action_logp, old_action_logp, entropy = sample_workspace[
                "advantage",
                "logprob_predict",
                "old_action_logprobs",
                "entropy"
            ]
            advantage = advantage[0]
            act_diff = action_logp[0] - old_action_logp[0]
            ratios = act_diff.exp()

            kl = None
            if kl_agent:
                kl_agent(sample_workspace, t=0, n_steps=1)
                kl = sample_workspace["kl"][0]

            actor_loss = compute_actor_loss(
                cfg.algorithm, advantage, ratios, kl
            )
                            
            # Entropy loss favor exploration
            entropy_loss = torch.mean(entropy[0])

            # Store the losses for tensorboard display
            if opt_epoch == 0:
                # Just for the first epoch
                logger.log_losses(nb_steps, critic_loss, entropy_loss, actor_loss)

            loss = (
                cfg.algorithm.critic_coef * critic_loss
                - cfg.algorithm.actor_coef * actor_loss
                - cfg.algorithm.entropy_coef * entropy_loss
            )
            

            old_policy.copy_parameters(policy)
            old_critic_agent = copy.deepcopy(critic_agent)


            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                critic_agent.parameters(), cfg.algorithm.max_grad_norm
            )
            torch.nn.utils.clip_grad_norm_(
                train_agent.parameters(), cfg.algorithm.max_grad_norm
            )
            optimizer.step() 

        # Evaluate if enough steps have been performed
        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:
            tmp_steps = nb_steps
            eval_workspace = Workspace()  # Used for evaluation
            eval_agent(
                eval_workspace,
                t=0,
                stop_variable="env/done",
                stochastic=True,
                predict_proba=False,
            )
            rewards = eval_workspace["env/cumulated_reward"][-1]
            mean = rewards.mean()
            logger.add_log("reward_mean", mean, nb_steps)
            logger.add_log("reward_max", rewards.max(), nb_steps)
            logger.add_log("reward_min", rewards.min(), nb_steps)
            logger.add_log("reward_std", rewards.std(), nb_steps)
            if verbose: print(f"nb_steps: {nb_steps}, reward: {mean}")
            if cfg.save_best and mean > best_reward:
                best_reward = mean
                directory = f"./ppo_agent/{cfg.gym_env.env_name}/{variant}/"
                if not os.path.exists(directory):
                    os.makedirs(directory)
                filename = directory + "ppo_" + str(mean.item()) + ".agt"
                policy.save_model(filename)

"""# SAC"""



"""# LUNARLANDER-DISCRETE / PPO"""

ENV = "LunarLander-v2"

# PPO LunarLander-v2:
#   n_envs: 16
#   n_timesteps: !!float 1e6
#   policy: 'MlpPolicy'
#   n_steps: 1024
#   batch_size: 64
#   gae_lambda: 0.98
#   gamma: 0.999
#   n_epochs: 4
#   ent_coef: 0.01

# !rm -rf ./tblogs

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir ./tblogs/ppo

# n_envss = [8, 16]
gae_lambdas = [0.90, 0.94, 0.98]
gammas = [0.95, 0.99, 0.999]
# ent_coefs = [1e-4, 1e-3, 1e-2]
archs = [[64, 64], [128, 128]]
lrs = [1e-4, 1e-3]

for gae_lambda in gae_lambdas:
  for gamma in gammas:
    for arch in archs:
      for lr in lrs:
        params_ppo={
          "save_best": True,
          "plot_policy": True,

          "logger":{
            "classname": "bbrl.utils.logger.TFLogger",
            "log_dir": f"{os.getcwd()}/tblogs/ppo",
            "cache_size": 10000,
            "every_n_seconds": 10,
            "verbose": False,    
            },

          "algorithm":{
            "seed": 4,
            "n_envs": 8,
            "max_grad_norm": 0.5,
            "nb_evals":10,
            "n_steps": 20,
            "eval_interval": 1000,
            "max_epochs": 3000,
            "discount_factor": gamma,
            "entropy_coef": 2.55e-5,
            "beta_kl": 1,
            "critic_coef": 0.6,
            "actor_coef": 1.0,
            "gae": gae_lambda,
            "clip_range": 0.2,
            "clip_range_vf": 0,
            "opt_epochs": 1,
            "minibatch_size": 0,
            "architecture":{
              "actor_hidden_size": arch,
              "critic_hidden_size": arch,
            },
          },
          "gym_env":{
            "classname": "__main__.make_gym_env",
            "env_name": ENV,
            },
          "optimizer":{
            "classname": "torch.optim.Adam",
            "lr": lr,
          }
        }

        config_ppo=OmegaConf.create(params_ppo)
        torch.manual_seed(config_ppo.algorithm.seed)

        print("--> Testing HP")
        print("       gae_lambda:", gae_lambda)
        print("       gamma:", gamma)
        print("       arch:", arch)
        print("       lr:", lr)
        start = time.time()
        run_ppo(config_ppo, verbose=False)
        print("--> Done in:", time.time() - start, "s")

"""# LUNARLANDER-CONTINUOUS / TD3 & SAC"""

ENV = "LunarLanderContinuous-v2"

# TD3 LunarLanderContinuous-v2:
#   n_timesteps: !!float 3e5
#   policy: 'MlpPolicy'
#   gamma: 0.98
#   buffer_size: 200000
#   learning_starts: 10000
#   noise_type: 'normal'
#   noise_std: 0.1
#   gradient_steps: -1
#   train_freq: [1, "episode"]
#   learning_rate: !!float 1e-3
#   policy_kwargs: "dict(net_arch=[400, 300])"

# SAC LunarLanderContinuous-v2:
#   n_timesteps: !!float 5e5
#   policy: 'MlpPolicy'
#   batch_size: 256
#   learning_rate: lin_7.3e-4
#   buffer_size: 1000000
#   ent_coef: 'auto'
#   gamma: 0.99
#   tau: 0.01
#   train_freq: 1
#   gradient_steps: 1
#   learning_starts: 10000
#   policy_kwargs: "dict(net_arch=[400, 300])"

params_td3={
  "save_best": True,
  "plot_agents": True,
  "logger":{
    "classname": "bbrl.utils.logger.TFLogger",
    "log_dir": "./tmp/" + str(time.time()),
    "cache_size": 10000,
    "every_n_seconds": 10,
    "verbose": False,    
    },

  "algorithm":{
    "seed": 1,
    "max_grad_norm": 0.5,
    "epsilon": 0.02,
    "n_envs": 1,
    "n_steps": 100,
    "eval_interval": 2000,
    "nb_evals": 10,
    "gae": 0.8,
    "max_epochs": 15000,
    "discount_factor": 0.98,
    "buffer_size": 2e5,
    "batch_size": 64,
    "tau_target": 0.05,
    "learning_starts": 10000,
    "action_noise": 0.1,
    "architecture":{
        "actor_hidden_size": [400, 300],
        "critic_hidden_size": [400, 300],
        },
  },
  "gym_env":{
    "classname": "__main__.make_gym_env",
    "env_name": ENV,
  },
  "actor_optimizer":{
    "classname": "torch.optim.Adam",
    "lr": 1e-3,
  },
  "critic_optimizer":{
    "classname": "torch.optim.Adam",
    "lr": 1e-3,
  }
}

config_td3=OmegaConf.create(params_td3)
torch.manual_seed(config_td3.algorithm.seed)

params_sac={
  "save_best": True,
  "plot_agents": True,
  "logger":{
    "classname": "bbrl.utils.logger.TFLogger",
    "log_dir": "./tmp/" + str(time.time()),
    "cache_size": 10000,
    "every_n_seconds": 10,
    "verbose": False,    
    },

  "algorithm":{
    "seed": 1,
    "max_grad_norm": 0.5,
    "epsilon": 0.02,
    "n_envs": 1,
    "n_steps": 100,
    "eval_interval": 2000,
    "nb_evals": 10,
    "gae": 0.8,
    "max_epochs": 15000,
    "discount_factor": 0.98,
    "buffer_size": 2e5,
    "batch_size": 64,
    "tau_target": 0.05,
    "learning_starts": 10000,
    "action_noise": 0.1,
    "architecture":{
        "actor_hidden_size": [400, 300],
        "critic_hidden_size": [400, 300],
        },
  },
  "gym_env":{
    "classname": "__main__.make_gym_env",
    "env_name": ENV,
  },
  "actor_optimizer":{
    "classname": "torch.optim.Adam",
    "lr": 1e-3,
  },
  "critic_optimizer":{
    "classname": "torch.optim.Adam",
    "lr": 1e-3,
  }
}

config_sac=OmegaConf.create(params_sac)
torch.manual_seed(config_sac.algorithm.seed)

# run_td3(config_td3)
# run_sac(config_sac)

