{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: easypip in /home/hanzopgp/miniconda3/envs/deepdac/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: packaging in /home/hanzopgp/miniconda3/envs/deepdac/lib/python3.9/site-packages (from easypip) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/hanzopgp/miniconda3/envs/deepdac/lib/python3.9/site-packages (from packaging->easypip) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install easypip\n",
    "\n",
    "from easypip import easyimport\n",
    "import functools\n",
    "import time\n",
    "\n",
    "easyimport(\"importlib_metadata==4.13.0\")\n",
    "OmegaConf = easyimport(\"omegaconf\").OmegaConf\n",
    "bbrl_gym = easyimport(\"bbrl_gym\")\n",
    "bbrl = easyimport(\"bbrl>=0.1.6\")\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.independent import Independent\n",
    "\n",
    "import gym\n",
    "from bbrl.agents.agent import Agent\n",
    "from bbrl import get_arguments, get_class, instantiate_class\n",
    "from bbrl.workspace import Workspace\n",
    "from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n",
    "from bbrl.agents.gymb import AutoResetGymAgent, NoAutoResetGymAgent\n",
    "from bbrl.visu.play import load_agent, play\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer\n",
    "from bbrl.utils.functionalb import gae\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(sizes, activation, output_activation=nn.Identity()):\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = activation if j < len(sizes) - 2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_gym_env(env_name):\n",
    "    return gym.make(env_name)\n",
    "\n",
    "def get_env_agents(cfg):\n",
    "    train_env_agent = AutoResetGymAgent(\n",
    "        get_class(cfg.gym_env),\n",
    "        get_arguments(cfg.gym_env),\n",
    "        cfg.algorithm.n_envs,\n",
    "        cfg.algorithm.seed,\n",
    "    )\n",
    "    eval_env_agent = NoAutoResetGymAgent(\n",
    "    get_class(cfg.gym_env),\n",
    "    get_arguments(cfg.gym_env),\n",
    "    cfg.algorithm.nb_evals,\n",
    "    cfg.algorithm.seed,\n",
    "    )\n",
    "    return train_env_agent, eval_env_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "\tdef __init__(self, cfg, log_string):\n",
    "\t\tself.logger = instantiate_class(cfg.logger)\n",
    "\n",
    "\tdef add_log(self, log_string, loss, epoch):\n",
    "\t\tself.logger.add_scalar(log_string, loss.item(), epoch)\n",
    "\n",
    "\tdef add_eps(self, log_string, eps, epoch):\n",
    "\t\tself.logger.add_scalar(log_string, eps, epoch)\n",
    "\t\t\n",
    "\tdef log_losses(self, epoch, critic_loss, entropy_loss, actor_loss):\n",
    "\t\tself.add_log(\"critic_loss\", critic_loss, epoch)\n",
    "\t\tself.add_log(\"entropy_loss\", entropy_loss, epoch)\n",
    "\t\tself.add_log(\"actor_loss\", actor_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteQAgent(Agent):\n",
    "    \"\"\"BBRL agent (discrete actions) based on a MLP\"\"\"\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = build_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t: int, choose_action=True, **kwargs):\n",
    "        \"\"\"An Agent can use self.workspace\"\"\"\n",
    "\n",
    "        # Retrieves the observation from the environment at time t\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "\n",
    "        # Computes the critic (Q) values for the observation\n",
    "        q_values = self.model(obs)\n",
    "\n",
    "        # ... and sets the q-values (one for each possible action)\n",
    "        self.set((\"q_values\", t), q_values)\n",
    "\n",
    "        # Flag to toggle the fact that the action is chosen\n",
    "        # by this agent; otherwise, we use a specific agent\n",
    "        # (ex. epsilon-greedy) that implements the current policy,\n",
    "        # see below (Exploration method)\n",
    "        if choose_action:\n",
    "            action = q_values.argmax(1)\n",
    "            self.set((\"action\", t), action)\n",
    "\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def forward(self, t: int, choose_action=True, **kwargs):\n",
    "        \"\"\"An Agent can use self.workspace\"\"\"\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = torch.randint(0, self.action_dim, (len(obs), ))\n",
    "        self.set((\"action\", t), action)\n",
    "\n",
    "\n",
    "# class EGreedyActionSelector(Agent):\n",
    "# \t\tdef __init__(self, epsilon, decay_rate, min_eps, logger):\n",
    "# \t\t\t\tsuper().__init__()\n",
    "# \t\t\t\tself.epsilon = epsilon\n",
    "# \t\t\t\tself.min_eps = min_eps\n",
    "# \t\t\t\tself.decay_rate = decay_rate\n",
    "# \t\t\t\tself.nb_epoch = 0\n",
    "# \t\t\t\tself.logger = logger\n",
    "\n",
    "# \t\tdef forward(self, t, **kwargs):\n",
    "# \t\t\t\tself.epsilon = (1/(1+self.decay_rate*self.nb_epoch)) * self.epsilon\n",
    "# \t\t\t\tif self.epsilon < self.min_eps:\n",
    "# \t\t\t\t\tself.epsilon = self.min_eps\n",
    "# \t\t\t\tself.logger.add_eps(\"epsilon\", self.epsilon, self.nb_epoch*500)\n",
    "# \t\t\t\tq_values = self.get((\"q_values\", t))\n",
    "# \t\t\t\tnb_actions = q_values.size()[1]\n",
    "# \t\t\t\tsize = q_values.size()[0]\n",
    "# \t\t\t\tis_random = torch.rand(size).lt(self.epsilon).float()\n",
    "# \t\t\t\trandom_action = torch.randint(low=0, high=nb_actions, size=(size,))\n",
    "# \t\t\t\tmax_action = q_values.max(1)[1]\n",
    "# \t\t\t\taction = is_random * random_action + (1 - is_random) * max_action\n",
    "# \t\t\t\taction = action.long()\n",
    "# \t\t\t\tself.set((\"action\", t), action)\n",
    "\n",
    "\n",
    "class EGreedyActionSelector(Agent):\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        # Retrieves the q values \n",
    "        # (matrix nb. of episodes x nb. of actions)\n",
    "        q_values = self.get((\"q_values\", t))\n",
    "        size, nb_actions = q_values.size()\n",
    "\n",
    "        # Flag \n",
    "        is_random = torch.rand(size).lt(self.epsilon).float()\n",
    "        random_action = torch.randint(low=0, high=nb_actions, size=(size,))\n",
    "        max_action = q_values.max(1)[1]\n",
    "\n",
    "        # Choose the action based on the is_random flag\n",
    "        action = is_random * random_action + (1 - is_random) * max_action\n",
    "\n",
    "        # Sets the action at time t\n",
    "        self.set((\"action\", t), action.long())\n",
    "\n",
    "\n",
    "# Configure the optimizer over the q agent\n",
    "def setup_optimizers(cfg, q_agent):\n",
    "    optimizer_args = get_arguments(cfg.optimizer)\n",
    "    parameters = q_agent.parameters()\n",
    "    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def create_dqn_agent(cfg, train_env_agent, eval_env_agent):\n",
    "\t\tobs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n",
    "\n",
    "\t\t# Get the two agents (critic and target critic)\n",
    "\t\tcritic = DiscreteQAgent(\n",
    "\t\t\t\tobs_size, cfg.algorithm.architecture.hidden_size, act_size)\n",
    "\t\ttarget_critic = copy.deepcopy(critic)\n",
    "\n",
    "\t\t# Builds the train agent that will produce transitions\n",
    "\t\texplorer = EGreedyActionSelector(\n",
    "\t\t\t\tcfg.algorithm.epsilon\n",
    "\t\t\t\t)\n",
    "\t\ttr_agent = Agents(train_env_agent, critic, explorer)\n",
    "\t\ttrain_agent = TemporalAgent(tr_agent)\n",
    "\n",
    "\t\t# Creates two temporal agents just for \"replaying\" some parts\n",
    "\t\t# of the transition buffer\n",
    "\t\tq_agent = TemporalAgent(critic)\n",
    "\t\ttarget_q_agent = TemporalAgent(target_critic)\n",
    "\n",
    "\t\t# Get an agent that is executed on a complete workspace\n",
    "\t\tev_agent = Agents(eval_env_agent, critic)\n",
    "\t\teval_agent = TemporalAgent(ev_agent)\n",
    "\t\ttrain_agent.seed(cfg.algorithm.seed)\n",
    "\n",
    "\t\treturn train_agent, eval_agent, q_agent, target_q_agent\n",
    "\t\t\n",
    "\n",
    "def compute_ddqn_loss(cfg, reward, must_bootstrap, q_values, target_q_values, action):\n",
    "    max_q = target_q_values.max(1)[0].detach()\n",
    "\n",
    "    target = (\n",
    "        reward[:-1] + cfg.algorithm.discount_factor * max_q * must_bootstrap.int()\n",
    "    )\n",
    "\n",
    "    qvals = q_values[0].gather(1, action[0].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Compute critic loss\n",
    "    td = target - qvals\n",
    "    td_error = td**2\n",
    "    critic_loss = td_error.mean()\n",
    "\n",
    "    return critic_loss\n",
    "\n",
    "\n",
    "def run_dqn(cfg, compute_critic_loss, log_string):\n",
    "    # 1)  Build the  logger\n",
    "    logger = Logger(cfg, log_string)\n",
    "    best_reward = -10e9\n",
    "\n",
    "    # 2) Create the environment agent\n",
    "    train_env_agent, eval_env_agent = get_env_agents(cfg)\n",
    "\n",
    "    # 3) Create the DQN-like Agent\n",
    "    train_agent, eval_agent, q_agent, target_q_agent = create_dqn_agent(\n",
    "        cfg, train_env_agent, eval_env_agent\n",
    "    )\n",
    "\n",
    "    # 5) Configure the workspace to the right dimension\n",
    "    # Note that no parameter is needed to create the workspace.\n",
    "    # In the training loop, calling the agent() and critic_agent()\n",
    "    # will take the workspace as parameter\n",
    "    train_workspace = Workspace()  # Used for training\n",
    "    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n",
    "\n",
    "    # 6) Configure the optimizer over the a2c agent\n",
    "    optimizer = setup_optimizers(cfg, q_agent)\n",
    "    nb_steps = 0\n",
    "    last_eval_step = 0\n",
    "    last_critic_update_step = 0\n",
    "    \n",
    "    list_mean = []\n",
    "    list_std = []\n",
    "\n",
    "    # 7) Training loop\n",
    "    for epoch in tqdm(range(cfg.algorithm.max_epochs)):\n",
    "        # Execute the agent in the workspace\n",
    "        if epoch > 0:\n",
    "            train_workspace.zero_grad()\n",
    "            train_workspace.copy_n_last_steps(1)\n",
    "            train_agent(\n",
    "                train_workspace, t=1, n_steps=cfg.algorithm.n_steps - 1, stochastic=True\n",
    "            )\n",
    "        else:\n",
    "            train_agent(\n",
    "                train_workspace, t=0, n_steps=cfg.algorithm.n_steps, stochastic=True\n",
    "            )\n",
    "\n",
    "        # Get the transitions\n",
    "        transition_workspace = train_workspace.get_transitions()\n",
    "\n",
    "        action = transition_workspace[\"action\"]\n",
    "        nb_steps += action[0].shape[0]\n",
    "        \n",
    "        # Adds the transitions to the workspace\n",
    "        rb.put(transition_workspace)\n",
    "        for _ in range(cfg.algorithm.n_updates):\n",
    "            rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n",
    "\n",
    "            # The q agent needs to be executed on the rb_workspace workspace (gradients are removed in workspace)\n",
    "            q_agent(rb_workspace, t=0, n_steps=2, choose_action=False)\n",
    "            q_values, done, truncated, reward, action = rb_workspace[\n",
    "                \"q_values\", \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n",
    "            ]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                target_q_agent(rb_workspace, t=0, n_steps=2, stochastic=True)\n",
    "            target_q_values = rb_workspace[\"q_values\"]\n",
    "\n",
    "            # Determines whether values of the critic should be propagated\n",
    "            # True if the episode reached a time limit or if the task was not done\n",
    "            # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj\n",
    "            must_bootstrap = torch.logical_or(~done[1], truncated[1])\n",
    "\n",
    "            if rb.size() > cfg.algorithm.learning_starts:\n",
    "              # Compute critic loss\n",
    "                critic_loss = compute_critic_loss(\n",
    "                  cfg, reward, must_bootstrap, q_values, target_q_values[1], action\n",
    "              )\n",
    "              # Store the loss for tensorboard display\n",
    "                logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(q_agent.parameters(), cfg.algorithm.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                \n",
    "        if nb_steps - last_critic_update_step > cfg.algorithm.target_critic_update:\n",
    "            last_critic_update_step = nb_steps\n",
    "            target_q_agent.agent = copy.deepcopy(q_agent.agent)\n",
    "\n",
    "        if nb_steps - last_eval_step > cfg.algorithm.eval_interval:\n",
    "            last_eval_step = nb_steps\n",
    "            eval_workspace = Workspace()  # Used for evaluation\n",
    "            eval_agent(\n",
    "                eval_workspace, t=0, stop_variable=\"env/done\", choose_action=True\n",
    "            )\n",
    "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
    "            mean = rewards.mean()\n",
    "            list_mean.append(mean)\n",
    "            list_std.append(rewards.std())\n",
    "            \n",
    "            logger.add_log(\"reward\", mean, nb_steps)\n",
    "            # print(f\"epoch: {epoch}, reward: {mean}, std : {rewards.std()}, BEST VALUE: {best_reward}\")\n",
    "            if cfg.save_best and mean > best_reward:\n",
    "                best_reward = mean\n",
    "                directory = \"./dqn_critic/\"\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                filename = directory + \"dqn0_\" + str(mean.item()) + \".agt\"\n",
    "                eval_agent.save_model(filename)\n",
    "\n",
    "    return list_mean, list_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 472/20000 [09:45<6:44:00,  1.24s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m config\u001b[38;5;241m=\u001b[39mOmegaConf\u001b[38;5;241m.\u001b[39mcreate(params)\n\u001b[1;32m     61\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(config\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mrun_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_ddqn_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 178\u001b[0m, in \u001b[0;36mrun_dqn\u001b[0;34m(cfg, compute_critic_loss, log_string)\u001b[0m\n\u001b[1;32m    176\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    177\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mcopy_n_last_steps(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m     \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_workspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstochastic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     train_agent(\n\u001b[1;32m    183\u001b[0m         train_workspace, t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, n_steps\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mn_steps, stochastic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/utils.py:79\u001b[0m, in \u001b[0;36mTemporalAgent.__call__\u001b[0;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m _t \u001b[39m=\u001b[39m t\n\u001b[1;32m     78\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent(workspace, t\u001b[39m=\u001b[39;49m_t, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m     \u001b[39mif\u001b[39;00m stop_variable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         s \u001b[39m=\u001b[39m workspace\u001b[39m.\u001b[39mget(stop_variable, _t)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/utils.py:31\u001b[0m, in \u001b[0;36mAgents.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, workspace, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents:\n\u001b[0;32m---> 31\u001b[0m         a(workspace, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/agent.py:64\u001b[0m, in \u001b[0;36mAgent.__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39massert\u001b[39;00m workspace \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39m[Agent.__call__] workspace must not be None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkspace \u001b[39m=\u001b[39m workspace\n\u001b[0;32m---> 64\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkspace \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/gymb.py:361\u001b[0m, in \u001b[0;36mAutoResetGymAgent.forward\u001b[0;34m(self, t, save_render, render, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput, t \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m    360\u001b[0m \u001b[39massert\u001b[39;00m action\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs, \u001b[39m\"\u001b[39m\u001b[39mIncompatible number of envs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 361\u001b[0m full_obs, reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step(k, action[k], save_render, render)\n\u001b[1;32m    362\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprevious_reward[k] \u001b[39m=\u001b[39m reward\n\u001b[1;32m    363\u001b[0m observations\u001b[39m.\u001b[39mappend(full_obs)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/gymb.py:335\u001b[0m, in \u001b[0;36mAutoResetGymAgent._step\u001b[0;34m(self, k, action, save_render, render)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_step\u001b[39m(\u001b[39mself\u001b[39m, k, action, save_render, render):\n\u001b[1;32m    334\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimestep[k] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 335\u001b[0m     full_obs, reward, done, truncated, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_step(\n\u001b[1;32m    336\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[k], action, k, save_render, render\n\u001b[1;32m    337\u001b[0m     )\n\u001b[1;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m    339\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_running[k] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/bbrl/agents/gymb.py:168\u001b[0m, in \u001b[0;36mGymAgent._make_step\u001b[0;34m(self, env, action, k, save_render, render)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_step\u001b[39m(\u001b[39mself\u001b[39m, env, action, k, save_render, render):\n\u001b[1;32m    166\u001b[0m     action \u001b[39m=\u001b[39m _convert_action(action)\n\u001b[0;32m--> 168\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mTimeLimit.truncated\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m info\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    170\u001b[0m         truncated \u001b[39m=\u001b[39m info[\u001b[39m\"\u001b[39m\u001b[39mTimeLimit.truncated\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/gym/wrappers/time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/gym/envs/box2d/lunar_lander.py:344\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    333\u001b[0m     p\u001b[39m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    334\u001b[0m         (ox \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power, oy \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power),\n\u001b[1;32m    335\u001b[0m         impulse_pos,\n\u001b[1;32m    336\u001b[0m         \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    337\u001b[0m     )\n\u001b[1;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    339\u001b[0m         (\u001b[39m-\u001b[39mox \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power, \u001b[39m-\u001b[39moy \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power),\n\u001b[1;32m    340\u001b[0m         impulse_pos,\n\u001b[1;32m    341\u001b[0m         \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m     )\n\u001b[0;32m--> 344\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworld\u001b[39m.\u001b[39;49mStep(\u001b[39m1.0\u001b[39;49m \u001b[39m/\u001b[39;49m FPS, \u001b[39m6\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m30\u001b[39;49m, \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m30\u001b[39;49m)\n\u001b[1;32m    346\u001b[0m pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mposition\n\u001b[1;32m    347\u001b[0m vel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mlinearVelocity\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/gym/envs/box2d/lunar_lander.py:73\u001b[0m, in \u001b[0;36mContactDetector.BeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     70\u001b[0m     contactListener\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m env\n\u001b[0;32m---> 73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mBeginContact\u001b[39m(\u001b[39mself\u001b[39m, contact):\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m     75\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mlander \u001b[39m==\u001b[39m contact\u001b[39m.\u001b[39mfixtureA\u001b[39m.\u001b[39mbody\n\u001b[1;32m     76\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mlander \u001b[39m==\u001b[39m contact\u001b[39m.\u001b[39mfixtureB\u001b[39m.\u001b[39mbody\n\u001b[1;32m     77\u001b[0m     ):\n\u001b[1;32m     78\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mgame_over \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gammas = [0.99]\n",
    "archs = [[128,64,32]]\n",
    "lrs = [0.0003]\n",
    "n_envss = [8]\n",
    "n_updates = [32]\n",
    "grad_norms = [0.5]\n",
    "epsilons = [0.02]\n",
    "minibatchs = [64]\n",
    "\n",
    "for grad_norm in grad_norms:\n",
    "\tfor gamma in gammas:\n",
    "\t\tfor arch in archs:\n",
    "\t\t\tfor lr in lrs:\n",
    "\t\t\t\tfor epsilon in epsilons:\n",
    "\t\t\t\t\tfor n_envs in n_envss:\n",
    "\t\t\t\t\t\tfor n_update in n_updates:\n",
    "\t\t\t\t\t\t\tfor batch in minibatchs:\n",
    "\t\t\t\t\t\t\t\tlog_string = \"DDQN_opt_\"+str(opt_epochs)+\"_batch_\"+str(batch)+\"_nenvs_\"+str(n_envs)+\"_gamma_\"+str(gamma)+\"_arch_\"+str(arch)+\"_lr_\"+str(lr)+\"_grad_norm_\"+str(grad_norm)+\"_eps_\"+str(epsilon)\n",
    "\t\t\t\t\t\t\t\tparams={\n",
    "\t\t\t\t\t\t\t\t\t\"save_best\": False,\n",
    "\t\t\t\t\t\t\t\t\t\"logger\":{\n",
    "\t\t\t\t\t\t\t\t\t\t\"classname\": \"bbrl.utils.logger.TFLogger\",\n",
    "\t\t\t\t\t\t\t\t\t\t\"log_dir\": \"./tmp/dqn-buffer-\" + str(time.time()),\n",
    "\t\t\t\t\t\t\t\t\t\t\"cache_size\": 10000,\n",
    "\t\t\t\t\t\t\t\t\t\t\"every_n_seconds\": 10,\n",
    "\t\t\t\t\t\t\t\t\t\t\"verbose\": False,    \n",
    "\t\t\t\t\t\t\t\t\t\t},\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\"algorithm\":{\n",
    "\t\t\t\t\t\t\t\t\t\t\"seed\": 4,\n",
    "\t\t\t\t\t\t\t\t\t\t\"n_updates\": n_update, \n",
    "\t\t\t\t\t\t\t\t\t\t\"max_grad_norm\": grad_norm, \n",
    "\t\t\t\t\t\t\t\t\t\t\"epsilon\": epsilon, \n",
    "\t\t\t\t\t\t\t\t\t\t# \"min_eps\": 0.1,\n",
    "\t\t\t\t\t\t\t\t\t\t# \"epsilon_decay\": 5e-6,\n",
    "\t\t\t\t\t\t\t\t\t\t\"epsilon_decay\": 5e-8,\n",
    "\t\t\t\t\t\t\t\t\t\t\"n_envs\": n_envs, \n",
    "\t\t\t\t\t\t\t\t\t\t\"n_steps\": 32,\n",
    "\t\t\t\t\t\t\t\t\t\t\"eval_interval\": 2000,\n",
    "\t\t\t\t\t\t\t\t\t\t\"learning_starts\": 2000,\n",
    "\t\t\t\t\t\t\t\t\t\t\"nb_evals\": 10,\n",
    "\t\t\t\t\t\t\t\t\t\t\"buffer_size\": 50000,\n",
    "\t\t\t\t\t\t\t\t\t\t\"batch_size\": batch, \n",
    "\t\t\t\t\t\t\t\t\t\t\"target_critic_update\": 5000,\n",
    "\t\t\t\t\t\t\t\t\t\t\"max_epochs\": 20000,\n",
    "\t\t\t\t\t\t\t\t\t\t\"discount_factor\": gamma, \n",
    "\t\t\t\t\t\t\t\t\t\t\"architecture\":{\"hidden_size\": arch},\n",
    "\t\t\t\t\t\t\t\t\t},\n",
    "\t\t\t\t\t\t\t\t\t\"gym_env\":{\n",
    "\t\t\t\t\t\t\t\t\t\t\"classname\": \"__main__.make_gym_env\",\n",
    "\t\t\t\t\t\t\t\t\t\t\"env_name\": \"LunarLander-v2\",\n",
    "\t\t\t\t\t\t\t\t\t},\n",
    "\t\t\t\t\t\t\t\t\t\"optimizer\":\n",
    "\t\t\t\t\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\t\t\t\t\"classname\": \"torch.optim.Adam\",\n",
    "\t\t\t\t\t\t\t\t\t\t\"lr\": lr, \n",
    "\t\t\t\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\t\t\t}\n",
    "\n",
    "\t\t\t\t\t\t\t\tconfig=OmegaConf.create(params)\n",
    "\t\t\t\t\t\t\t\ttorch.manual_seed(config.algorithm.seed)\n",
    "\t\t\t\t\t\t\t\trun_dqn(config, compute_ddqn_loss, log_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('deepdac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bf73cbd865082e3cd06c05babb8829fd907c8e55c85361e30fff74a8577746e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
