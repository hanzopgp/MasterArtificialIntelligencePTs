{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/hanzopgp/projects/MasterArtificialIntelligencePTs/S3/RLD/TME_bonus/gym-rocketlander\n",
      "\u001b[31mERROR: file:///home/hanzopgp/projects/MasterArtificialIntelligencePTs/S3/RLD/TME_bonus/gym-rocketlander does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from easypip import easyimport\n",
    "import functools\n",
    "import time\n",
    "\n",
    "easyimport(\"importlib_metadata==4.13.0\")\n",
    "OmegaConf = easyimport(\"omegaconf\").OmegaConf\n",
    "bbrl_gym = easyimport(\"bbrl_gym\")\n",
    "bbrl = easyimport(\"bbrl>=0.1.6\")\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.independent import Independent\n",
    "\n",
    "import gym\n",
    "from bbrl.agents.agent import Agent\n",
    "from bbrl import get_arguments, get_class, instantiate_class\n",
    "from bbrl.workspace import Workspace\n",
    "from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n",
    "from bbrl.agents.gymb import AutoResetGymAgent, NoAutoResetGymAgent\n",
    "from bbrl.visu.play import load_agent, play\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer\n",
    "from bbrl.utils.functionalb import gae\n",
    "\n",
    "# !rm -rf gym-rocketlander\n",
    "# !git clone https://github.com/EmbersArc/gym-rocketlander\n",
    "\n",
    "# !conda uninstall gym-rocketlander\n",
    "!pip install -e ./gym-rocketlander\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from bbrl.utils.distributions import SquashedDiagGaussianDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocketLanderWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Specific wrapper to shape the reward of the rocket lander environment\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(RocketLanderWrapper, self).__init__(env)\n",
    "        self.env = env\n",
    "        self.prev_shaping = None\n",
    "        \n",
    "    def reset(self):\n",
    "        self.prev_shaping = None\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        d = 1\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        # reward shaping\n",
    "        \"\"\"\n",
    "        shaping = -0.5 * (self.env.distance + self.env.speed + abs(self.env.angle) ** 2)\n",
    "        shaping += 0.1 * (\n",
    "            self.env.legs[0].ground_contact + self.env.legs[1].ground_contact\n",
    "        )\n",
    "        if self.prev_shaping is not None:\n",
    "            reward += shaping - self.prev_shaping\n",
    "        self.prev_shaping = shaping\n",
    "        \"\"\"\n",
    "        # print (\"distance\", self.env.distance)\n",
    "        \n",
    "        # shaping = 0.02\n",
    "        shaping = 0.008 * (1 - self.env.distance)\n",
    "        # shaping = 0.1 * (self.env.groundcontact - self.env.speed)\n",
    "        if (\n",
    "            self.env.legs[0].ground_contact > 0\n",
    "            and self.env.legs[1].ground_contact > 0\n",
    "            and self.env.speed < 0.1\n",
    "        ):\n",
    "            d = d * 2\n",
    "            print(\"landed !\")\n",
    "            print (\"speed\", self.env.speed)\n",
    "            shaping += 6.0 * d / self.env.speed\n",
    "        else:\n",
    "          d = 1\n",
    "        reward += shaping\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def old_step(self, action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        # reward shaping\n",
    "        # shaping = -0.5 * (self.env.distance + self.env.speed + abs(self.env.angle) ** 2)\n",
    "        # shaping += 0.1 * (self.env.legs[0].ground_contact + self.env.legs[1].ground_contact)\n",
    "        shaping = 0\n",
    "        if self.prev_shaping is not None:\n",
    "            reward += shaping - self.prev_shaping\n",
    "        self.prev_shaping = shaping\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "\n",
    "class FrameSkip(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Return only every ``skip``-th frame (frameskipping)\n",
    "    :param env: the environment\n",
    "    :param skip: number of ``skip``-th frame\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, skip: int = 1):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        \"\"\"\n",
    "        Step the environment with the given action\n",
    "        Repeat action, sum reward, and max over last observations.\n",
    "        :param action: the action\n",
    "        :return: observation, reward, done, information\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Box([ -1.  -1.  -1.  -1.  -1.  -1.  -1. -inf -inf -inf], [ 1.  1.  1.  1.  1.  1.  1. inf inf inf], (10,), float32)\n",
      "The action space: Box([-1. -1. -1.], [1. 1. 1.], (3,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"RocketLander-v0\")\n",
    "env = RocketLanderWrapper(env)\n",
    "env = FrameSkip(env, skip=1)\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(\"The observation space: {}\".format(obs_space))\n",
    "print(\"The action space: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bbrl.utils.distributions import SquashedDiagGaussianDistribution\n",
    "\n",
    "class SquashedGaussianActor(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        backbone_dim = [state_dim] + list(hidden_layers)\n",
    "        self.layers = build_backbone(backbone_dim, activation=nn.ReLU())\n",
    "        self.backbone = nn.Sequential(*self.layers)\n",
    "        self.last_mean_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
    "        self.last_std_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
    "        self.action_dist = SquashedDiagGaussianDistribution(action_dim)\n",
    "        # std must be positive\n",
    "        self.std_layer = nn.Softplus()\n",
    "\n",
    "    def dist(self, obs: torch.Tensor):\n",
    "        \"\"\"Computes action distributions given observation(s)\"\"\"\n",
    "        backbone_output = self.backbone(obs)\n",
    "        mean = self.last_mean_layer(backbone_output)\n",
    "        std_out = self.last_std_layer(backbone_output)\n",
    "        std = self.std_layer(std_out)\n",
    "        return self.action_dist.make_distribution(mean, std)\n",
    "\n",
    "\n",
    "    def forward(self, t, stochastic):\n",
    "        action_dist = self.dist(self.get((\"env/env_obs\", t)))\n",
    "        action = action_dist.sample() if stochastic else action_dist.mode()\n",
    "\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "        self.set((f\"action\", t), action)\n",
    "        self.set((\"action_logprobs\", t), log_prob)\n",
    "\n",
    "    def predict_action(self, obs, stochastic: bool):\n",
    "        action_dist = self.dist(obs)\n",
    "        action = action_dist.sample() if stochastic else action_dist.mode()\n",
    "        return action\n",
    "\n",
    "\n",
    "class ContinuousQAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.is_q_function = True\n",
    "        self.model = build_mlp(\n",
    "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, detach_actions=False):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = self.get((\"action\", t))\n",
    "        if detach_actions:\n",
    "            action = action.detach()\n",
    "        osb_act = torch.cat((obs, action), dim=1)\n",
    "        q_value = self.model(osb_act)\n",
    "        self.set((\"q_value\", t), q_value)\n",
    "\n",
    "    def predict_value(self, obs, action):\n",
    "        osb_act = torch.cat((obs, action), dim=0)\n",
    "        q_value = self.model(osb_act)\n",
    "        return q_value\n",
    "\n",
    "\n",
    "\n",
    "# Create the SAC Agent\n",
    "def create_sac_agent(cfg, train_env_agent, eval_env_agent):\n",
    "    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n",
    "    assert (\n",
    "        train_env_agent.is_continuous_action()\n",
    "    ), \"SAC code dedicated to continuous actions\"\n",
    "\n",
    "    # Actor\n",
    "    actor = SquashedGaussianActor(\n",
    "        obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "    )\n",
    "\n",
    "    # Train/Test agents\n",
    "    tr_agent = Agents(train_env_agent, actor)\n",
    "    ev_agent = Agents(eval_env_agent, actor)\n",
    "\n",
    "    # Builds the critics\n",
    "    critic_1 = ContinuousQAgent(\n",
    "        obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
    "    )\n",
    "    target_critic_1 = copy.deepcopy(critic_1)\n",
    "    critic_2 = ContinuousQAgent(\n",
    "        obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
    "    )\n",
    "    target_critic_2 = copy.deepcopy(critic_2)\n",
    "\n",
    "    train_agent = TemporalAgent(tr_agent)\n",
    "    eval_agent = TemporalAgent(ev_agent)\n",
    "    train_agent.seed(cfg.algorithm.seed)\n",
    "    return (\n",
    "        train_agent,\n",
    "        eval_agent,\n",
    "        actor,\n",
    "        critic_1,\n",
    "        target_critic_1,\n",
    "        critic_2,\n",
    "        target_critic_2,\n",
    "    )\n",
    "\n",
    "\n",
    "# Configure the optimizer for the actor and critic\n",
    "def setup_optimizers(cfg, actor, critic_1, critic_2):\n",
    "    actor_optimizer_args = get_arguments(cfg.actor_optimizer)\n",
    "    parameters = actor.parameters()\n",
    "    actor_optimizer = get_class(cfg.actor_optimizer)(parameters, **actor_optimizer_args)\n",
    "    critic_optimizer_args = get_arguments(cfg.critic_optimizer)\n",
    "    parameters = nn.Sequential(critic_1, critic_2).parameters()\n",
    "    critic_optimizer = get_class(cfg.critic_optimizer)(\n",
    "        parameters, **critic_optimizer_args\n",
    "    )\n",
    "    return actor_optimizer, critic_optimizer\n",
    "\n",
    "\n",
    "def setup_entropy_optimizers(cfg):\n",
    "    if cfg.algorithm.target_entropy == \"auto\":\n",
    "        entropy_coef_optimizer_args = get_arguments(cfg.entropy_coef_optimizer)\n",
    "        # Note: we optimize the log of the entropy coefficient which is slightly different from the paper\n",
    "        # as discussed in https://github.com/rail-berkeley/softlearning/issues/37\n",
    "        # Comment and code taken from the SB3 version of SAC\n",
    "        log_entropy_coef = torch.log(\n",
    "            torch.ones(1) * cfg.algorithm.entropy_coef\n",
    "        ).requires_grad_(True)\n",
    "        entropy_coef_optimizer = get_class(cfg.entropy_coef_optimizer)(\n",
    "            [log_entropy_coef], **entropy_coef_optimizer_args\n",
    "        )\n",
    "    else:\n",
    "        log_entropy_coef = 0\n",
    "        entropy_coef_optimizer = None\n",
    "    return entropy_coef_optimizer, log_entropy_coef\n",
    "\n",
    "\n",
    "def compute_critic_loss(\n",
    "    cfg, reward, must_bootstrap,\n",
    "    t_actor, \n",
    "    q_agent_1, q_agent_2, \n",
    "    target_q_agent_1, target_q_agent_2, \n",
    "    rb_workspace,\n",
    "    ent_coef\n",
    "):\n",
    "    \"\"\"Computes the critic loss for a set of $S$ transition samples\n",
    "\n",
    "    Args:\n",
    "        cfg: The experimental configuration\n",
    "        reward: _description_\n",
    "        must_bootstrap: Tensor of indicators (2 x S)\n",
    "        t_actor: The actor agent (as a TemporalAgent)\n",
    "        q_agent_1: The first critic (as a TemporalAgent)\n",
    "        q_agent_2: The second critic (as a TemporalAgent)\n",
    "        target_q_agent_1: The target of the first critic\n",
    "        target_q_agent_2: The target of the second critic\n",
    "        rb_workspace: The transition workspace\n",
    "        ent_coef: The entropy coefficient\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: The two critic losses (scalars)\n",
    "    \"\"\"\n",
    "    # Compute q_values from both critics with the actions present in the buffer:\n",
    "    # at t, we have Q(s,a) from the (s,a) in the RB\n",
    "    q_agent_1(rb_workspace, t=0, n_steps=1)\n",
    "    q_values_rb_1 = rb_workspace[\"q_value\"]\n",
    "    \n",
    "    q_agent_2(rb_workspace, t=0, n_steps=1)\n",
    "    q_values_rb_2 = rb_workspace[\"q_value\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Replay the current actor on the replay buffer to get actions of the\n",
    "        # current policy\n",
    "        t_actor(rb_workspace, t=1, n_steps=1, stochastic=True)\n",
    "        action_logprobs_next = rb_workspace[\"action_logprobs\"]\n",
    "\n",
    "        # Compute target q_values from both target critics: at t+1, we have\n",
    "        # Q(s+1,a+1) from the (s+1,a+1) where a+1 has been replaced in the RB\n",
    "\n",
    "        target_q_agent_1(rb_workspace, t=1, n_steps=1)\n",
    "        post_q_values_1 = rb_workspace[\"q_value\"]\n",
    "\n",
    "        target_q_agent_2(rb_workspace, t=1, n_steps=1)\n",
    "        post_q_values_2 = rb_workspace[\"q_value\"]\n",
    "\n",
    "    post_q_values = torch.min(post_q_values_1, post_q_values_2).squeeze(-1)\n",
    "\n",
    "    v_phi = post_q_values[1] - ent_coef * action_logprobs_next\n",
    "    target = (\n",
    "        reward[:-1][0] + cfg.algorithm.discount_factor * v_phi * must_bootstrap.int()\n",
    "    )\n",
    "    td_1 = target - q_values_rb_1[0].squeeze(-1)\n",
    "    td_2 = target - q_values_rb_2[0].squeeze(-1)\n",
    "    td_error_1 = td_1**2\n",
    "    td_error_2 = td_2**2\n",
    "    critic_loss_1 = td_error_1.mean()\n",
    "    critic_loss_2 = td_error_2.mean()\n",
    "\n",
    "    return critic_loss_1, critic_loss_2\n",
    "\n",
    "\n",
    "def soft_update_params(net, target_net, tau):\n",
    "    for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "def compute_actor_loss(ent_coef, t_actor, q_agent_1, q_agent_2, rb_workspace):\n",
    "    \"\"\"Actor loss computation\n",
    "    \n",
    "    :param ent_coef: The entropy coefficient $\\alpha$\n",
    "    :param t_actor: The actor agent (temporal agent)\n",
    "    :param q_agent_1: The first critic (temporal agent)\n",
    "    :param q_agent_2: The second critic (temporal agent)\n",
    "    :param rb_workspace: The replay buffer (2 time steps, $t$ and $t+1$)\n",
    "    \"\"\"\n",
    "    # Recompute the q_values from the current policy, not from the actions in the buffer\n",
    "\n",
    "    t_actor(rb_workspace, t=0, n_steps=1, stochastic=True)\n",
    "    action_logprobs_new = rb_workspace[\"action_logprobs\"]\n",
    "\n",
    "    q_agent_1(rb_workspace, t=0, n_steps=1)\n",
    "    q_values_1 = rb_workspace[\"q_value\"]\n",
    "    q_agent_2(rb_workspace, t=0, n_steps=1)\n",
    "    q_values_2 = rb_workspace[\"q_value\"]\n",
    "\n",
    "    current_q_values = torch.min(q_values_1, q_values_2).squeeze(-1)\n",
    "\n",
    "    actor_loss = ent_coef * action_logprobs_new[0] - current_q_values[0]\n",
    "\n",
    "    return actor_loss.mean()\n",
    "\n",
    "\n",
    "\n",
    "def run_sac(cfg, log_string):\n",
    "    # 1)  Build the  logger\n",
    "    logger = Logger(cfg, log_string)\n",
    "    best_reward = -10e9\n",
    "    ent_coef = cfg.algorithm.entropy_coef\n",
    "\n",
    "    # 2) Create the environment agent\n",
    "    train_env_agent = AutoResetGymAgent(\n",
    "        get_class(cfg.gym_env),\n",
    "        get_arguments(cfg.gym_env),\n",
    "        cfg.algorithm.n_envs,\n",
    "        cfg.algorithm.seed,\n",
    "    )\n",
    "    eval_env_agent = NoAutoResetGymAgent(\n",
    "        get_class(cfg.gym_env),\n",
    "        get_arguments(cfg.gym_env),\n",
    "        cfg.algorithm.nb_evals,\n",
    "        cfg.algorithm.seed,\n",
    "    )\n",
    "\n",
    "    # 3) Create the A2C Agent\n",
    "    (\n",
    "        train_agent,\n",
    "        eval_agent,\n",
    "        actor,\n",
    "        critic_1,\n",
    "        target_critic_1,\n",
    "        critic_2,\n",
    "        target_critic_2,\n",
    "    ) = create_sac_agent(cfg, train_env_agent, eval_env_agent)\n",
    "\n",
    "    t_actor = TemporalAgent(actor)\n",
    "    q_agent_1 = TemporalAgent(critic_1)\n",
    "    target_q_agent_1 = TemporalAgent(target_critic_1)\n",
    "    q_agent_2 = TemporalAgent(critic_2)\n",
    "    target_q_agent_2 = TemporalAgent(target_critic_2)\n",
    "    train_workspace = Workspace()\n",
    "\n",
    "    # Creates a replay buffer\n",
    "    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n",
    "\n",
    "    # Configure the optimizer\n",
    "    actor_optimizer, critic_optimizer = setup_optimizers(cfg, actor, critic_1, critic_2)\n",
    "    entropy_coef_optimizer, log_entropy_coef = setup_entropy_optimizers(cfg)\n",
    "    nb_steps = 0\n",
    "    tmp_steps = 0\n",
    "\n",
    "    # Initial value of the entropy coef alpha. If target_entropy is not auto,\n",
    "    # will remain fixed\n",
    "    if cfg.algorithm.target_entropy == \"auto\":\n",
    "        target_entropy = -np.prod(train_env_agent.action_space.shape).astype(np.float32)\n",
    "    else:\n",
    "        target_entropy = cfg.algorithm.target_entropy\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(cfg.algorithm.max_epochs)):\n",
    "        # Execute the agent in the workspace\n",
    "        if epoch > 0:\n",
    "            train_workspace.zero_grad()\n",
    "            train_workspace.copy_n_last_steps(1)\n",
    "            train_agent(\n",
    "                train_workspace,\n",
    "                t=1,\n",
    "                n_steps=cfg.algorithm.n_steps - 1,\n",
    "                stochastic=True,\n",
    "            )\n",
    "        else:\n",
    "            train_agent(\n",
    "                train_workspace,\n",
    "                t=0,\n",
    "                n_steps=cfg.algorithm.n_steps,\n",
    "                stochastic=True,\n",
    "            )\n",
    "\n",
    "        transition_workspace = train_workspace.get_transitions()\n",
    "        action = transition_workspace[\"action\"]\n",
    "        nb_steps += action[0].shape[0]\n",
    "        rb.put(transition_workspace)\n",
    "\n",
    "        if nb_steps > cfg.algorithm.learning_starts:\n",
    "            # Get a sample from the workspace\n",
    "            rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n",
    "\n",
    "            done, truncated, reward, action_logprobs_rb = rb_workspace[\n",
    "                \"env/done\", \"env/truncated\", \"env/reward\", \"action_logprobs\"\n",
    "            ]\n",
    "\n",
    "            # Determines whether values of the critic should be propagated\n",
    "            # True if the episode reached a time limit or if the task was not done\n",
    "            # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing\n",
    "            must_bootstrap = torch.logical_or(~done[1], truncated[1])\n",
    "\n",
    "            (\n",
    "                critic_loss_1, critic_loss_2\n",
    "            ) = compute_critic_loss(\n",
    "                cfg, \n",
    "                reward, \n",
    "                must_bootstrap,\n",
    "                t_actor,\n",
    "                q_agent_1,\n",
    "                q_agent_2,\n",
    "                target_q_agent_1,\n",
    "                target_q_agent_2,\n",
    "                rb_workspace,\n",
    "                ent_coef\n",
    "            )\n",
    "\n",
    "            logger.add_log(\"critic_loss_1\", critic_loss_1, nb_steps)\n",
    "            logger.add_log(\"critic_loss_2\", critic_loss_2, nb_steps)\n",
    "            critic_loss = critic_loss_1 + critic_loss_2\n",
    "\n",
    "            actor_loss = compute_actor_loss(\n",
    "                ent_coef, t_actor, q_agent_1, q_agent_2, rb_workspace\n",
    "            )\n",
    "            logger.add_log(\"actor_loss\", actor_loss, nb_steps)\n",
    "\n",
    "            # Entropy coef update part #####################################################\n",
    "            if entropy_coef_optimizer is not None:\n",
    "                # Important: detach the variable from the graph\n",
    "                # so that we don't change it with other losses\n",
    "                # see https://github.com/rail-berkeley/softlearning/issues/60\n",
    "                ent_coef = torch.exp(log_entropy_coef.detach())\n",
    "                # See Eq. (17) of the SAC and Applications paper\n",
    "                entropy_coef_loss = -(\n",
    "                    log_entropy_coef * (action_logprobs_rb + target_entropy)\n",
    "                ).mean()\n",
    "                entropy_coef_optimizer.zero_grad()\n",
    "                # We need to retain the graph because we reuse the\n",
    "                # action_logprobs are used to compute both the actor loss and\n",
    "                # the critic loss\n",
    "                entropy_coef_loss.backward(retain_graph=True)\n",
    "                entropy_coef_optimizer.step()\n",
    "                logger.add_log(\"entropy_coef_loss\", entropy_coef_loss, nb_steps)\n",
    "                logger.add_log(\"entropy_coef\", ent_coef, nb_steps)\n",
    "\n",
    "            # Actor update part ###############################\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                actor.parameters(), cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            actor_optimizer.step()\n",
    "\n",
    "\n",
    "            # Critic update part ###############################\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                critic_1.parameters(), cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                critic_2.parameters(), cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            critic_optimizer.step()\n",
    "            ####################################################\n",
    "\n",
    "            # Soft update of target q function\n",
    "            tau = cfg.algorithm.tau_target\n",
    "            soft_update_params(critic_1, target_critic_1, tau)\n",
    "            soft_update_params(critic_2, target_critic_2, tau)\n",
    "            # soft_update_params(actor, target_actor, tau)\n",
    "\n",
    "        # Evaluate ###########################################\n",
    "        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n",
    "            tmp_steps = nb_steps\n",
    "            eval_workspace = Workspace()  # Used for evaluation\n",
    "            eval_agent(\n",
    "                eval_workspace,\n",
    "                t=0,\n",
    "                stop_variable=\"env/done\",\n",
    "                stochastic=False,\n",
    "            )\n",
    "            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n",
    "            mean = rewards.mean()\n",
    "            logger.add_log(\"reward/mean\", mean, nb_steps)\n",
    "            logger.add_log(\"reward/max\", rewards.max(), nb_steps)\n",
    "            logger.add_log(\"reward/min\", rewards.min(), nb_steps)\n",
    "            # logger.add_log(\"reward/med\", rewards.median(), nb_steps)\n",
    "\n",
    "            # print(f\"nb_steps: {nb_steps}, reward: {mean}\")\n",
    "            # print(\"ent_coef\", ent_coef)\n",
    "            if cfg.save_best and mean > best_reward:\n",
    "                best_reward = mean\n",
    "                directory = f\"./agents/rocketlander/sac/\"+log_string\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                filename = directory + \"sac_\" + str(mean.item()) + \".agt\"\n",
    "                actor.save_model(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAC_256_0.5_0.9_1e-07_[128, 128]_[400, 400]_0.0001_5e-05_0.0001sac_1.7579810619354248.agt\n",
      "SAC_256_0.5_0.9_1e-07_128_128_400_400_0.0001_5e-05_0.0001sac_1.7579810619354248.agt:Zone.Identifier\n",
      "SAC_256_0.5_0.9_1e-07_128_128_256_256_0.0001_0.0001_0.0001sac_1.5376551151275635.agt:Zone.Identifier\n",
      "test.agt\n",
      "rocketlander\n",
      "SAC_256_0.5_0.9_1e-07_256_256_256_256_5e-05_0.0005_0.0001sac_1.340839147567749.agt:Zone.Identifier\n",
      "SAC_256_1_0.8_1e-07_128_128_256_256_0.0001_0.0001_0.0001sac_1.4790141582489014.agt:Zone.Identifier\n",
      "SAC_256_0.5_0.9_1e-07_128_128_256_256_0.0001_5e-05_0.0001sac_1.7534806728363037.agt:Zone.Identifier\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(\"agents/\"):\n",
    "\tprint(filename)\n",
    "model = torch.load(\"./agents/SAC_256_0.5_0.9_1e-07_[128, 128]_[400, 400]_0.0001_5e-05_0.0001sac_1.7579810619354248.agt\").double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ContextException",
     "evalue": "Could not create GL context",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContextException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \t\taction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_action(obs, stochastic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \t\tobs, rewards,  dones, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m---> 17\u001b[0m \t\t\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/bbrl_gym/envs/rocket_lander.py:497\u001b[0m, in \u001b[0;36mRocketLanderEnv.render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    495\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassic_control\u001b[39;00m \u001b[39mimport\u001b[39;00m rendering\n\u001b[1;32m    499\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer \u001b[39m=\u001b[39m rendering\u001b[39m.\u001b[39mViewer(VIEWPORT_W, VIEWPORT_H)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/gym/envs/classic_control/rendering.py:27\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     18\u001b[0m         \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m    Cannot import pyglet.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     26\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpyglet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgl\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     29\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     30\u001b[0m         \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m    Error occurred while running `from pyglet.gl import *`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/pyglet/gl/__init__.py:232\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_pyglet_doc_run \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mpyglet.window\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _sys\u001b[39m.\u001b[39mmodules \u001b[39mand\u001b[39;00m _pyglet\u001b[39m.\u001b[39moptions[\u001b[39m'\u001b[39m\u001b[39mshadow_window\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    230\u001b[0m     \u001b[39m# trickery is for circular import\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     _pyglet\u001b[39m.\u001b[39mgl \u001b[39m=\u001b[39m _sys\u001b[39m.\u001b[39mmodules[\u001b[39m__name__\u001b[39m]\n\u001b[0;32m--> 232\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpyglet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwindow\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/pyglet/window/__init__.py:1919\u001b[0m\n\u001b[1;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_pyglet_doc_run:\n\u001b[1;32m   1918\u001b[0m     pyglet\u001b[39m.\u001b[39mwindow \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules[\u001b[39m__name__\u001b[39m]\n\u001b[0;32m-> 1919\u001b[0m     gl\u001b[39m.\u001b[39;49m_create_shadow_window()\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/pyglet/gl/__init__.py:206\u001b[0m, in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyglet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwindow\u001b[39;00m \u001b[39mimport\u001b[39;00m Window\n\u001b[0;32m--> 206\u001b[0m _shadow_window \u001b[39m=\u001b[39m Window(width\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, height\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, visible\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    207\u001b[0m _shadow_window\u001b[39m.\u001b[39mswitch_to()\n\u001b[1;32m    209\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyglet\u001b[39;00m \u001b[39mimport\u001b[39;00m app\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/pyglet/window/xlib/__init__.py:171\u001b[0m, in \u001b[0;36mXlibWindow.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_handlers[message] \u001b[39m=\u001b[39m func\n\u001b[0;32m--> 171\u001b[0m \u001b[39msuper\u001b[39;49m(XlibWindow, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[39mglobal\u001b[39;00m _can_detect_autorepeat\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m _can_detect_autorepeat \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/pyglet/window/__init__.py:616\u001b[0m, in \u001b[0;36mBaseWindow.__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, file_drops, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    613\u001b[0m     config \u001b[39m=\u001b[39m screen\u001b[39m.\u001b[39mget_best_config(config)\n\u001b[1;32m    615\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context:\n\u001b[0;32m--> 616\u001b[0m     context \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39;49mcreate_context(gl\u001b[39m.\u001b[39;49mcurrent_context)\n\u001b[1;32m    618\u001b[0m \u001b[39m# Set these in reverse order to above, to ensure we get user preference\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_context \u001b[39m=\u001b[39m context\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/pyglet/gl/xlib.py:204\u001b[0m, in \u001b[0;36mXlibCanvasConfig13.create_context\u001b[0;34m(self, share)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_context\u001b[39m(\u001b[39mself\u001b[39m, share):\n\u001b[1;32m    203\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglx_info\u001b[39m.\u001b[39mhave_extension(\u001b[39m'\u001b[39m\u001b[39mGLX_ARB_create_context\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 204\u001b[0m         \u001b[39mreturn\u001b[39;00m XlibContextARB(\u001b[39mself\u001b[39;49m, share)\n\u001b[1;32m    205\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         \u001b[39mreturn\u001b[39;00m XlibContext13(\u001b[39mself\u001b[39m, share)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/pyglet/gl/xlib.py:314\u001b[0m, in \u001b[0;36mXlibContext13.__init__\u001b[0;34m(self, config, share)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config, share):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39msuper\u001b[39;49m(XlibContext13, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config, share)\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglx_window \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/pyglet/gl/xlib.py:218\u001b[0m, in \u001b[0;36mBaseXlibContext.__init__\u001b[0;34m(self, config, share)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglx_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_glx_context(share)\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglx_context:\n\u001b[1;32m    217\u001b[0m     \u001b[39m# TODO: Check Xlib error generated\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mraise\u001b[39;00m gl\u001b[39m.\u001b[39mContextException(\u001b[39m'\u001b[39m\u001b[39mCould not create GL context\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_have_SGI_video_sync \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mglx_info\u001b[39m.\u001b[39mhave_extension(\u001b[39m'\u001b[39m\u001b[39mGLX_SGI_video_sync\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_have_SGI_swap_control \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mglx_info\u001b[39m.\u001b[39mhave_extension(\u001b[39m'\u001b[39m\u001b[39mGLX_SGI_swap_control\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mContextException\u001b[0m: Could not create GL context"
     ]
    }
   ],
   "source": [
    "environment_name = \"RocketLander-v0\" #Env\n",
    "env = gym.make(environment_name)\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "\t\t# print(obs)\n",
    "\t\ttry:\n",
    "\t\t\tobs = torch.tensor(obs.astype(np.double), dtype=torch.double)\n",
    "\t\texcept:\n",
    "\t\t\tcontinue\n",
    "\t\t# obs = obs.type(torch.DoubleTensor)\n",
    "\t\t# obs = [float(obs[i].double()) for i in range(len(obs))]\n",
    "\t\t# obs = torch.tensor(obs, dtype=torch.double)\n",
    "\t\t# print(obs)\n",
    "\t\taction = model.predict_action(obs, stochastic=False)\n",
    "\t\tobs, rewards,  dones, info = env.step(action.detach().numpy())\n",
    "\t\tenv.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('deepdac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bf73cbd865082e3cd06c05babb8829fd907c8e55c85361e30fff74a8577746e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
