{"cells":[{"cell_type":"markdown","metadata":{"id":"dYfGJCe52lP4"},"source":["# Outlook"]},{"cell_type":"markdown","metadata":{"id":"aZUSf0n_2otG"},"source":["In this notebook, using BBRL, we code the DDPG algorithm. To understand this code, you need [to know more about BBRL](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing). You should first have a look at [the BBRL interaction model](https://colab.research.google.com/drive/1gSdkOBPkIQi_my9TtwJ-qWZQS0b2X7jt?usp=sharing), then [a first example](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) and, most importantly, [details about the AutoResetGymAgent](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing)."]},{"cell_type":"markdown","metadata":{"id":"HYW4-1lsSwwS"},"source":["The DDPG algorithm is explained in [this video](https://www.youtube.com/watch?v=0D6a0a1HTtc) and you can also read [the corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/ddpg.pdf)."]},{"cell_type":"markdown","metadata":{"id":"zJZDcDafp7Uf"},"source":["## Installation and Imports"]},{"cell_type":"markdown","metadata":{"id":"aHO1nIdM21Lq"},"source":["### Installation"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1780,"status":"ok","timestamp":1665328319879,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"},"user_tz":-120},"id":"sYDSy2eEWyOY","outputId":"01a12da9-0698-48f9-fdd9-60a4d5adebfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: importlib-metadata==4.13.0 in /usr/local/lib/python3.7/dist-packages (4.13.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata==4.13.0) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata==4.13.0) (3.8.1)\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"]}],"source":["!pip install importlib-metadata==4.13.0"]},{"cell_type":"markdown","metadata":{"id":"Ymc-lbXi9vDE"},"source":["The BBRL library is [here](https://github.com/osigaud/bbrl)."]},{"cell_type":"markdown","metadata":{"id":"pDy9yuQH73tJ"},"source":["This is OmegaConf that makes it possible that by just defining the `def run_dqn(cfg):` function and then executing a long `params = {...}` variable at the bottom of this colab, the code is run with the parameters without calling an explicit main.\n","\n","More precisely, the code is run by calling\n","\n","`config=OmegaConf.create(params)`\n","\n","`run_dqn(config)`\n","\n","at the very bottom of the colab, after starting tensorboard."]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1665328319879,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"},"user_tz":-120},"id":"j0MaggiOl4KU"},"outputs":[],"source":["import os\n","import functools\n","import time\n","from typing import Tuple\n","\n","try:\n","    from omegaconf import OmegaConf\n","except ModuleNotFoundError as e:\n","    !pip install omegaconf\n","    from omegaconf import OmegaConf\n","\n","try:\n","    import torch\n","except ModuleNotFoundError as e:\n","    !pip install torch\n","\n","\n","try:\n","    import my_gym\n","    import gym\n","    import bbrl\n","except ModuleNotFoundError as e:\n","    !pip install git+https://github.com/osigaud/bbrl.git\n","    import my_gym\n","    import gym\n","    import bbrl\n"]},{"cell_type":"markdown","metadata":{"id":"m4kV9pWV3wRe"},"source":["### Imports"]},{"cell_type":"markdown","metadata":{"id":"caqhJYbe5YcO"},"source":["Below, we import standard python packages, pytorch packages and gym environments."]},{"cell_type":"markdown","metadata":{"id":"4l7sTVXbJBE_"},"source":["[OpenAI gym](https://gym.openai.com/) is a collection of benchmark environments to evaluate RL algorithms."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"vktQB-AO5biu","executionInfo":{"status":"ok","timestamp":1665328319879,"user_tz":-120,"elapsed":13,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["import copy\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import gym"]},{"cell_type":"markdown","metadata":{"id":"fE1c7ZLf60X_"},"source":["### BBRL imports"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"RcuqoAvG3zMZ","executionInfo":{"status":"ok","timestamp":1665328319879,"user_tz":-120,"elapsed":12,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["from bbrl.agents.agent import Agent\n","from bbrl import get_arguments, get_class, instantiate_class\n","\n","# The workspace is the main class in BBRL, this is where all data is collected and stored\n","from bbrl.workspace import Workspace\n","\n","# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n","# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n","# or until a given condition is reached\n","from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n","\n","# AutoResetGymAgent is an agent able to execute a batch of gym environments\n","# with auto-resetting. These agents produce multiple variables in the workspace: \n","# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/done’, ’env/initial_state’, ’env/cumulated_reward’, \n","# ... When called at timestep t=0, then the environments are automatically reset. \n","# At timestep t>0, these agents will read the ’action’ variable in the workspace at time t − 1\n","from bbrl.agents.gymb import AutoResetGymAgent, NoAutoResetGymAgent\n","# Not present in the A2C version...\n","from bbrl.utils.logger import TFLogger\n","from bbrl.utils.replay_buffer import ReplayBuffer"]},{"cell_type":"markdown","metadata":{"id":"JVvAfhKm9S8p"},"source":["## Definition of agents"]},{"cell_type":"markdown","metadata":{"id":"RdqzKSLKDtqz"},"source":["The [DDPG](https://arxiv.org/pdf/1509.02971.pdf) algorithm is an actor critic algorithm. We use the standard function for building the neural networks that will play the role of the actor and the critic."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"r_QIxxHNtBMH","executionInfo":{"status":"ok","timestamp":1665328319880,"user_tz":-120,"elapsed":13,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["def build_mlp(sizes, activation, output_activation=nn.Identity()):\n","    layers = []\n","    for j in range(len(sizes) - 1):\n","        act = activation if j < len(sizes) - 2 else output_activation\n","        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n","    return nn.Sequential(*layers)"]},{"cell_type":"markdown","metadata":{"id":"9YEh57AHeVB4"},"source":["To implement the gym environment, we use the same approach as usual."]},{"cell_type":"code","execution_count":34,"metadata":{"id":"Fsb5QRzw7V0o","executionInfo":{"status":"ok","timestamp":1665328319880,"user_tz":-120,"elapsed":13,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["def make_gym_env(env_name):\n","    return gym.make(env_name)"]},{"cell_type":"markdown","metadata":{"id":"hgE2yEat36XV"},"source":["The critic is a neural network taking the state $s$ and action $a$ as input, and its output layer has a unique neuron whose value is the value of being in that state and performing that action $Q(s,a)$.\n","\n","As usual, the ```forward(...)``` function is used to write Q-values in the workspace from time indexes, whereas the ```predict_value(...)``` function` is used in other contexts, such as plotting a view of the Q function."]},{"cell_type":"code","execution_count":35,"metadata":{"id":"-ygl_STM3zKa","executionInfo":{"status":"ok","timestamp":1665328319880,"user_tz":-120,"elapsed":13,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["class ContinuousQAgent(Agent):\n","    def __init__(self, state_dim, hidden_layers, action_dim):\n","        super().__init__()\n","        self.is_q_function = True\n","        self.model = build_mlp(\n","            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n","        )\n","\n","    def forward(self, t, detach_actions=False):\n","        obs = self.get((\"env/env_obs\", t))\n","        action = self.get((\"action\", t))\n","        if detach_actions:\n","            action = action.detach()\n","        osb_act = torch.cat((obs, action), dim=1)\n","        q_value = self.model(osb_act)\n","        self.set((\"q_value\", t), q_value)\n","\n","    def predict_value(self, obs, action):\n","        osb_act = torch.cat((obs, action), dim=0)\n","        q_value = self.model(osb_act)\n","        return q_value"]},{"cell_type":"markdown","metadata":{"id":"zW0Vrm2438Hz"},"source":["The actor is also a neural network, it takes a state $s$ as input and outputs an action $a$."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"C67eALQ63mQm","executionInfo":{"status":"ok","timestamp":1665328319880,"user_tz":-120,"elapsed":12,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["class ContinuousDeterministicActor(Agent):\n","    def __init__(self, state_dim, hidden_layers, action_dim):\n","        super().__init__()\n","        layers = [state_dim] + list(hidden_layers) + [action_dim]\n","        self.model = build_mlp(\n","            layers, activation=nn.ReLU(), output_activation=nn.Tanh()\n","        )\n","\n","    def forward(self, t):\n","        obs = self.get((\"env/env_obs\", t))\n","        action = self.model(obs)\n","        self.set((\"action\", t), action)\n","\n","    def predict_action(self, obs, stochastic):\n","        assert (\n","            not stochastic\n","        ), \"ContinuousDeterministicActor cannot provide stochastic predictions\"\n","        return self.model(obs)"]},{"cell_type":"markdown","metadata":{"id":"yoG1eNBguNTN"},"source":["### Creating an Exploration method"]},{"cell_type":"markdown","metadata":{"id":"qvxOy0e180_6"},"source":["In the continuous action domain, basic exploration differs from the methods used in the discrete action domain. Here we generally add some Gaussian noise to the output of the actor."]},{"cell_type":"code","execution_count":37,"metadata":{"id":"ATPprO4B8smL","executionInfo":{"status":"ok","timestamp":1665328319880,"user_tz":-120,"elapsed":11,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["from torch.distributions import Normal"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"IXroJC7D3ZiD","executionInfo":{"status":"ok","timestamp":1665328319880,"user_tz":-120,"elapsed":11,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["class AddGaussianNoise(Agent):\n","    def __init__(self, sigma):\n","        super().__init__()\n","        self.sigma = sigma\n","\n","    def forward(self, t, **kwargs):\n","        act = self.get((\"action\", t))\n","        dist = Normal(act, self.sigma)\n","        action = dist.sample()\n","        self.set((\"action\", t), action)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"d-j7mPDc3g9J"},"source":["In [the original DDPG paper](https://arxiv.org/pdf/1509.02971.pdf), the authors rather used the more sophisticated Ornstein-Uhlenbeck noise where noise is correlated between one step and the next."]},{"cell_type":"code","execution_count":39,"metadata":{"id":"oo3DoKew3dWS","executionInfo":{"status":"ok","timestamp":1665328319881,"user_tz":-120,"elapsed":12,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["class AddOUNoise(Agent):\n","    \"\"\"\n","    Ornstein Uhlenbeck process noise for actions as suggested by DDPG paper\n","    \"\"\"\n","\n","    def __init__(self, std_dev, theta=0.15, dt=1e-2):\n","        self.theta = theta\n","        self.std_dev = std_dev\n","        self.dt = dt\n","        self.x_prev = 0\n","\n","    def forward(self, t, **kwargs):\n","        act = self.get((\"action\", t))\n","        x = (\n","            self.x_prev\n","            + self.theta * (act - self.x_prev) * self.dt\n","            + self.std_dev * math.sqrt(self.dt) * torch.randn(act.shape)\n","        )\n","        self.x_prev = x\n","        self.set((\"action\", t), x)\n"]},{"cell_type":"markdown","metadata":{"id":"W0AgHYc2ywoS"},"source":["### Training and evaluation environments"]},{"cell_type":"markdown","metadata":{"id":"1gfbKZMFrghw"},"source":["We build two environments: one for training and another one for evaluation."]},{"cell_type":"markdown","metadata":{"id":"jDM2Z0THyrtx"},"source":["For training, it is more efficient to use an AutoResetGymAgent, as we do not want to waste time if the task is done in an environment sooner than in the others."]},{"cell_type":"markdown","metadata":{"id":"7kN-SniayxRq"},"source":["By contrast, for evaluation, we just need to perform a fixed number of episodes (for statistics), thus it is more convenient to use a NoAutoResetGymAgent with a set of environments and just run one episode in each environment. Thus we can use the `env/done` stop variable and take the average over the cumulated reward of all environments."]},{"cell_type":"markdown","metadata":{"id":"7_D4Fb4Fz6M1"},"source":["\n","See [this notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) for explanations about agents and environment agents."]},{"cell_type":"code","execution_count":40,"metadata":{"id":"hT5mr2yGyeUP","executionInfo":{"status":"ok","timestamp":1665328319881,"user_tz":-120,"elapsed":12,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["def get_env_agents(cfg):\n","    train_env_agent = AutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.n_envs,\n","        cfg.algorithm.seed,\n","    )\n","    eval_env_agent = NoAutoResetGymAgent(\n","    get_class(cfg.gym_env),\n","    get_arguments(cfg.gym_env),\n","    cfg.algorithm.nb_evals,\n","    cfg.algorithm.seed,\n","    )\n","    return train_env_agent, eval_env_agent"]},{"cell_type":"markdown","metadata":{"id":"EzxoIPtLVJ_i"},"source":["### Create the DDPG agent"]},{"cell_type":"markdown","metadata":{"id":"aaNnZw3bXEYd"},"source":["In this function we create the critic and the actor, but also an exploration agent to add noise and a target critic. The version below does not use a target actor as it proved hard to tune, but such a target actor is used in the original paper."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"G8Uk_RQh8QrO","executionInfo":{"status":"ok","timestamp":1665328319881,"user_tz":-120,"elapsed":11,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["# Create the DDPG Agent\n","def create_ddpg_agent(cfg, train_env_agent, eval_env_agent):\n","    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n","    critic = ContinuousQAgent(\n","        obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n","    )\n","    target_critic = copy.deepcopy(critic)\n","    actor = ContinuousDeterministicActor(\n","        obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n","    )\n","    # target_actor = copy.deepcopy(actor) # not used in practice, though described in the paper\n","    noise_agent = AddGaussianNoise(cfg.algorithm.action_noise) # alternative : AddOUNoise\n","    tr_agent = Agents(train_env_agent, actor, noise_agent)  \n","    ev_agent = Agents(eval_env_agent, actor)\n","\n","    # Get an agent that is executed on a complete workspace\n","    train_agent = TemporalAgent(tr_agent)\n","    eval_agent = TemporalAgent(ev_agent)\n","    train_agent.seed(cfg.algorithm.seed)\n","    return train_agent, eval_agent, actor, critic, target_critic  # , target_actor"]},{"cell_type":"markdown","metadata":{"id":"lU3cO6znHyDc"},"source":["### The Logger class"]},{"cell_type":"markdown","metadata":{"id":"_1gszmpwhitv"},"source":["Explanations for the logger were already given in [this notebook](https://colab.research.google.com/drive/1raeuB6uUVUpl-4PLArtiAoGnXj0sGjSV?usp=sharing)."]},{"cell_type":"code","execution_count":42,"metadata":{"id":"aOkauz_0H2GA","executionInfo":{"status":"ok","timestamp":1665328319881,"user_tz":-120,"elapsed":11,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["class Logger():\n","\n","  def __init__(self, cfg):\n","    self.logger = instantiate_class(cfg.logger)\n","\n","  def add_log(self, log_string, loss, epoch):\n","    self.logger.add_scalar(log_string, loss.item(), epoch)\n","\n","  # Log losses\n","  def log_losses(self, cfg, epoch, critic_loss, entropy_loss, a2c_loss):\n","    self.add_log(\"critic_loss\", critic_loss, epoch)\n","    self.add_log(\"entropy_loss\", entropy_loss, epoch)\n","    self.add_log(\"a2c_loss\", a2c_loss, epoch)\n"]},{"cell_type":"markdown","metadata":{"id":"f2vq1OJHWCIE"},"source":["### Setup the optimizers"]},{"cell_type":"markdown","metadata":{"id":"VzmEKF4J8qjg"},"source":["We use two separate optimizers to tune the parameters of the actor and the critic separately. That makes it possible to use a different learning rate for the actor and the critic."]},{"cell_type":"code","execution_count":43,"metadata":{"id":"YFfzXEu2WFWj","executionInfo":{"status":"ok","timestamp":1665328319881,"user_tz":-120,"elapsed":11,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["# Configure the optimizers\n","def setup_optimizers(cfg, actor, critic):\n","    actor_optimizer_args = get_arguments(cfg.actor_optimizer)\n","    parameters = actor.parameters()\n","    actor_optimizer = get_class(cfg.actor_optimizer)(parameters, **actor_optimizer_args)\n","    critic_optimizer_args = get_arguments(cfg.critic_optimizer)\n","    parameters = critic.parameters()\n","    critic_optimizer = get_class(cfg.critic_optimizer)(\n","        parameters, **critic_optimizer_args\n","    )\n","    return actor_optimizer, critic_optimizer"]},{"cell_type":"markdown","metadata":{"id":"YQNvhO_VAJbh"},"source":["### Compute critic loss"]},{"cell_type":"markdown","metadata":{"id":"fxxobbxRaJXO"},"source":["Detailed explanations of the function to compute the critic loss when using a NoAutoResetGymAgent are given in [this notebook](https://colab.research.google.com/drive/1raeuB6uUVUpl-4PLArtiAoGnXj0sGjSV?usp=sharing)."]},{"cell_type":"markdown","metadata":{"id":"fXwrjbueoDw6"},"source":["The case where we use the AutoResetGymAgent is very similar, but we need to specify that we use the first part of the Q-values (`q_values[0]`) for representing $Q(s_t,a_t)$ and the second part (`q_values[1]`) for representing $Q(s_{t+1},a)$, as these values are stored into a transition model. Then the values used to compute the target in the critic update are stored into ```target_q_values```."]},{"cell_type":"code","execution_count":44,"metadata":{"id":"2sepUK-gAM3u","executionInfo":{"status":"ok","timestamp":1665328319882,"user_tz":-120,"elapsed":12,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["def compute_critic_loss(cfg, reward, must_bootstrap, q_values, target_q_values):\n","    # Compute temporal difference\n","    q_next = target_q_values\n","    target = (\n","        reward[:-1][0]\n","        + cfg.algorithm.discount_factor * q_next.squeeze(-1) * must_bootstrap.int()\n","    )\n","    td = target - q_values.squeeze(-1)\n","    # Compute critic loss\n","    td_error = td**2\n","    critic_loss = td_error.mean()\n","    return critic_loss"]},{"cell_type":"markdown","metadata":{"id":"ljCg7i26XLgb"},"source":["To update the target critic, one uses the following equation:"]},{"cell_type":"markdown","metadata":{"id":"AO0AShomKBz8"},"source":["$\\theta' \\leftarrow \\tau \\theta + (1- \\tau) \\theta'$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EFY9VV1UXSsT"},"source":["where $\\theta$ is the vector of parameters of the critic, and $\\theta'$ is the vector of parameters of the target critic."]},{"cell_type":"markdown","metadata":{"id":"ty_UYntZXf9Q"},"source":["The `soft_update_params(...)` function is in charge of performing this soft update."]},{"cell_type":"code","execution_count":45,"metadata":{"id":"cfq7tKEI8UWQ","executionInfo":{"status":"ok","timestamp":1665328319882,"user_tz":-120,"elapsed":11,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["def soft_update_params(net, target_net, tau):\n","    for param, target_param in zip(net.parameters(), target_net.parameters()):\n","        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"]},{"cell_type":"markdown","metadata":{"id":"ixiF82r3x1ZT"},"source":["### Compute actor loss"]},{"cell_type":"markdown","metadata":{"id":"CUlRVqJKx5g9"},"source":["The actor loss is straightforward. We want the actor to maximize Q-values, thus we minimize the mean of negated Q-values."]},{"cell_type":"code","execution_count":46,"metadata":{"id":"DTXjXTRi5Ipp","executionInfo":{"status":"ok","timestamp":1665328319882,"user_tz":-120,"elapsed":11,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["def compute_actor_loss(q_values):\n","    return -q_values.mean()"]},{"cell_type":"markdown","metadata":{"id":"Jmi91gANWT4z"},"source":["## Main training loop"]},{"cell_type":"markdown","metadata":{"id":"I6SuPOdW_hxl"},"source":["### Agent execution"]},{"cell_type":"markdown","metadata":{"id":"WqlH-8DaVWx2"},"source":["This is the tricky part with BBRL, the one we need to understand in detail. The difficulty lies in the copy of the last step and the way to deal with the n_steps return."]},{"cell_type":"markdown","metadata":{"id":"bWAmm0pPotTC"},"source":["The call to `train_agent(workspace, t=1, n_steps=cfg.algorithm.n_timesteps - 1, stochastic=True)` makes the agent run a number of steps in the workspace. In practice, it calls the [`__call__(...)`](https://github.com/osigaud/bbrl/blob/master/bbrl/agents/agent.py#L54) function which makes a forward pass of the agent network using the workspace data and updates the workspace accordingly."]},{"cell_type":"markdown","metadata":{"id":"Rn3MlNQ3qGPr"},"source":["Now, if we start at the first epoch (`epoch=0`), we start from the first step (`t=0`). But when subsequently we perform the next epochs (`epoch>0`), we must not forget to cover the transition at the border between the previous epoch and the current epoch. To avoid this risk, we copy the information from the last time step of the previous epoch into the first time step of the next epoch. This is explained in more details in [a previous notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5)."]},{"cell_type":"markdown","metadata":{"id":"gAnnEjF9L9gk"},"source":["A [previous notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5) explains a lot of these details. In particular, read it to understand the `execute_agents(...)` function, the `transition_workspace = train_workspace.get_transitions()` line and the computation of `must_bootstrap`."]},{"cell_type":"markdown","metadata":{"id":"OFB1XFE5YEc6"},"source":["Note that we `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()` lines. \n","\n","`optimizer.zero_grad()` is necessary to cancel all the gradients computed at the previous iterations\n"]},{"cell_type":"markdown","metadata":{"id":"SqFEgJIrhdxE"},"source":["Note the way we count the steps, to properly ignore the steps corresponding to a transition from an episode to the next."]},{"cell_type":"markdown","metadata":{"id":"O_cswkRmhnTX"},"source":["Note also that every ```cfg.algorithm.eval_interval```, we evaluate the current agent and save it and plot it if its performance is the new best performance."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"-Eu4Pjvp8HWp","executionInfo":{"status":"ok","timestamp":1665328319882,"user_tz":-120,"elapsed":11,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["from bbrl.visu.visu_policies import plot_policy\n","from bbrl.visu.visu_critics import plot_critic"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"sk85_sRWW-5s","executionInfo":{"status":"ok","timestamp":1665328319883,"user_tz":-120,"elapsed":12,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["def run_ddpg(cfg):\n","    # 1)  Build the  logger\n","    logger = Logger(cfg)\n","    best_reward = -10e9\n","\n","    # 2) Create the environment agent\n","    train_env_agent = AutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.n_envs,\n","        cfg.algorithm.seed,\n","    )\n","    eval_env_agent = NoAutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.nb_evals,\n","        cfg.algorithm.seed,\n","    )\n","\n","    # 3) Create the DDPG Agent\n","    (\n","        train_agent,\n","        eval_agent,\n","        actor,\n","        critic,\n","        # target_actor,\n","        target_critic,\n","    ) = create_ddpg_agent(cfg, train_env_agent, eval_env_agent)\n","    ag_actor = TemporalAgent(actor)\n","    # ag_target_actor = TemporalAgent(target_actor)\n","    q_agent = TemporalAgent(critic)\n","    target_q_agent = TemporalAgent(target_critic)\n","\n","    train_workspace = Workspace()\n","    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n","\n","    # Configure the optimizer\n","    actor_optimizer, critic_optimizer = setup_optimizers(cfg, actor, critic)\n","    nb_steps = 0\n","    tmp_steps = 0\n","\n","    # Training loop\n","    for epoch in range(cfg.algorithm.max_epochs):\n","        # Execute the agent in the workspace\n","        if epoch > 0:\n","            train_workspace.zero_grad()\n","            train_workspace.copy_n_last_steps(1)\n","            train_agent(train_workspace, t=1, n_steps=cfg.algorithm.n_steps - 1)\n","        else:\n","            train_agent(train_workspace, t=0, n_steps=cfg.algorithm.n_steps)\n","\n","        transition_workspace = train_workspace.get_transitions()\n","        action = transition_workspace[\"action\"]\n","        nb_steps += action[0].shape[0]\n","        rb.put(transition_workspace)\n","        rb_workspace = rb.get_shuffled(cfg.algorithm.batch_size)\n","\n","        done, truncated, reward, action = rb_workspace[\n","            \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n","        ]\n","        if nb_steps > cfg.algorithm.learning_starts:\n","            # Determines whether values of the critic should be propagated\n","            # True if the episode reached a time limit or if the task was not done\n","            # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing\n","            must_bootstrap = torch.logical_or(~done[1], truncated[1])\n","\n","            # Critic update\n","            # compute q_values: at t, we have Q(s,a) from the (s,a) in the RB\n","            q_agent(rb_workspace, t=0, n_steps=1)\n","            q_values = rb_workspace[\"q_value\"]\n","\n","            with torch.no_grad():\n","                # replace the action at t+1 in the RB with \\pi(s_{t+1}), to compute Q(s_{t+1}, \\pi(s_{t+1}) below\n","                ag_actor(rb_workspace, t=1, n_steps=1)\n","                # compute q_values: at t+1 we have Q(s_{t+1}, \\pi(s_{t+1})\n","                target_q_agent(rb_workspace, t=1, n_steps=1)\n","                # q_agent(rb_workspace, t=1, n_steps=1)\n","            # finally q_values contains the above collection at t=0 and t=1\n","            post_q_values = rb_workspace[\"q_value\"]\n","\n","            # Compute critic loss\n","            critic_loss = compute_critic_loss(\n","                cfg, reward, must_bootstrap, q_values[0], post_q_values[1]\n","            )\n","            logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n","            critic_optimizer.zero_grad()\n","            critic_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(\n","                critic.parameters(), cfg.algorithm.max_grad_norm\n","            )\n","            critic_optimizer.step()\n","\n","            # Actor update\n","            # Now we determine the actions the current policy would take in the states from the RB\n","            ag_actor(rb_workspace, t=0, n_steps=1)\n","            # We determine the Q values resulting from actions of the current policy\n","            q_agent(rb_workspace, t=0, n_steps=1)\n","            # and we back-propagate the corresponding loss to maximize the Q values\n","            q_values = rb_workspace[\"q_value\"]\n","            actor_loss = compute_actor_loss(q_values)\n","            logger.add_log(\"actor_loss\", actor_loss, nb_steps)\n","            actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(\n","                actor.parameters(), cfg.algorithm.max_grad_norm\n","            )\n","            actor_optimizer.step()\n","            # Soft update of target q function\n","            tau = cfg.algorithm.tau_target\n","            soft_update_params(critic, target_critic, tau)\n","            # soft_update_params(actor, target_actor, tau)\n","\n","        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n","            tmp_steps = nb_steps\n","            eval_workspace = Workspace()  # Used for evaluation\n","            eval_agent(eval_workspace, t=0, stop_variable=\"env/done\")\n","            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n","            mean = rewards.mean()\n","            logger.add_log(\"reward\", mean, nb_steps)\n","            print(f\"nb_steps: {nb_steps}, reward: {mean}\")\n","            if cfg.save_best and mean > best_reward:\n","                best_reward = mean\n","                directory = \"./ddpg_agent/\"\n","                if not os.path.exists(directory):\n","                    os.makedirs(directory)\n","                filename = directory + \"ddpg_\" + str(mean.item()) + \".agt\"\n","                eval_agent.save_model(filename)\n","                if cfg.plot_agents:\n","                    plot_policy(\n","                        actor,\n","                        eval_env_agent,\n","                        \"./ddpg_plots/\",\n","                        cfg.gym_env.env_name,\n","                        best_reward,\n","                        stochastic=False,\n","                    )\n","                    plot_critic(\n","                        q_agent.agent,\n","                        eval_env_agent,\n","                        \"./ddpg_plots/\",\n","                        cfg.gym_env.env_name,\n","                        best_reward,\n","                    )\n"]},{"cell_type":"markdown","metadata":{"id":"uo6bc3zzKua_"},"source":["## Definition of the parameters"]},{"cell_type":"markdown","metadata":{"id":"36r4PAfvKx-f"},"source":["The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation."]},{"cell_type":"code","execution_count":49,"metadata":{"id":"JB2B8zELNWQd","executionInfo":{"status":"ok","timestamp":1665328319883,"user_tz":-120,"elapsed":11,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["params={\n","  \"save_best\": True,\n","  \"plot_agents\": True,\n","  \"logger\":{\n","    \"classname\": \"bbrl.utils.logger.TFLogger\",\n","    \"log_dir\": \"./tmp/\" + str(time.time()),\n","    \"cache_size\": 10000,\n","    \"every_n_seconds\": 10,\n","    \"verbose\": False,    \n","    },\n","\n","  \"algorithm\":{\n","    \"seed\": 1,\n","    \"max_grad_norm\": 0.5,\n","    \"epsilon\": 0.02,\n","    \"n_envs\": 1,\n","    \"n_steps\": 100,\n","    \"eval_interval\": 2000,\n","    \"nb_evals\": 10,\n","    \"gae\": 0.8,\n","    \"max_epochs\": 21000,\n","    \"discount_factor\": 0.98,\n","    \"buffer_size\": 2e5,\n","    \"batch_size\": 64,\n","    \"tau_target\": 0.05,\n","    \"learning_starts\": 10000,\n","    \"action_noise\": 0.1,\n","    \"architecture\":{\n","        \"actor_hidden_size\": [400, 300],\n","        \"critic_hidden_size\": [400, 300],\n","        },\n","  },\n","  \"gym_env\":{\n","    \"classname\": \"__main__.make_gym_env\",\n","    \"env_name\": \"Pendulum-v1\",\n","  },\n","  \"actor_optimizer\":{\n","    \"classname\": \"torch.optim.Adam\",\n","    \"lr\": 1e-3,\n","  },\n","  \"critic_optimizer\":{\n","    \"classname\": \"torch.optim.Adam\",\n","    \"lr\": 1e-3,\n","  }\n","}"]},{"cell_type":"markdown","metadata":{"id":"jp7jDeGkaoM1"},"source":["### Launching tensorboard to visualize the results"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"elapsed":202,"status":"error","timestamp":1665328320074,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"},"user_tz":-120},"id":"-BlLQlwDn5Vh","outputId":"9dc4eb27-3cb3-4d15-d0ed-5ce0754bd77a"},"outputs":[{"output_type":"stream","name":"stdout","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]},{"output_type":"display_data","data":{"text/plain":["Launching TensorBoard..."]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-dad69c319c1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tensorboard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorboard'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--logdir ./tmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorboard/notebook.py\u001b[0m in \u001b[0;36m_start_magic\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_start_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;34m\"\"\"Implementation of the `%tensorboard` line magic.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorboard/notebook.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(args_string)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mparsed_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mstart_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStartLaunched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorboard/manager.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(arguments, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0mend_time_seconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_time_seconds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mend_time_seconds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_interval_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0msubprocess_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msubprocess_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["%load_ext tensorboard\n","%tensorboard --logdir ./tmp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l42OUoGROlSt","executionInfo":{"status":"aborted","timestamp":1665328320075,"user_tz":-120,"elapsed":2,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[],"source":["config=OmegaConf.create(params)\n","torch.manual_seed(config.algorithm.seed)\n","run_ddpg(config)"]},{"cell_type":"markdown","metadata":{"id":"paHdoNlz9Lpg"},"source":["## Exercise 1: code TD3"]},{"cell_type":"markdown","metadata":{"id":"F2OIv4em9Lpj"},"source":["Starting from the above version , you sould code [the TD3 algorithm](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf).\n","\n","For that, you need to use two critics (and two target critics) and always take the minimum output between the two when you ask for the Q-value of a (state, action) pair.\n","\n","In more detail, you have to do the following:\n","- replace the single critic and corresponding target critic with two critics and target critics (name them ```critic_1, critic_2, target_critic_1, target_critic_2```)\n","- get the q-values and target q-values corresponding to all these critics.\n","- then the target q-values you should consider to update the critic should be the min over the target q-values at each step (use ```torch.min(...)``` to get this min over a sequence of data).\n","- to update the actor, do it with the q-values of an arbitrarily chosen critic, e.g. critic_1.\n","\n"]},{"cell_type":"code","source":["def run_td3(cfg):\n","    # 1)  Build the  logger\n","    logger = Logger(cfg)\n","    best_reward = -10e9\n","\n","    # 2) Create the environment agent\n","    train_env_agent = AutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.n_envs,\n","        cfg.algorithm.seed,\n","    )\n","    eval_env_agent = NoAutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.nb_evals,\n","        cfg.algorithm.seed,\n","    )\n","\n","    # 3) Create the TD3 Agent\n","    (\n","        train_agent,\n","        eval_agent,\n","        actor,\n","        critic1,\n","        # target_actor,\n","        target_critic1,\n","    ) = create_ddpg_agent(cfg, train_env_agent, eval_env_agent)\n","    ag_actor = TemporalAgent(actor)\n","    # ag_target_actor = TemporalAgent(target_actor)\n","\n","    # Manually create second critic agent\n","    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n","    critic2 = ContinuousQAgent(\n","        obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n","    )\n","    target_critic2 = copy.deepcopy(critic2)\n","\n","    q_agent1, q_agent2 = TemporalAgent(critic1), TemporalAgent(critic2)\n","    target_q_agent1, target_q_agent2 = TemporalAgent(target_critic1), TemporalAgent(target_critic2)\n","\n","    train_workspace = Workspace()\n","    rb = ReplayBuffer(max_size=cfg.algorithm.buffer_size)\n","\n","    # Configure the optimizer\n","    actor_optimizer1, critic_optimizer1 = setup_optimizers(cfg, actor, critic1)\n","    actor_optimizer2, critic_optimizer2 = setup_optimizers(cfg, actor, critic2)\n","    nb_steps = 0\n","    tmp_steps = 0\n","\n","    # Training loop\n","    for epoch in range(cfg.algorithm.max_epochs):\n","        # Execute the agent in the workspace\n","        if epoch > 0:\n","            train_workspace.zero_grad()\n","            train_workspace.copy_n_last_steps(1)\n","            train_agent(train_workspace, t=1, n_steps=cfg.algorithm.n_steps - 1)\n","        else:\n","            train_agent(train_workspace, t=0, n_steps=cfg.algorithm.n_steps)\n","\n","        transition_workspace = train_workspace.get_transitions()\n","        action = transition_workspace[\"action\"]\n","        nb_steps += action[0].shape[0]\n","        rb.put(transition_workspace)\n","        \n","        rb_workspace1 = rb.get_shuffled(cfg.algorithm.batch_size)\n","        rb_workspace2 = copy.deepcopy(rb_workspace1)\n","\n","        done1, truncated1, reward1, action1 = rb_workspace1[\n","            \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n","        ]\n","        done2, truncated2, reward2, action2 = rb_workspace2[\n","            \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n","        ]\n","        if nb_steps > cfg.algorithm.learning_starts:\n","            # Determines whether values of the critic should be propagated\n","            # True if the episode reached a time limit or if the task was not done\n","            # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing\n","            must_bootstrap1 = torch.logical_or(~done1[1], truncated1[1])\n","            must_bootstrap2 = torch.logical_or(~done2[1], truncated2[1])\n","\n","            # Critic update\n","            # compute q_values: at t, we have Q(s,a) from the (s,a) in the RB\n","            q_agent1(rb_workspace1, t=0, n_steps=1)\n","            q_values1 = rb_workspace1[\"q_value\"]\n","            q_agent2(rb_workspace2, t=0, n_steps=1)\n","            q_values2 = rb_workspace2[\"q_value\"]\n","\n","            with torch.no_grad():\n","                # replace the action at t+1 in the RB with \\pi(s_{t+1}), to compute Q(s_{t+1}, \\pi(s_{t+1}) below\n","                ag_actor(rb_workspace1, t=1, n_steps=1)\n","                ag_actor(rb_workspace2, t=1, n_steps=1)\n","                # compute q_values: at t+1 we have Q(s_{t+1}, \\pi(s_{t+1})\n","                target_q_agent1(rb_workspace1, t=1, n_steps=1)\n","                target_q_agent2(rb_workspace2, t=1, n_steps=1)\n","                # q_agent(rb_workspace, t=1, n_steps=1)\n","\n","                # finally q_values contains the above collection at t=0 and t=1\n","                post_q_values1 = rb_workspace1[\"q_value\"]\n","                post_q_values2 = rb_workspace2[\"q_value\"]\n","                target_q_values = torch.min(post_q_values1[1], post_q_values2[1])\n","\n","            # Compute critic loss\n","            critic_loss1 = compute_critic_loss(\n","                cfg, reward1, must_bootstrap1, q_values1[0], target_q_values\n","            )\n","            critic_loss2 = compute_critic_loss(\n","                cfg, reward2, must_bootstrap2, q_values2[0], target_q_values\n","            )\n","            logger.add_log(\"critic_loss1\", critic_loss1, nb_steps)\n","            logger.add_log(\"critic_loss2\", critic_loss2, nb_steps)\n","            critic_optimizer1.zero_grad()\n","            critic_optimizer2.zero_grad()\n","            critic_loss1.backward()\n","            critic_loss2.backward()\n","            torch.nn.utils.clip_grad_norm_(\n","                critic1.parameters(), cfg.algorithm.max_grad_norm\n","            )\n","            torch.nn.utils.clip_grad_norm_(\n","                critic2.parameters(), cfg.algorithm.max_grad_norm\n","            )\n","            critic_optimizer1.step()\n","            critic_optimizer2.step()\n","\n","            # Actor update (we can use critic 1 for actor update)\n","            # Now we determine the actions the current policy would take in the states from the RB\n","            ag_actor(rb_workspace1, t=0, n_steps=1)\n","            # We determine the Q values resulting from actions of the current policy\n","            q_agent1(rb_workspace1, t=0, n_steps=1)\n","            # and we back-propagate the corresponding loss to maximize the Q values\n","            q_values = rb_workspace1[\"q_value\"]\n","            actor_loss = compute_actor_loss(q_values)\n","            logger.add_log(\"actor_loss\", actor_loss, nb_steps)\n","            actor_optimizer1.zero_grad()\n","            actor_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(\n","                actor.parameters(), cfg.algorithm.max_grad_norm\n","            )\n","            actor_optimizer1.step()\n","            # Soft update of target q function\n","            tau = cfg.algorithm.tau_target\n","            soft_update_params(critic1, target_critic1, tau)\n","            # soft_update_params(actor, target_actor, tau)\n","\n","        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n","            tmp_steps = nb_steps\n","            eval_workspace = Workspace()  # Used for evaluation\n","            eval_agent(eval_workspace, t=0, stop_variable=\"env/done\")\n","            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n","            mean = rewards.mean()\n","            logger.add_log(\"reward\", mean, nb_steps)\n","            print(f\"nb_steps: {nb_steps}, reward: {mean}\")\n","            if cfg.save_best and mean > best_reward:\n","                best_reward = mean\n","                directory = \"./ddpg_agent/\"\n","                if not os.path.exists(directory):\n","                    os.makedirs(directory)\n","                filename = directory + \"ddpg_\" + str(mean.item()) + \".agt\"\n","                eval_agent.save_model(filename)\n","                # if cfg.plot_agents:\n","                #     plot_policy(\n","                #         actor,\n","                #         eval_env_agent,\n","                #         \"./ddpg_plots/\",\n","                #         cfg.gym_env.env_name,\n","                #         best_reward,\n","                #         stochastic=False,\n","                #     )\n","                #     plot_critic(\n","                #         q_agent1.agent,\n","                #         eval_env_agent,\n","                #         \"./ddpg_plots/\",\n","                #         cfg.gym_env.env_name,\n","                #         best_reward,\n","                #     )\n","                #     plot_critic(\n","                #         q_agent2.agent,\n","                #         eval_env_agent,\n","                #         \"./ddpg_plots/\",\n","                #         cfg.gym_env.env_name,\n","                #         best_reward,\n","                #     )\n"],"metadata":{"id":"ZHszlZRmLJo0","executionInfo":{"status":"ok","timestamp":1665329389062,"user_tz":-120,"elapsed":312,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["params={\n","  \"save_best\": True,\n","  \"plot_agents\": True,\n","  \"logger\":{\n","    \"classname\": \"bbrl.utils.logger.TFLogger\",\n","    \"log_dir\": \"./tmp/\" + str(time.time()),\n","    \"cache_size\": 10000,\n","    \"every_n_seconds\": 10,\n","    \"verbose\": False,    \n","    },\n","\n","  \"algorithm\":{\n","    \"seed\": 1,\n","    \"max_grad_norm\": 0.5,\n","    \"epsilon\": 0.02,\n","    \"n_envs\": 1,\n","    \"n_steps\": 100,\n","    \"eval_interval\": 2000,\n","    \"nb_evals\": 10,\n","    \"gae\": 0.8,\n","    \"max_epochs\": 21000,\n","    \"discount_factor\": 0.98,\n","    \"buffer_size\": 2e5,\n","    \"batch_size\": 64,\n","    \"tau_target\": 0.05,\n","    \"learning_starts\": 10000,\n","    \"action_noise\": 0.1,\n","    \"architecture\":{\n","        \"actor_hidden_size\": [400, 300],\n","        \"critic_hidden_size\": [400, 300],\n","        },\n","  },\n","  \"gym_env\":{\n","    \"classname\": \"__main__.make_gym_env\",\n","    \"env_name\": \"LunarLanderContinuous-v2\",\n","  },\n","  \"actor_optimizer\":{\n","    \"classname\": \"torch.optim.Adam\",\n","    \"lr\": 1e-3,\n","  },\n","  \"critic_optimizer\":{\n","    \"classname\": \"torch.optim.Adam\",\n","    \"lr\": 1e-3,\n","  }\n","}\n","\n","config=OmegaConf.create(params)\n","torch.manual_seed(config.algorithm.seed)\n","run_td3(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":451},"id":"oeqtAX5OO2FF","executionInfo":{"status":"error","timestamp":1665329417991,"user_tz":-120,"elapsed":27087,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}},"outputId":"893cfe4f-0cd0-4c63-9cb2-2c7857f1f2d5"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["nb_steps: 2063, reward: -254.19784545898438\n","nb_steps: 4127, reward: -158.485107421875\n","nb_steps: 6194, reward: -193.5455780029297\n","nb_steps: 8254, reward: -192.84030151367188\n","nb_steps: 10315, reward: -2120.326416015625\n","nb_steps: 12375, reward: -894.7658081054688\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-69-1ce1859493e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mrun_td3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-68-53b5f64608a4>\u001b[0m in \u001b[0;36mrun_td3\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtrain_workspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mtrain_workspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_n_last_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_workspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_workspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bbrl/agents/utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0m_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_variable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bbrl/agents/utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bbrl/agents/agent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mworkspace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[Agent.__call__] workspace must not be None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-36-656d561a9744>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"env/env_obs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"action\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"jkp3YCxcLlfh"},"source":["## Exercise 2: experimental comparison"]},{"cell_type":"markdown","metadata":{"id":"dD9eCH3w-hdc"},"source":["Take an environment where the over-estimation bias may matter, and compare the performance of DDPG and TD3. Visualize the Q-value long before convergence to see whether indeed DDPG overestimates the Q-values with respect to TD3."]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1APBtDiaFwQHKE2rfTZioGfDM8C41e7Il","timestamp":1665323484519},{"file_id":"1H9_gkenmb_APnbygme1oEdhqMLSDc_bM","timestamp":1659366486909},{"file_id":"1yAQlrShysj4Q9EBpYM8pBsp2aXInhP7x","timestamp":1653892366111},{"file_id":"1XUJSplQm_MttDKtzsJ1AKhhJQT_W0oWi","timestamp":1650557706114},{"file_id":"1J74foctf26QfZ4DuKxGfAwrO2wZmedNi","timestamp":1643612886194},{"file_id":"1-aidxjij0JwVyOgYSLqR-v4KMow4BsbQ","timestamp":1641470409971},{"file_id":"1tZ744yXYoDhwk0xk73baYa7Ks4MRpba8","timestamp":1641465913520},{"file_id":"1SEFpe1yUMjUsKzYkqWF_xQzVzZOQutHZ","timestamp":1641289551712}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}