{"cells":[{"cell_type":"markdown","metadata":{"id":"dYfGJCe52lP4"},"source":["# Outlook"]},{"cell_type":"markdown","metadata":{"id":"aZUSf0n_2otG"},"source":["In this notebook, we will implement a simple version of the A2C algorithm using BBRL. To understand this code, you need [to know more about BBRL](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing). You should first have a look at [the BBRL interaction model](https://colab.research.google.com/drive/1gSdkOBPkIQi_my9TtwJ-qWZQS0b2X7jt?usp=sharing), then [a first example](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) and, most importantly, details about the [NoAutoResetGymAgent](https://colab.research.google.com/drive/1EX5O03mmWFp9wCL_Gb_-p08JktfiL2l5?usp=sharing)."]},{"cell_type":"markdown","metadata":{"id":"MD3A9UmNS2OX"},"source":["The A2C algorithm is explained in [this video](https://www.youtube.com/watch?v=BUmsTlIgrBI) and you can also read [the corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/a2c.pdf)."]},{"cell_type":"markdown","metadata":{"id":"zJZDcDafp7Uf"},"source":["## Installation and Imports"]},{"cell_type":"markdown","metadata":{"id":"aHO1nIdM21Lq"},"source":["### Installation"]},{"cell_type":"code","source":["!pip install importlib-metadata==4.13.0"],"metadata":{"id":"yq6_0IftWquH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666605510732,"user_tz":-120,"elapsed":5785,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}},"outputId":"3d96ced7-b204-4c7d-97b5-aef10fc013a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: importlib-metadata==4.13.0 in /usr/local/lib/python3.7/dist-packages (4.13.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata==4.13.0) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata==4.13.0) (3.9.0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ymc-lbXi9vDE"},"source":["The BBRL library is [here](https://github.com/osigaud/bbrl)."]},{"cell_type":"markdown","metadata":{"id":"pDy9yuQH73tJ"},"source":["This is OmegaConf that makes it possible that by just defining the `def run_a2c(cfg):` function and then executing a long `params = {...}` variable at the bottom of this colab, the code is run with the parameters without calling an explicit main.\n","\n","More precisely, the code is run by calling\n","\n","`config=OmegaConf.create(params)`\n","\n","`run_a2c(config)`\n","\n","at the very bottom of the colab, after starting tensorboard."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j0MaggiOl4KU","outputId":"0140073a-c6b4-415c-ab1b-80ef01f34594","executionInfo":{"status":"ok","timestamp":1666605532083,"user_tz":-120,"elapsed":21379,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: omegaconf in /usr/local/lib/python3.7/dist-packages (2.2.3)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.7/dist-packages (from omegaconf) (4.9.3)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf) (6.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/osigaud/my_gym.git\n","  Cloning https://github.com/osigaud/my_gym.git to /tmp/pip-req-build-dsqnu5gp\n","  Running command git clone -q https://github.com/osigaud/my_gym.git /tmp/pip-req-build-dsqnu5gp\n","Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.7/dist-packages (from bbrl-gym==0.1.2) (1.21.6)\n","Requirement already satisfied: gym==0.21.0 in /usr/local/lib/python3.7/dist-packages (from bbrl-gym==0.1.2) (0.21.0)\n","Requirement already satisfied: Box2D in /usr/local/lib/python3.7/dist-packages (from bbrl-gym==0.1.2) (2.3.10)\n","Requirement already satisfied: mazemdp>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from bbrl-gym==0.1.2) (0.1.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.21.0->bbrl-gym==0.1.2) (1.5.0)\n","Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym==0.21.0->bbrl-gym==0.1.2) (4.13.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym==0.21.0->bbrl-gym==0.1.2) (3.9.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym==0.21.0->bbrl-gym==0.1.2) (4.1.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mazemdp>=0.1.1->bbrl-gym==0.1.2) (3.2.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mazemdp>=0.1.1->bbrl-gym==0.1.2) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mazemdp>=0.1.1->bbrl-gym==0.1.2) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mazemdp>=0.1.1->bbrl-gym==0.1.2) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mazemdp>=0.1.1->bbrl-gym==0.1.2) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mazemdp>=0.1.1->bbrl-gym==0.1.2) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/osigaud/bbrl.git\n","  Cloning https://github.com/osigaud/bbrl.git to /tmp/pip-req-build-vpo23qpc\n","  Running command git clone -q https://github.com/osigaud/bbrl.git /tmp/pip-req-build-vpo23qpc\n","Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bbrl==0.1.5) (1.12.1+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from bbrl==0.1.5) (0.13.1+cu113)\n","Requirement already satisfied: gym>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from bbrl==0.1.5) (0.21.0)\n","Requirement already satisfied: bbrl_gym>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from bbrl==0.1.5) (0.1.2)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from bbrl==0.1.5) (2.9.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bbrl==0.1.5) (4.64.1)\n","Requirement already satisfied: hydra-core in /usr/local/lib/python3.7/dist-packages (from bbrl==0.1.5) (1.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bbrl==0.1.5) (1.21.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from bbrl==0.1.5) (1.3.5)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from bbrl==0.1.5) (4.6.0.66)\n","Requirement already satisfied: omegaconf in /usr/local/lib/python3.7/dist-packages (from bbrl==0.1.5) (2.2.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bbrl==0.1.5) (3.2.2)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from bbrl==0.1.5) (0.11.2)\n","Requirement already satisfied: mazemdp>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from bbrl_gym>=0.1.2->bbrl==0.1.5) (0.1.1)\n","Requirement already satisfied: Box2D in /usr/local/lib/python3.7/dist-packages (from bbrl_gym>=0.1.2->bbrl==0.1.5) (2.3.10)\n","Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21.0->bbrl==0.1.5) (4.13.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.21.0->bbrl==0.1.5) (1.5.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym>=0.21.0->bbrl==0.1.5) (3.9.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym>=0.21.0->bbrl==0.1.5) (4.1.1)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.7/dist-packages (from hydra-core->bbrl==0.1.5) (4.9.3)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core->bbrl==0.1.5) (5.10.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from hydra-core->bbrl==0.1.5) (21.3)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf->bbrl==0.1.5) (6.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bbrl==0.1.5) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bbrl==0.1.5) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bbrl==0.1.5) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bbrl==0.1.5) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->bbrl==0.1.5) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->bbrl==0.1.5) (2022.4)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->bbrl==0.1.5) (1.7.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.1.5) (3.4.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.1.5) (0.6.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.1.5) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.1.5) (1.8.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.1.5) (57.4.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.1.5) (1.35.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.1.5) (2.23.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.1.5) (1.49.1)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.1.5) (3.17.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.1.5) (0.37.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.1.5) (1.3.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.1.5) (0.4.6)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->bbrl==0.1.5) (4.9)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->bbrl==0.1.5) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->bbrl==0.1.5) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->bbrl==0.1.5) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->bbrl==0.1.5) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->bbrl==0.1.5) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->bbrl==0.1.5) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->bbrl==0.1.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->bbrl==0.1.5) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->bbrl==0.1.5) (3.2.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->bbrl==0.1.5) (7.1.2)\n"]}],"source":["import functools\n","import time\n","!pip install omegaconf\n","from omegaconf import OmegaConf\n","\n","import gym\n","!pip install git+https://github.com/osigaud/my_gym.git\n","!pip install git+https://github.com/osigaud/bbrl.git\n","\n","import bbrl"]},{"cell_type":"markdown","metadata":{"id":"m4kV9pWV3wRe"},"source":["### Imports"]},{"cell_type":"markdown","metadata":{"id":"caqhJYbe5YcO"},"source":["Below, we import standard python packages, pytorch packages and gym environments."]},{"cell_type":"markdown","metadata":{"id":"4l7sTVXbJBE_"},"source":["[OpenAI gym](https://gym.openai.com/) is a collection of benchmark environments to evaluate RL algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vktQB-AO5biu"},"outputs":[],"source":["import copy\n","import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import gym"]},{"cell_type":"markdown","metadata":{"id":"fE1c7ZLf60X_"},"source":["### BBRL imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcuqoAvG3zMZ"},"outputs":[],"source":["from bbrl.agents.agent import Agent\n","from bbrl import get_arguments, get_class, instantiate_class\n","\n","# The workspace is the main class in BBRL, this is where all data is collected and stored\n","from bbrl.workspace import Workspace\n","\n","# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n","# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n","# or until a given condition is reached\n","from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n","\n","# AutoResetGymAgent is an agent able to execute a batch of gym environments\n","# with auto-resetting. These agents produce multiple variables in the workspace: \n","# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/done’, ’env/initial_state’, ’env/cumulated_reward’, \n","# ... When called at timestep t=0, then the environments are automatically reset. \n","# At timestep t>0, these agents will read the ’action’ variable in the workspace at time t − 1\n","from bbrl.agents.gymb import AutoResetGymAgent"]},{"cell_type":"markdown","metadata":{"id":"JVvAfhKm9S8p"},"source":["## Definition of agents"]},{"cell_type":"markdown","metadata":{"id":"RdqzKSLKDtqz"},"source":["The [A2C](http://proceedings.mlr.press/v48/mniha16.pdf) algorithm is an actor-critic algorithm. Thus we need an Actor agent, a Critic agent and an Environment agent. \n","The actor agent is built on an intermediate ProbAgent, see [this notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) for explanations about the  ProbaAgent, the ActorAgent and the environment agent."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xe2thODO7E40"},"outputs":[],"source":["class ProbAgent(Agent):\n","    def __init__(self, observation_size, hidden_size, n_actions):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(observation_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, n_actions),\n","        )\n","\n","    def forward(self, t, **kwargs):\n","        observation = self.get((\"env/env_obs\", t))\n","        scores = self.model(observation)\n","        probs = torch.softmax(scores, dim=-1)\n","        self.set((\"action_probs\", t), probs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ARPW1Mmo7NB-"},"outputs":[],"source":["class ActorAgent(Agent):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, t, stochastic, **kwargs):\n","        probs = self.get((\"action_probs\", t))\n","        if stochastic:\n","            action = torch.distributions.Categorical(probs).sample()\n","        else:\n","            action = probs.argmax(1)\n","\n","        self.set((\"action\", t), action)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fsb5QRzw7V0o"},"outputs":[],"source":["def make_env(env_name):\n","    return gym.make(env_name)"]},{"cell_type":"markdown","metadata":{"id":"Din6iU-1DnyH"},"source":["### CriticAgent"]},{"cell_type":"markdown","metadata":{"id":"Nf0mXQvbEw7V"},"source":["A CriticAgent is a one hidden layer neural network which takes an observation as input and whose output is the value of this observation. It thus implements a $V(s)$ function. It would be straightforward to define another CriticAgent (call it a CriticQAgent by contrast to a CriticVAgent) that would take an observation and an action as input."]},{"cell_type":"markdown","metadata":{"id":"bQLc3dywFiqy"},"source":[" The `squeeze(-1)` removes the last dimension of the tensor. TODO: explain why we need it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8y-63nq7Pjo"},"outputs":[],"source":["class CriticAgent(Agent):\n","    def __init__(self, observation_size, hidden_size):\n","        super().__init__()\n","        layers_size = [observation_size] + list(hidden_size) + [1]\n","        model = [nn.Linear(layers_size[i], layers_size[i+1])]\n","        for i in range(len(layers_size) - 2):\n","\n","        self.critic_model = nn.Sequential(\n","            nn.Linear(observation_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, 1),\n","        )\n","\n","    def forward(self, t, **kwargs):\n","        observation = self.get((\"env/env_obs\", t))\n","        critic = self.critic_model(observation).squeeze(-1)\n","        self.set((\"critic\", t), critic)"]},{"cell_type":"markdown","metadata":{"id":"EzxoIPtLVJ_i"},"source":["### Create the A2C agent"]},{"cell_type":"markdown","metadata":{"id":"tJ0qhRVgVarb"},"source":["The code below is rather straightforward. Note that we have not defined anything about data collection, using a RolloutBuffer or something to store the n_step return so far. This will come inside the training loop below."]},{"cell_type":"markdown","metadata":{"id":"aaNnZw3bXEYd"},"source":["Interestingly, the loop between the policy and the environment is first defined as a collection of agents, and then embedded into a single TemporalAgent."]},{"cell_type":"markdown","metadata":{"id":"1kj9y1hjVrXi"},"source":["We delete the environment (not the environment agent) with `del env_agent.env` once we do not need it anymore just to avoid mistakes afterwards."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8Uk_RQh8QrO"},"outputs":[],"source":["# Create the A2C Agent\n","def create_a2c_agent(cfg, env_agent):\n","  observation_size,  n_actions = env_agent.get_obs_and_actions_sizes()\n","  prob_agent = ProbAgent(\n","      observation_size, cfg.algorithm.architecture.actor_hidden_size, n_actions\n","  )\n","  action_agent = ActorAgent()\n","  critic_agent = CriticAgent(\n","    observation_size, cfg.algorithm.architecture.critic_hidden_size)\n","\n","  # Combine env and policy agents\n","  agent = Agents(env_agent, prob_agent, action_agent)\n","  # Get an agent that is executed on a complete workspace\n","  agent = TemporalAgent(agent)\n","  agent.seed(cfg.algorithm.seed)\n","  return agent, prob_agent, critic_agent"]},{"cell_type":"markdown","metadata":{"id":"lU3cO6znHyDc"},"source":["### The Logger class"]},{"cell_type":"markdown","metadata":{"id":"E4BrXwTLdK0Z"},"source":["The logger class below is not generic, it is specifically designed in the context of this A2C colab."]},{"cell_type":"markdown","metadata":{"id":"VKYYp8IHLhd-"},"source":["The logger parameters are defined below in `params = { \"logger\":{ ...`"]},{"cell_type":"markdown","metadata":{"id":"rhwNN4oCNOhi"},"source":["In this colab, the logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation (see the parameters part below).\n","Note that the salina Logger is also saving the log in a readable format such that you can use `Logger.read_directories(...)` to read multiple logs, create a dataframe, and analyze many experiments afterward in a notebook for instance. "]},{"cell_type":"markdown","metadata":{"id":"10TUc-PHMqNm"},"source":["The code for the different kinds of loggers is available in the [bbrl/utils/logger.py](https://github.com/osigaud/bbrl/blob/master/bbrl/utils/logger.py) file."]},{"cell_type":"markdown","metadata":{"id":"c872tM4WM5FH"},"source":["Having logging provided under the hood is one of the features where using RL libraries like BBRL will allow you to save time."]},{"cell_type":"markdown","metadata":{"id":"lmsf5BENLz10"},"source":["`instantiate_class` is an inner BBRL mechanism. The `instantiate_class`function is available in the [`bbrl/__init__.py`](https://github.com/osigaud/bbrl/blob/master/bbrl/__init__.py) file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOkauz_0H2GA"},"outputs":[],"source":["class Logger():\n","\n","  def __init__(self, cfg):\n","    self.logger = instantiate_class(cfg.logger)\n","\n","  def add_log(self, log_string, loss, epoch):\n","    self.logger.add_scalar(log_string, loss.item(), epoch)\n","\n","  # Log losses\n","  def log_losses(self, cfg, epoch, critic_loss, entropy_loss, a2c_loss):\n","    self.add_log(\"critic_loss\", critic_loss, epoch)\n","    self.add_log(\"entropy_loss\", entropy_loss, epoch)\n","    self.add_log(\"a2c_loss\", a2c_loss, epoch)\n"]},{"cell_type":"markdown","metadata":{"id":"f2vq1OJHWCIE"},"source":["### Setup the optimizer"]},{"cell_type":"markdown","metadata":{"id":"VzmEKF4J8qjg"},"source":["We use a single optimizer to tune the parameters of the actor (in the prob_agent part) and the critic (in the critic_agent part). It would be possible to have two optimizers which would work separately on the parameters of each component agent, but it would be more complicated because updating the actor requires the gradient of the critic."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YFfzXEu2WFWj"},"outputs":[],"source":["# Configure the optimizer over the a2c agent\n","def setup_optimizer(cfg, prob_agent, critic_agent):\n","  optimizer_args = get_arguments(cfg.optimizer)\n","  parameters = nn.Sequential(prob_agent, critic_agent).parameters()\n","  optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n","  return optimizer"]},{"cell_type":"markdown","metadata":{"id":"I6SuPOdW_hxl"},"source":["### Execute agent"]},{"cell_type":"markdown","metadata":{"id":"WqlH-8DaVWx2"},"source":["This is the tricky part with BBRL, the one we need to understand in detail. The difficulty lies in the copy of the last step and the way to deal with the n_steps return."]},{"cell_type":"markdown","metadata":{"id":"bWAmm0pPotTC"},"source":["The call to `agent(workspace, t=1, n_steps=cfg.algorithm.n_timesteps - 1, stochastic=True)` makes the agent run a number of steps in the workspace. In practice, it calls the [`__call__(...)`](https://github.com/osigaud/bbrl/blob/master/bbrl/agents/agent.py#L54) function which makes a forward pass of the agent network using the workspace data and updates the workspace accordingly."]},{"cell_type":"markdown","metadata":{"id":"Rn3MlNQ3qGPr"},"source":["Now, if we start at the first epoch (`epoch=0`), we start from the first step (`t=0`). But when subsequently we perform the next epochs (`epoch>0`), we must not forget to cover the transition at the border between the previous epoch and the current epoch. To avoid this risk, we copy the information from the last time step of the previous epoch into the first time step of the next epoch. This is explained in more details in [a previous notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4wkNJqa_k5c"},"outputs":[],"source":["def execute_agent(cfg, epoch, workspace, agent):\n","  if epoch > 0:\n","      workspace.zero_grad()\n","      workspace.copy_n_last_steps(1)\n","      agent(\n","        workspace, t=1, n_steps=cfg.algorithm.n_timesteps - 1, stochastic=True\n","      )\n","  else:\n","    agent(workspace, t=0, n_steps=cfg.algorithm.n_timesteps, stochastic=True)"]},{"cell_type":"markdown","metadata":{"id":"YQNvhO_VAJbh"},"source":["### Compute critic loss"]},{"cell_type":"markdown","metadata":{"id":"fxxobbxRaJXO"},"source":["Note the `critic[1:].detach()` in the computation of the temporal difference target. The idea is that we compute this target as a function of $V(s_{t+1})$, but we do not want to apply gradient descent on this $V(s_{t+1})$, we will only apply gradient descent to the $V(s_t)$ according to this target value."]},{"cell_type":"markdown","metadata":{"id":"Ngf5NHvWBbrE"},"source":["In practice, `x.detach()` detaches a computation graph from a tensor, so it avoids computing a gradient over this tensor."]},{"cell_type":"markdown","metadata":{"id":"fXwrjbueoDw6"},"source":["Note also the trick to deal with terminal states. If the state is terminal, $V(s_{t+1})$ does not make sense. Thus we need to ignore this term. So we multiply the term by `must_bootstrap`: if `must_bootstrap` is True (converted into an int, it becomes a 1), we get the term. If `must_bootstrap` is False (=0), we are at a terminal state, so we ignore the term. This trick is used in many RL libraries, e.g. SB3."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2sepUK-gAM3u"},"outputs":[],"source":["def compute_advantage_loss(cfg, reward, must_bootstrap, critic):\n","  # Compute temporal difference\n","  target = reward[:-1] + cfg.algorithm.discount_factor * critic[1:].detach() * must_bootstrap.int()\n","  advantage = target - critic[:-1]\n","\n","  # Compute critic loss\n","  td_error = advantage ** 2\n","  critic_loss = td_error.mean()\n","  return critic_loss, advantage"]},{"cell_type":"markdown","metadata":{"id":"Jmi91gANWT4z"},"source":["## Main training loop"]},{"cell_type":"markdown","metadata":{"id":"8ixFZeCRN6Y6"},"source":["Note that everything about the shared workspace between all the agents is completely hidden under the hood. This results in a gain of productivity, at the expense of having to dig into the BBRL code if you want to understand the details, change the multiprocessing model, etc."]},{"cell_type":"markdown","metadata":{"id":"gAnnEjF9L9gk"},"source":["This version uses an AutoResetGymAgent. If you haven't done so yet, read  [this notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing) which explains a lot of details. In particular, read it to understand the `execute_agents(...)` function, the `transition_workspace = train_workspace.get_transitions()` line. Read also [the notebook about TimeLimits](https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing) to know more about the computation of `must_bootstrap`."]},{"cell_type":"markdown","metadata":{"id":"OFB1XFE5YEc6"},"source":["Note that we `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()` lines. Several things need to be explained here.\n","- `optimizer.zero_grad()` is necessary to cancel all the gradients computed at the previous iterations\n","- note that we sum all the losses, both for the critic and the actor, before applying back-propagation with `loss.backward()`. At first glance, summing these losses may look weird, as the actor and the critic receive different updates with different parts of the loss. This mechanism relies on the central property of tensor manipulation libraries like TensorFlow and pytorch. In pytorch, each loss tensor comes with its own graph of computation for back-propagating the gradient, in such a way that when you back-propagate the loss, the adequate part of the loss is applied to the adequate parameters.\n","These mechanisms are partly explained [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html).\n","- since the optimizer has been set to work with both the actor and critic parameters, `optimizer.step()` will optimize both agents and pytorch ensure that each will receive its own part of the gradient."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sk85_sRWW-5s"},"outputs":[],"source":["def run_a2c(cfg):\n","  # 1)  Build the  logger\n","  logger = Logger(cfg)\n","  \n","  # 2) Create the environment agent\n","  env_agent = AutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.n_envs,\n","        cfg.algorithm.seed,\n","    )\n","\n","  # 3) Create the A2C Agent\n","  a2c_agent, prob_agent, critic_agent = create_a2c_agent(cfg, env_agent)\n","\n","  # 4) Create the temporal critic agent to compute critic values over the workspace\n","  tcritic_agent = TemporalAgent(critic_agent)\n","\n","  # 5) Configure the workspace to the right dimension\n","  # Note that no parameter is needed to create the workspace. \n","  # In the training loop, calling the agent() and critic_agent() \n","  # will take the workspace as parameter\n","  train_workspace = Workspace()\n","\n","  # 6) Configure the optimizer over the a2c agent\n","  optimizer = setup_optimizer(cfg, prob_agent, critic_agent)\n","  \n","  # 7) Training loop\n","  for epoch in range(cfg.algorithm.max_epochs):\n","    # Execute the agent in the workspace\n","    execute_agent(cfg, epoch, train_workspace, a2c_agent)\n","\n","    # Compute the critic value over the whole workspace\n","    tcritic_agent(train_workspace, n_steps=cfg.algorithm.n_timesteps)\n","\n","    transition_workspace = train_workspace.get_transitions()\n","\n","    # Get relevant tensors (size are timestep x n_envs x ....)\n","\n","    critic, done, reward, action, action_probs, truncated = transition_workspace[\n","                \"critic\", \"env/done\", \"env/reward\", \"action\", \"action_probs\", \"env/truncated\"]\n","\n","    # Determines whether values of the critic should be propagated\n","    # True if the episode reached a time limit or if the task was not done\n","    # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj\n","    must_bootstrap = torch.logical_or(~done[1], truncated[1])\n","\n","    # Compute critic loss (see function above)\n","    critic_loss, advantage = compute_advantage_loss(cfg, reward, must_bootstrap, critic)\n","\n","    # Take the log probability of the actions performed, after some reorganization\n","    action_logp = action_probs[0].gather(1, action[0].view(-1, 1)).squeeze().log()\n","\n","    # Compute the policy gradient loss based on the log probability of the actions performed\n","    a2c_loss = action_logp * advantage.detach()\n","    a2c_loss = a2c_loss.mean()\n","\n","    # Compute entropy loss\n","    entropy_loss = torch.distributions.Categorical(action_probs).entropy().mean()\n","\n","    # Store the losses for tensorboard display\n","    logger.log_losses(cfg, epoch, critic_loss, entropy_loss, a2c_loss)\n","\n","    # Compute the total loss\n","    loss = (\n","      -cfg.algorithm.entropy_coef * entropy_loss\n","      + cfg.algorithm.critic_coef * critic_loss\n","      - cfg.algorithm.a2c_coef * a2c_loss\n","    )\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","\n","    # Compute the cumulated reward on the final states\n","    creward = train_workspace[\"env/cumulated_reward\"]\n","    done = train_workspace[\"env/done\"]\n","    creward = creward[done]\n","    if creward.size()[0] > 0:\n","      # print(creward)\n","      logger.add_log(\"reward\", creward.mean(), epoch)"]},{"cell_type":"markdown","metadata":{"id":"uo6bc3zzKua_"},"source":["## Definition of the parameters"]},{"cell_type":"markdown","metadata":{"id":"36r4PAfvKx-f"},"source":["The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JB2B8zELNWQd"},"outputs":[],"source":["params={\n","  \"logger\":{\n","    \"classname\": \"bbrl.utils.logger.TFLogger\",\n","    \"log_dir\": \"./tmp/\" + str(time.time()),\n","    \"cache_size\": 10000,\n","    \"every_n_seconds\": 10,\n","    \"verbose\": False,    \n","    },\n","\n","  \"algorithm\":{\n","    \"seed\": 432,\n","    \"n_envs\": 2,\n","    \"n_timesteps\": 16,\n","    \"max_epochs\": 7000,\n","    \"discount_factor\": 0.95,\n","    \"entropy_coef\": 0.001,\n","    \"critic_coef\": 1.0,\n","    \"a2c_coef\": 0.1,\n","    \"architecture\":{\n","      \"actor_hidden_size\": 32,\n","      \"critic_hidden_size\": [24, 36],\n","    },\n","  },\n","\n","  \"gym_env\":{\n","    \"classname\": \"__main__.make_env\",\n","    \"env_name\": \"CartPole-v1\",\n","  },\n","  \"optimizer\":\n","  {\n","    \"classname\": \"torch.optim.Adam\",\n","    \"lr\": 0.01,\n","  }\n","}"]},{"cell_type":"markdown","metadata":{"id":"jp7jDeGkaoM1"},"source":["### Launching tensorboard to visualize the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bT-yD1ZnnNBQ","colab":{"base_uri":"https://localhost:8080/","height":821},"executionInfo":{"status":"ok","timestamp":1666605539949,"user_tz":-120,"elapsed":6383,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}},"outputId":"0e0816e2-5bc9-4a94-a7dc-737848806b59"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        (async () => {\n","            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n","            url.searchParams.set('tensorboardColab', 'true');\n","            const iframe = document.createElement('iframe');\n","            iframe.src = url;\n","            iframe.setAttribute('width', '100%');\n","            iframe.setAttribute('height', '800');\n","            iframe.setAttribute('frameborder', 0);\n","            document.body.appendChild(iframe);\n","        })();\n","    "]},"metadata":{}}],"source":["%load_ext tensorboard\n","%tensorboard --logdir ./tmp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l42OUoGROlSt","colab":{"base_uri":"https://localhost:8080/","height":382},"executionInfo":{"status":"error","timestamp":1666605545893,"user_tz":-120,"elapsed":5961,"user":{"displayName":"Damien Dam","userId":"03495965520933490918"}},"outputId":"8a035809-f55e-4ce0-e223-3d4ed6af0770"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-3d51235f07db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_a2c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-14-27a2db83d27a>\u001b[0m in \u001b[0;36mrun_a2c\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m# 3) Create the A2C Agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0ma2c_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_a2c_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# 4) Create the temporal critic agent to compute critic values over the workspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-cfdbd07cc4f8>\u001b[0m in \u001b[0;36mcreate_a2c_agent\u001b[0;34m(cfg, env_agent)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0maction_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   critic_agent = CriticAgent(\n\u001b[0;32m----> 9\u001b[0;31m     observation_size, cfg.algorithm.architecture.critic_hidden_size)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# Combine env and policy agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-f649200306c9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, observation_size, hidden_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         self.critic_model = nn.Sequential(\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"]}],"source":["config=OmegaConf.create(params)\n","torch.manual_seed(config.algorithm.seed)\n","run_a2c(config)"]},{"cell_type":"markdown","metadata":{"id":"vx8c0n2dKuRS"},"source":["With the parameters provided in this colab, you should observe that the reward is collapsing after 6K time steps."]},{"cell_type":"markdown","metadata":{"id":"paHdoNlz9Lpg"},"source":["## What's next?"]},{"cell_type":"markdown","metadata":{"id":"F2OIv4em9Lpj"},"source":["The simple version of A2C above suffers from several limitations:\n","- During training, the cumulated reward is measured from the training agent itself while it is changing. It is a better practice to stop training and perform a few evaluations on the trained agent from time to time.\n","- separating the ProbAgent and the ActionAgent is nice for illustrating the properties of SaLinA, but it is not so convenient, for instance when one wants to know the action of the agent for any state without calling upon a workspace.\n","- The code above only illustrates A2C with discrete actions, though the algorithm can also deal with continuous actions. Doing so requires defining new Agent classes and uniformizing the way they are used to avoid using \"if discrete/continuous\" parts of codes."]},{"cell_type":"markdown","metadata":{"id":"RexnSCwFCIjO"},"source":["We will perform the improvements corresponding to removing all these limitations in [this notebook](https://colab.research.google.com/drive/1C_mgKSTvFEF04qNc_Ljj0cZPucTJDFlO?usp=sharing). We will also add a few features, such as saving and loading agents or drawing pictures of the policy and critic agents."]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1yAQlrShysj4Q9EBpYM8pBsp2aXInhP7x","timestamp":1666603323592},{"file_id":"1XUJSplQm_MttDKtzsJ1AKhhJQT_W0oWi","timestamp":1650557706114},{"file_id":"1J74foctf26QfZ4DuKxGfAwrO2wZmedNi","timestamp":1643612886194},{"file_id":"1-aidxjij0JwVyOgYSLqR-v4KMow4BsbQ","timestamp":1641470409971},{"file_id":"1tZ744yXYoDhwk0xk73baYa7Ks4MRpba8","timestamp":1641465913520},{"file_id":"1SEFpe1yUMjUsKzYkqWF_xQzVzZOQutHZ","timestamp":1641289551712}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}