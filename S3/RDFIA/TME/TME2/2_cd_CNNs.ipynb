{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgWb11WT-07r"
      },
      "source": [
        "# Warning : \n",
        "# Do \"File -> Save a copy in Drive\" before you start modifying the notebook, otherwise your modifications will not be saved.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac5Js1iML3d_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7300e96-b683-495c-cf10-c65922287177"
      },
      "source": [
        "!git clone https://github.com/cdancette/deep-learning-polytech-tp6-7.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-polytech-tp6-7'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 19 (delta 2), reused 7 (delta 1), pack-reused 8\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTv6tOOdOCr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4def18d3-6a70-4889-cff1-6cf0bf4fc19d"
      },
      "source": [
        "cd deep-learning-polytech-tp6-7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-polytech-tp6-7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I7lgAEJvPCh"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from tme6 import *\n",
        "\n",
        "PRINT_INTERVAL = 200\n",
        "PATH=\"datasets\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xVkUxvwy6a0"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the structure of the neural network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # We first define the convolution and pooling layers as a features extractor\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, (5, 5), stride=1, padding=2),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(6, 16, (5, 5), stride=1, padding=0),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "        )\n",
        "        # We then define fully connected layers as a classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(400, 120),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(84, 10)\n",
        "            # Reminder: The softmax is included in the loss, do not put it here\n",
        "        )\n",
        "\n",
        "    # Method called when we apply the network to an input batch\n",
        "    def forward(self, input):\n",
        "        bsize = input.size(0) # batch size\n",
        "        output = self.features(input) # output of the conv layers\n",
        "        output = output.view(bsize, -1) # we flatten the 2D feature maps into one 1D vector for each input\n",
        "        output = self.classifier(output) # we compute the output of the fc layers\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "def get_dataset(batch_size, cuda=False):\n",
        "    \"\"\"\n",
        "    This function loads the dataset and performs transformations on each\n",
        "    image (listed in `transform = ...`).\n",
        "    \"\"\"\n",
        "    train_dataset = datasets.MNIST(PATH, train=True, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ]))\n",
        "    val_dataset = datasets.MNIST(PATH, train=False, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ]))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                        batch_size=batch_size, shuffle=True, pin_memory=cuda, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                        batch_size=batch_size, shuffle=False, pin_memory=cuda, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "\n",
        "def epoch(data, model, criterion, optimizer=None, cuda=False):\n",
        "    \"\"\"\n",
        "    Make a pass (called epoch in English) on the data `data` with the\n",
        "     model `model`. Evaluates `criterion` as loss.\n",
        "     If `optimizer` is given, perform a training epoch using\n",
        "     the given optimizer, otherwise, perform an evaluation epoch (no backward)\n",
        "     of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # indicates whether the model is in eval or train mode (some layers behave differently in train and eval)\n",
        "    model.eval() if optimizer is None else model.train()\n",
        "\n",
        "    # objects to store metric averages\n",
        "    avg_loss = AverageMeter()\n",
        "    avg_top1_acc = AverageMeter()\n",
        "    avg_top5_acc = AverageMeter()\n",
        "    avg_batch_time = AverageMeter()\n",
        "    global loss_plot\n",
        "\n",
        "    # we iterate on the batches\n",
        "    tic = time.time()\n",
        "    for i, (input, target) in enumerate(data):\n",
        "\n",
        "        if cuda: # only with GPU, and not with CPU\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        # forward\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # backward if we are training\n",
        "        if optimizer:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # compute metrics\n",
        "        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
        "        batch_time = time.time() - tic\n",
        "        tic = time.time()\n",
        "\n",
        "        # update\n",
        "        avg_loss.update(loss.item())\n",
        "        avg_top1_acc.update(prec1.item())\n",
        "        avg_top5_acc.update(prec5.item())\n",
        "        avg_batch_time.update(batch_time)\n",
        "        if optimizer:\n",
        "            loss_plot.update(avg_loss.val)\n",
        "        # print info\n",
        "        if i % PRINT_INTERVAL == 0:\n",
        "            print('[{0:s} Batch {1:03d}/{2:03d}]\\t'\n",
        "                  'Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:5.1f} ({top1.avg:5.1f})\\t'\n",
        "                  'Prec@5 {top5.val:5.1f} ({top5.avg:5.1f})'.format(\n",
        "                   \"EVAL\" if optimizer is None else \"TRAIN\", i, len(data), batch_time=avg_batch_time, loss=avg_loss,\n",
        "                   top1=avg_top1_acc, top5=avg_top5_acc))\n",
        "            if optimizer:\n",
        "                loss_plot.plot()\n",
        "\n",
        "    # Print summary\n",
        "    print('\\n===============> Total time {batch_time:d}s\\t'\n",
        "          'Avg loss {loss.avg:.4f}\\t'\n",
        "          'Avg Prec@1 {top1.avg:5.2f} %\\t'\n",
        "          'Avg Prec@5 {top5.avg:5.2f} %\\n'.format(\n",
        "           batch_time=int(avg_batch_time.sum), loss=avg_loss,\n",
        "           top1=avg_top1_acc, top5=avg_top5_acc))\n",
        "\n",
        "    return avg_top1_acc, avg_top5_acc, avg_loss\n",
        "\n",
        "\n",
        "def main(batch_size=128, lr=0.1, epochs=5, cuda=False):\n",
        "\n",
        "    # ex :\n",
        "    #   {\"batch_size\": 128, \"epochs\": 5, \"lr\": 0.1}\n",
        "    \n",
        "    # define model, loss, optim\n",
        "    model = ConvNet()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "\n",
        "    if cuda: # only with GPU, and not with CPU\n",
        "        cudnn.benchmark = True\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "    # Get the data\n",
        "    train, test = get_dataset(batch_size, cuda)\n",
        "\n",
        "    # init plots\n",
        "    plot = AccLossPlot()\n",
        "    global loss_plot\n",
        "    loss_plot = TrainLossPlot()\n",
        "\n",
        "    # We iterate on the epochs\n",
        "    for i in range(epochs):\n",
        "        print(\"=================\\n=== EPOCH \"+str(i+1)+\" =====\\n=================\\n\")\n",
        "        # Train phase\n",
        "        top1_acc, avg_top5_acc, loss = epoch(train, model, criterion, optimizer, cuda)\n",
        "        # Test phase\n",
        "        top1_acc_test, top5_acc_test, loss_test = epoch(test, model, criterion, cuda=cuda)\n",
        "        # plot\n",
        "        plot.update(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXoiJsO0PI5C"
      },
      "source": [
        "main(128, 0.1, cuda=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}